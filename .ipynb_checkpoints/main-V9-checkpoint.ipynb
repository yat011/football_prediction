{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    from IPython.core.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "import sys\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotCurve(train_mean, train_std,test_mean,test_std,sizes):\n",
    "    plt.plot(sizes, train_mean, \n",
    "            color='blue', marker='o', \n",
    "            markersize=5, \n",
    "            label='training accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                  train_mean + train_std,\n",
    "                   train_mean - train_std, alpha=0.15, color='blue')\n",
    "\n",
    "    plt.plot(sizes, test_mean, \n",
    "              color='green', linestyle='--', \n",
    "              marker='s', markersize=5, \n",
    "             label='validation accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                      test_mean + test_std,\n",
    "                     test_mean - test_std, \n",
    "                    alpha=0.15, color='green')\n",
    "    plt.xlabel('x_range')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "def likehoodScore(proba,y):\n",
    "    return np.sum(proba * y)/proba.shape[0]\n",
    "\n",
    "def firstNScore(n, pred, y):\n",
    "    backup = np.array(pred, copy =True)\n",
    "    for r in range(pred.shape[0]):\n",
    "        row = backup[r]\n",
    "        s = np.sort(row)\n",
    "        for c in range(pred.shape[1]):\n",
    "            temp = backup[r][c]\n",
    "            backup[r][c] = False\n",
    "            for j in range(1,n+1):\n",
    "                if temp == s[-j]:\n",
    "                    backup[r][c] = True\n",
    "                    break\n",
    "    res = np.sum(np.logical_and(backup,y))/pred.shape[0]\n",
    "    return res               \n",
    "\n",
    "def oneHotDecode(self, X_sample):\n",
    "    result=None\n",
    "    fiPos = 0\n",
    "    colIndex = 0\n",
    "    while colIndex < X_sample.shape[1]:\n",
    "        if fiPos < len(self.ohe.n_values_) and colIndex == self.ohe.feature_indices_[fiPos]:                \n",
    "            start = self.ohe.feature_indices_[fiPos]\n",
    "            end_ = start+ self.ohe.n_values_[fiPos]\n",
    "            #print(\"start{} end{}\".format(start,end_))\n",
    "            classes = np.argmax(X_sample[:,start:end_],axis=1).reshape(X_sample.shape[0],1)\n",
    "            if result is None:\n",
    "                result = classes\n",
    "            else:\n",
    "                result=np.hstack([result,classes])\n",
    "            colIndex = end_\n",
    "            fiPos = fiPos +1\n",
    "        else:\n",
    "            if result is None:\n",
    "                result = X_sample[:,colIndex:colIndex+1]\n",
    "            else:\n",
    "                result=np.hstack([result, X_sample[:,colIndex:colIndex+1]])\n",
    "            colIndex = colIndex +1\n",
    "        \n",
    "    return result \n",
    "def convertToDate(dayStamps):\n",
    "    res = [] \n",
    "    for v in dayStamps:\n",
    "        res.append(datetime.datetime.fromtimestamp(float(v)*24*60*60))\n",
    "    return res\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def precisionMatrix(proba, y):\n",
    "    def _precisionClassify(df,proba, wins, c =0 ):\n",
    "        for indx, v in enumerate(proba):\n",
    "            row = 0\n",
    "            col = 0\n",
    "            if wins[indx] == c:\n",
    "                col = 0\n",
    "            else:\n",
    "                col =1\n",
    "            if v <0.2:\n",
    "                row =6 \n",
    "            elif v < 0.3 and  v >=0.2:\n",
    "                row =5 \n",
    "            elif v < 0.4 and v >= 0.3:\n",
    "                row = 4 \n",
    "            elif v < 0.5 and v >= 0.4:\n",
    "                row = 3 \n",
    "            elif v < 0.6 and v >= 0.5:\n",
    "                row = 2 \n",
    "            elif v < 0.8 and v >= 0.6:\n",
    "                row = 1\n",
    "            df.iloc[row,col] = df.iloc[row,col]+1 \n",
    "        df[df.columns[2]] = df[df.columns[0]] /(df[df.columns[1]] + df[df.columns[0]])\n",
    "        return df\n",
    "    rowHeader = ['>80','60-80','50-60','40-50','30-40','20-30','<20']\n",
    "    df = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['h_Correct', 'h_Wrong','h_Precent'])\n",
    "    hproba = proba[:,0]\n",
    "    wins = np.argmax(y,axis=1)\n",
    "    df = _precisionClassify(df,hproba,wins)\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['d_Correct', 'd_Wrong','d_Precent'])\n",
    "    dproba = proba[:,1]\n",
    "    df = df.join(_precisionClassify(temp,dproba,wins,c=1))\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['a_Correct', 'a_Wrong','a_Precent'])\n",
    "    aproba = proba[:,2]\n",
    "    df = df.join(_precisionClassify(temp,aproba,wins,c=2))\n",
    "    \n",
    "    bound = pd.DataFrame(np.array([[0.8,1.0],[0.6,0.8],[0.5,0.6],[0.4,0.5],[0.3,0.4],[0.2,0.3],[0,0.2]] )\n",
    "                                ,index=rowHeader, columns=['[lower', 'upper)'])\n",
    "            \n",
    "    return bound.join(df)\n",
    "       \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "def futureTest(mlp, X,y,numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(X[:,c.dateColumn])\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            mlp.fit(X_train,y_train)\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = mlp.predict_proba(X_test)\n",
    "            train_proba =mlp.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "            if verbose == True:\n",
    "                print(\"week{}\".format(w))\n",
    "                print(\"numOftest {} , score {}\".format(X_test.shape[0],mlp.score(X_test,y_test)))\n",
    "                print(np.hstack([stack[errorIndx],proba[errorIndx],y_test[errorIndx]]))\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "        \n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    return firstNScore(1,sum_train_proba,sum_train_y), score, like2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "class FootballDataHelper:\n",
    "    def __init__ (self, recentNum=5):\n",
    "        self.win_mapping = {'H':0, 'D':1,'A':2}\n",
    "        self.recentNum = recentNum\n",
    "        self.df = None\n",
    "        self.teamsData={}\n",
    "        self.session = 0\n",
    "        #self.hiddensCount = 2\n",
    "       \n",
    "    def addColumns(self,df, addition):    \n",
    "        dates = df[\"Date\"].drop_duplicates().values\n",
    "        col_adds = []\n",
    "        for colAdd in addition.columns:\n",
    "             if colAdd not in df.columns:\n",
    "                    df[colAdd]=np.zeros(shape=(df.shape[0],))\n",
    "                    col_adds.append(colAdd)\n",
    "        for date in dates:\n",
    "            dateAddition= addition[addition['Date'] == date].sort(columns='HomeTeam')\n",
    "            dateDf  = df [df['Date']==date].sort(columns='HomeTeam')\n",
    "            for col in col_adds:\n",
    "                dateDf[col] = dateAddition[col].values\n",
    "            df.update(dateDf)\n",
    "        return df\n",
    "            \n",
    "    def saveDf(self,filename):\n",
    "        self.df.to_csv(filename,index=False)\n",
    "    def loadDf(self,filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])    \n",
    "        self.df = df\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "    def readFootBallData(self,year): \n",
    "        filename = \"dataSet/E{}.csv\".format(year)\n",
    "        df = pd.read_csv(filename)\n",
    "        #df = df.drop(df.columns[range(23,df.shape[1])], axis=1)\n",
    "        #df = df.drop(\"Div\",axis=1)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "        df['session'] = pd.Series(np.ones(shape=(df.shape[0],))*self.session, index=df.index)\n",
    "        self.session = self.session +1\n",
    "        \n",
    "        matchDetail = pd.read_csv(\"dataSet/match{}.csv\".format(year))\n",
    "        matchDetail['Date'] =pd.to_datetime(matchDetail['Date'])\n",
    "        df = self.addColumns(df,matchDetail)\n",
    "        \n",
    "        df[\"Future\"] = np.zeros(shape=(df.shape[0],))\n",
    "        \n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = pd.concat([self.df,df])\n",
    "            \n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "    def readFuture(self):\n",
    "        filename = \"dataSet/future.csv\"\n",
    "        df = pd.read_csv(filename)\n",
    "        #df = df.drop(df.columns[range(23,df.shape[1])], axis=1)\n",
    "        #df = df.drop(\"Div\",axis=1)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df[\"Future\"] = np.ones(shape=(df.shape[0],))\n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = pd.concat([self.df,df])\n",
    "            \n",
    "        self.df['HTR']=self.df['HTR'].fillna('D')\n",
    "        self.df = self.df.fillna(0)\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "        \n",
    "    def getTeam(self,dataFrame, teamName):       \n",
    "        return dataFrame[(dataFrame[\"HomeTeam\"] == teamName) | (dataFrame[\"AwayTeam\"] == teamName)]\n",
    " \n",
    "        \n",
    "    def previousRecords(self,team, date , recentNum):\n",
    "        prev = team[( team[\"Date\"] < date)]\n",
    "        \n",
    "        if prev.shape[0] < recentNum :\n",
    "            #print(\"less than min Num\")\n",
    "            return None\n",
    "        else:\n",
    "            return prev.iloc[-recentNum:]\n",
    "    def readPredict (self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "  \n",
    "    def inverseTeamMapping (self, col):\n",
    "        inverseMap ={}\n",
    "        for name in self.teamsMap.keys():        \n",
    "            inverseMap[self.teamsMap[name]] = name\n",
    "        res =[]\n",
    "        for idex, v in enumerate(col):\n",
    "            res.append(inverseMap[v])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def readTeamMatch(self, teamName):\n",
    "        df = pd.read_csv('teams/'+teamName+'.csv')\n",
    "        df['1'] = pd.to_datetime(df['1'],yearfirst=True)\n",
    "        #df['1']= (pd.to_numeric(df['1'])/1e9/24/60/60)\n",
    "        self.teamsData[teamName]=df.sort(['1'],ascending=[False])\n",
    "        self.teamsById[self.teamsMap[teamName]]=self.teamsData[teamName]\n",
    "    \n",
    "    def commonMapping(self, X):\n",
    "        X['HomeTeam'] = X['HomeTeam'].map(self.teamsMap)\n",
    "        X['AwayTeam'] = X['AwayTeam'].map(self.teamsMap)\n",
    "        X['Referee']=X['Referee'].map(self.refereesMap).fillna(0)\n",
    "        X['HTR'] = X['HTR'].map(self.win_mapping)\n",
    "        X['FTR'] = X['FTR'].map(self.win_mapping)\n",
    "        return X\n",
    "    def initData(self, X, target,encode):\n",
    "        X  = X.sort_values(by=\"Date\")\n",
    "        isInput = False\n",
    "        if target is None:\n",
    "            target =X      \n",
    "        else:\n",
    "            if self.ohe is None:\n",
    "                raise Exception(\"Not yet get train data\")\n",
    "            isInput = True\n",
    "            if encode == True:\n",
    "                target = self.commonMapping(target)\n",
    "        y=None\n",
    "        if encode == True:    \n",
    "            X =self.commonMapping(X)\n",
    "            y = []\n",
    "            for v in target['FTR']:\n",
    "                y.append(range(3)==v)\n",
    "        else:\n",
    "            y = target['FTR'].values\n",
    "        target_date = (pd.to_numeric(target['Date'])/1e9/24/60/60).values\n",
    "        return isInput, X,y, target, target_date\n",
    "   \n",
    "    def aggregate(self,recents,nonExpand,isInput,encode):\n",
    "        res =None\n",
    "        if encode == True:\n",
    "            if isInput==False:\n",
    "                self.ohe = OneHotEncoder(categorical_features='all')\n",
    "                self.ohe.fit(recents)\n",
    "            res = self.ohe.transform(recents).toarray()\n",
    "        else:\n",
    "            res = np.array(recents)\n",
    "        self.dateColumn = res.shape[1]\n",
    "        res = np.hstack([res,nonExpand])\n",
    "        return res\n",
    "  \n",
    "    def getH7(self,removeInsufficient=False, target=None,encode = True,future =0):\n",
    "        #Simple recent win,draw, lose \n",
    "        df = self.df\n",
    "        if removeInsufficient == True:\n",
    "            df= df[df['Sufficient'] == 1]\n",
    "        df=df[df['Future']==future]\n",
    "        \n",
    "        isInput, X, y,target, target_date = self.initData(df,target,encode)\n",
    "        resy=[]\n",
    "        resx=[]\n",
    "        print(\"start format\")\n",
    "        recents = X[['HomeTeam','AwayTeam','Referee']].values\n",
    "        haccp = X['HAccP'].values.reshape(X.shape[0],1)\n",
    "        aaccp = X['AAccP'].values.reshape(X.shape[0],1)\n",
    "        homeRecent = np.hstack([X[['HWin','HDraw','HLose']].values,\n",
    "                                (X['HScore'].values - X['HConcede'].values).reshape(X.shape[0],1)])\n",
    "        awayRecent = np.hstack([X[['AWin','ADraw','ALose']].values,\n",
    "                                (X['AScore'].values - X['AConcede'].values).reshape(X.shape[0],1)])\n",
    "        homeMoral = X['HMoral'].values.reshape(X.shape[0],1)\n",
    "        awayMoral = X['AMoral'].values.reshape(X.shape[0],1)\n",
    "        target_date = target_date.reshape(X.shape[0],1)\n",
    "        nonExpand =np.hstack([target_date,X[['HRestDay','ARestDay','HS_Acc','AS_Acc','HST_Acc','AST_Acc',\n",
    "                                            'H_Poss_Acc','A_Poss_Acc','H_atkPass_tot_Acc','A_atkPass_tot_Acc',\n",
    "                                             'H_tkPass_OK_Acc','A_atkPass_Ok_Acc','H_ins_Acc','A_ins_Acc'\n",
    "                                            ]].values,haccp-aaccp,(haccp+1)/(aaccp+1),\n",
    "                                homeRecent,awayRecent, homeMoral - awayMoral + haccp - aaccp])\n",
    "        res = self.aggregate(recents,nonExpand,future,encode)\n",
    "        print(\"finish\")\n",
    "        sys.stdout.flush()\n",
    "        return res, np.array(y)\n",
    "    def _getRank(self,x, X,teamName,recentNum):\n",
    "        team = self.getTeam(X,teamName)\n",
    "        prev = team[team['Date'] < x['Date']].values      \n",
    "        for i in range(recentNum):\n",
    "            pass\n",
    "    def initRanking(self, n = 20):\n",
    "        defaultPt = 1\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HPoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"APoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        hpoints= df[\"HPoints\"].values\n",
    "        apoints=df[\"APoints\"].values\n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hpoints[i] = 3\n",
    "                apoints[i] = 0\n",
    "            elif ftr[i] == 'D':\n",
    "                hpoints[i] = 1\n",
    "                apoints[i] = 1\n",
    "            else :\n",
    "                hpoints[i] = 0\n",
    "                apoints[i] = 3\n",
    "        df[\"HPoints\"]=hpoints\n",
    "        df[\"APoints\"]=apoints\n",
    "        for teamName in self.teamsMap.keys():\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hpoints = team['HPoints'].values\n",
    "            apoints = team['APoints'].values\n",
    "            psum = 0\n",
    "            haccp = team['HAccP'].values\n",
    "            aaccp = team['AAccP'].values\n",
    "        \n",
    "            for  i in range(0,n):\n",
    "                if i < hpoints.shape[0]:\n",
    "                    psum = psum + (hpoints[i] if hometeam[i] == teamName else apoints[i] ) \n",
    "                else:\n",
    "                    psum = psum + defaultPt        \n",
    "                    \n",
    "        \n",
    "            for j in range(team.shape[0]):\n",
    "\n",
    "                if j+n < hpoints.shape[0]:                     \n",
    "                    psum = psum + (hpoints[j+n] if hometeam[j+n]==teamName else apoints[j+n])\n",
    "                else:\n",
    "                    psum = psum + defaultPt \n",
    "                \n",
    "                psum = psum - (hpoints[j] if hometeam[j]==teamName else apoints[j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    haccp[j]=psum\n",
    "                else:\n",
    "                    aaccp[j]=psum\n",
    "            team['HAccp']=haccp\n",
    "            team['AAccP']=aaccp\n",
    "            #print(team[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "            df.update(team)\n",
    "            \n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initRecentData(self, n =5):\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HDraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ADraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HLose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ALose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "                \n",
    "        df[\"HScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HRestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ARestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        #general\n",
    "        generalList = ['HS','AS','HST','AST','H_Poss','A_Poss','H_tkPass_OK','A_atkPass_Ok',\n",
    "                       'H_atkPass_tot','A_atkPass_tot','H_ins','A_ins']\n",
    "        generalOutput = []\n",
    "        for attr in generalList:\n",
    "            temp = attr+'_Acc'\n",
    "            df[temp]=pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "            generalOutput.append(temp)\n",
    "        #\n",
    "        df[\"Sufficient\"] = pd.Series(np.ones(shape=(df.shape[0],)))\n",
    "        \n",
    "      \n",
    "            \n",
    "        \n",
    "        hscore = df['FTHG'].values\n",
    "        ascore = df['FTAG'].values\n",
    "        hconcede = df ['FTAG'].values\n",
    "        aconcede = df['FTHG'].values\n",
    "               \n",
    "        hwin = df['HWin'].values\n",
    "        awin = df['AWin'].values\n",
    "        hlose = df['HLose'].values\n",
    "        alose = df['ALose'].values\n",
    "        hdraw = df['HDraw'].values\n",
    "        adraw = df['ADraw'].values\n",
    "        hmoral = df['HMoral'].values\n",
    "        amoral = df['AMoral'].values\n",
    "        \n",
    "        rankRatio = (df['HAccP'].values+1) / (df['AAccP'].values +1)\n",
    "        \n",
    "        \n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hwin[i] = 1\n",
    "                hmoral[i] = 3 * 1/rankRatio[i]\n",
    "                alose[i]= 1\n",
    "                amoral[i] = -3 * 1/rankRatio[i]\n",
    "            elif ftr[i] == 'D':\n",
    "                hdraw[i] = 1\n",
    "                hmoral[i] = 1 * 1/rankRatio[i]\n",
    "                adraw[i] = 1\n",
    "                amoral[i] = 1 * rankRatio[i]\n",
    "            else :\n",
    "                hlose[i] = 1\n",
    "                hmoral[i] = -3*rankRatio[i]\n",
    "                awin [i] = 1\n",
    "                amoral[i] = 3*rankRatio[i]\n",
    "        \n",
    "        \n",
    "        df[\"HWin\"]=hwin\n",
    "        df[\"AWin\"]=awin\n",
    "        df[\"HDraw\"]=hdraw\n",
    "        df[\"ADraw\"]=adraw\n",
    "        df[\"HLose\"]=hlose\n",
    "        df[\"ALose\"]=alose\n",
    "        df[\"HScore\"]=hscore\n",
    "        df[\"AScore\"]=ascore\n",
    "        df[\"HConcede\"]=hconcede\n",
    "        df[\"AConcede\"]=aconcede\n",
    "        df[\"HMoral\"] = hmoral\n",
    "        df[\"AMoral\"] = amoral\n",
    "        \n",
    "        \n",
    "        \n",
    "        for teamName in self.teamsMap.keys():\n",
    "            print(teamName)\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hwin = team[\"HWin\"].values\n",
    "            awin = team[\"AWin\"].values\n",
    "            hlose= team[\"HLose\"].values\n",
    "            alose = team[\"ALose\"].values\n",
    "            hdraw = team[\"HDraw\"].values\n",
    "            adraw = team[\"ADraw\"].values\n",
    "            hscore = team[\"HScore\"].values\n",
    "            ascore = team[\"AScore\"].values\n",
    "            hconcede = team[\"HConcede\"].values\n",
    "            aconcede = team[\"AConcede\"].values\n",
    "            hmoral = team[\"HMoral\"].values\n",
    "            amoral = team[\"AMoral\"].values\n",
    "            hrestday = team[\"HRestDay\"].values\n",
    "            arestday = team[\"ARestDay\"].values\n",
    "            \n",
    "            #general\n",
    "            hs= team[\"HS\"].values\n",
    "            as_ = team[\"AS\"].values\n",
    "            hst = team[\"HST\"].values\n",
    "            ast = team[\"AST\"].values\n",
    "            original_list =[]\n",
    "            output_list=[]\n",
    "            for indx, o_attr in enumerate(generalList):\n",
    "                original_list.append(team[o_attr].values)\n",
    "                output_list.append(team[generalOutput[indx]].values)\n",
    "            \n",
    "            \n",
    "            matchDate =team['Date'].values\n",
    "            sufficient = team['Sufficient'].values\n",
    "            teamMatchesDate = self.teamsData[teamName].sort('1',ascending=False)['1'].values\n",
    "            \n",
    "            restday = 0\n",
    "            winsum =0 \n",
    "            losesum=0\n",
    "            drawsum=0\n",
    "            scoresum =0\n",
    "            concedesum=0\n",
    "            moralsum = 0\n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            teamAttrSum_list=[0 for i in range(int(len(original_list)/2))]\n",
    "            for  i in range(0,n):\n",
    "                if i < team.shape[0]:\n",
    "                    scoresum = scoresum + (hscore[i] if hometeam[i] == teamName else ascore[i])\n",
    "                    winsum = winsum + (hwin[i] if hometeam[i] == teamName else awin[i])\n",
    "                    losesum= losesum + (hlose[i] if hometeam[i] == teamName else alose[i])\n",
    "                    drawsum= drawsum + (hdraw[i] if hometeam[i] == teamName else adraw[i])\n",
    "                    concedesum = concedesum+ (hconcede[i] if hometeam[i] == teamName else aconcede[i])\n",
    "                    moralsum= moralsum+ (hmoral[i] if hometeam[i] == teamName else amoral[i])\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        teamAttrSum_list[attrIndx] +=(original_list[2*attrIndx][i] if hometeam[i] == teamName else original_list[2*attrIndx+1][i])\n",
    "                else:\n",
    "                    # + 0\n",
    "                    pass\n",
    "            dateIndx = 0\n",
    "            for j in range(team.shape[0]):\n",
    "                while True:\n",
    "                    if dateIndx >= teamMatchesDate.shape[0]:\n",
    "                        sufficient[j] = False\n",
    "                        break\n",
    "                    if teamMatchesDate[dateIndx] < matchDate[j] :\n",
    "                        restday = (matchDate[j] - teamMatchesDate[dateIndx])/np.timedelta64(1,'D')\n",
    "                        break\n",
    "                    else:\n",
    "                        dateIndx = dateIndx + 1\n",
    "                \n",
    "                if j+n < team.shape[0]:                     \n",
    "                    scoresum = scoresum + (hscore[j+n] if hometeam[j+n] == teamName else ascore[j+n])\n",
    "                    winsum = winsum + (hwin[j+n] if hometeam[j+n] == teamName else awin[j+n])\n",
    "                    losesum= losesum + (hlose[j+n] if hometeam[j+n] == teamName else alose[j+n])\n",
    "                    drawsum= drawsum + (hdraw[j+n] if hometeam[j+n] == teamName else adraw[j+n])\n",
    "                    concedesum = concedesum+ (hconcede[j+n] if hometeam[j+n] == teamName else aconcede[j+n])\n",
    "                    moralsum= moralsum+ (hmoral[j+n] if hometeam[j+n] == teamName else amoral[j+n])\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        teamAttrSum_list[attrIndx] += (original_list[2*attrIndx][j+n] if hometeam[j+n] == teamName else original_list[2*attrIndx+1][j+n])\n",
    "                else:\n",
    "                    sufficient[j] = False\n",
    "                    \n",
    "                \n",
    "                scoresum = scoresum - (hscore[j] if hometeam[j] == teamName else ascore[j])\n",
    "                winsum = winsum - (hwin[j] if hometeam[j] == teamName else awin[j])\n",
    "                losesum= losesum - (hlose[j] if hometeam[j] == teamName else alose[j])\n",
    "                drawsum= drawsum - (hdraw[j] if hometeam[j] == teamName else adraw[j])\n",
    "                concedesum = concedesum - (hconcede[j] if hometeam[j] == teamName else aconcede[j])\n",
    "                moralsum= moralsum - (hmoral[j] if hometeam[j] == teamName else amoral[j])\n",
    "                for attrIndx in range(len(teamAttrSum_list)):\n",
    "                    teamAttrSum_list[attrIndx] -=  (original_list[2*attrIndx][j] if hometeam[j] == teamName else original_list[2*attrIndx+1][j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    hscore[j] = scoresum\n",
    "                    hwin[j] = winsum\n",
    "                    hlose[j] = losesum\n",
    "                    hdraw[j] = drawsum\n",
    "                    hconcede[j] = concedesum\n",
    "                    hmoral[j] = moralsum\n",
    "                    hrestday[j] = restday\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        output_list[2*attrIndx][j] = teamAttrSum_list[attrIndx]\n",
    "                else:\n",
    "                    ascore[j] = scoresum\n",
    "                    awin[j] = winsum\n",
    "                    alose[j] = losesum\n",
    "                    adraw[j] = drawsum\n",
    "                    aconcede[j] = concedesum\n",
    "                    amoral[j] = moralsum\n",
    "                    arestday[j] = restday\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        output_list[2*attrIndx+1][j] = teamAttrSum_list[attrIndx]\n",
    "            team[\"HWin\"]=hwin\n",
    "            team[\"AWin\"]=awin\n",
    "            team[\"HDraw\"]=hdraw\n",
    "            team[\"ADraw\"]=adraw\n",
    "            team[\"HLose\"]=hlose\n",
    "            team[\"ALose\"]=alose\n",
    "            team[\"HScore\"]=hscore\n",
    "            team[\"AScore\"]=ascore\n",
    "            team[\"HConcede\"]=hconcede\n",
    "            team[\"AConcede\"]=aconcede\n",
    "            team[\"HMoral\"] = hmoral\n",
    "            team[\"AMoral\"] = amoral\n",
    "            team['Sufficient'] = sufficient\n",
    "            for indx in range(len(output_list)):\n",
    "                team[generalOutput[indx]]= output_list[indx]\n",
    "        \n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            df.update(team)\n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initTeamData(self):\n",
    "        self.teamsData={}\n",
    "        self.teamsById={}\n",
    "        for name in self.teamsMap.keys():\n",
    "            self.readTeamMatch(name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "c = FootballDataHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:23: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:24: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#c.readFootBallData(\"E0_1112.csv\")\n",
    "c.readFootBallData(2012)\n",
    "c.readFootBallData(2013)\n",
    "c.readFootBallData(2014)\n",
    "c.readFootBallData(2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>A_Poss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>37.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Man City</td>\n",
       "      <td>45.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>46.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>44.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Everton</td>\n",
       "      <td>46.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>40.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Watford</td>\n",
       "      <td>47.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>49.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>50.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>2016-03-13</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>62.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Norwich</td>\n",
       "      <td>66.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>53.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>61.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>51.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Everton</td>\n",
       "      <td>58.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Man City</td>\n",
       "      <td>28.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>50.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>44.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Watford</td>\n",
       "      <td>49.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>65.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>44.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Man United</td>\n",
       "      <td>38.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>37.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2012-09-15</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>48.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>31.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>55.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>38.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Man City</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>51.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>66.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>37.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Norwich</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Man United</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>47.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>60.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012-08-22</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>28.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012-08-20</td>\n",
       "      <td>Everton</td>\n",
       "      <td>69.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-08-19</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012-08-19</td>\n",
       "      <td>Man City</td>\n",
       "      <td>35.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>48.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Reading</td>\n",
       "      <td>47.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>QPR</td>\n",
       "      <td>50.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>59.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>65.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>29.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        HomeTeam  A_Poss\n",
       "1442 2016-03-20       Tottenham    37.2\n",
       "1441 2016-03-20     Southampton    50.8\n",
       "1440 2016-03-20       Newcastle    40.1\n",
       "1439 2016-03-20        Man City    45.5\n",
       "1436 2016-03-19         Swansea    46.2\n",
       "1434 2016-03-19  Crystal Palace    44.1\n",
       "1435 2016-03-19         Everton    46.1\n",
       "1433 2016-03-19         Chelsea    40.6\n",
       "1437 2016-03-19         Watford    47.9\n",
       "1438 2016-03-19       West Brom    49.9\n",
       "1432 2016-03-14       Leicester    50.1\n",
       "1431 2016-03-13     Aston Villa    62.2\n",
       "1430 2016-03-12           Stoke    39.4\n",
       "1429 2016-03-12         Norwich    66.1\n",
       "1428 2016-03-12     Bournemouth    56.0\n",
       "1427 2016-03-06       West Brom    53.4\n",
       "1426 2016-03-06  Crystal Palace    61.5\n",
       "1422 2016-03-05     Southampton    34.9\n",
       "1418 2016-03-05         Chelsea    51.3\n",
       "1419 2016-03-05         Everton    58.5\n",
       "1420 2016-03-05        Man City    28.8\n",
       "1421 2016-03-05       Newcastle    50.7\n",
       "1423 2016-03-05         Swansea    44.2\n",
       "1424 2016-03-05       Tottenham    48.2\n",
       "1425 2016-03-05         Watford    49.8\n",
       "1417 2016-03-02        West Ham    65.1\n",
       "1416 2016-03-02           Stoke    44.7\n",
       "1415 2016-03-02      Man United    38.8\n",
       "1414 2016-03-02       Liverpool    50.5\n",
       "1413 2016-03-02         Arsenal    37.4\n",
       "...         ...             ...     ...\n",
       "36   2012-09-15      Sunderland    65.9\n",
       "28   2012-09-02     Southampton    54.9\n",
       "27   2012-09-02       Newcastle    48.3\n",
       "26   2012-09-02       Liverpool    47.0\n",
       "25   2012-09-01           Wigan    31.3\n",
       "24   2012-09-01        West Ham    54.9\n",
       "23   2012-09-01       West Brom    55.7\n",
       "22   2012-09-01       Tottenham    38.4\n",
       "21   2012-09-01         Swansea    36.2\n",
       "20   2012-09-01        Man City    40.1\n",
       "18   2012-08-26       Liverpool    51.3\n",
       "19   2012-08-26           Stoke    66.7\n",
       "17   2012-08-25       Tottenham    41.0\n",
       "16   2012-08-25         Swansea    37.8\n",
       "15   2012-08-25     Southampton    50.8\n",
       "14   2012-08-25         Norwich    44.9\n",
       "13   2012-08-25      Man United    40.1\n",
       "12   2012-08-25         Chelsea    47.5\n",
       "11   2012-08-25     Aston Villa    60.8\n",
       "10   2012-08-22         Chelsea    28.4\n",
       "9    2012-08-20         Everton    69.2\n",
       "8    2012-08-19           Wigan    48.0\n",
       "7    2012-08-19        Man City    35.9\n",
       "2    2012-08-18       Newcastle    48.1\n",
       "4    2012-08-18         Reading    47.4\n",
       "3    2012-08-18             QPR    50.1\n",
       "1    2012-08-18          Fulham    40.2\n",
       "5    2012-08-18       West Brom    59.5\n",
       "6    2012-08-18        West Ham    65.8\n",
       "0    2012-08-18         Arsenal    29.9\n",
       "\n",
       "[1443 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.df[['Date','HomeTeam','A_Poss']].sort(columns='Date',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "c.readFuture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:140: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c.initTeamData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:224: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df = c.initRanking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:284: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:400: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:479: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:480: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:481: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:484: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:485: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:486: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:487: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:488: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:489: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:492: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burnley\n",
      "West Ham\n",
      "West Brom\n",
      "Aston Villa\n",
      "Arsenal\n",
      "Crystal Palace\n",
      "Norwich\n",
      "Man United\n",
      "Hull\n",
      "Chelsea\n",
      "QPR\n",
      "Watford\n",
      "Cardiff\n",
      "Fulham\n",
      "Everton\n",
      "Sunderland\n",
      "Swansea\n",
      "Reading\n",
      "Man City\n",
      "Leicester\n",
      "Southampton\n",
      "Bournemouth\n",
      "Stoke\n",
      "Tottenham\n",
      "Newcastle\n",
      "Wigan\n",
      "Liverpool\n"
     ]
    }
   ],
   "source": [
    "df=c.initRecentData(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.saveDf('dataSet/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.loadDf('dataSet/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y=c.getH7(removeInsufficient=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "def plotErrorDate(X_test, X_err, dateCol = 10):\n",
    "    X_test_date =np.sort(X_test[:,c.dateColumn])\n",
    "    X_date=[]\n",
    "    y_date=[]\n",
    "    for v in X_test_date:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_date) ==0  or X_date[-1] != date:\n",
    "            X_date.append(date)\n",
    "            y_date.append(1)\n",
    "        else:\n",
    "            y_date[-1] = y_date[-1] +1\n",
    "    plt.plot_date(X_date,y_date,xdate=True)\n",
    "    X_err_d = np.sort(X_err[:,c.dateColumn])\n",
    "    X_err_date=[]\n",
    "    y_err_date = []\n",
    "    for v in X_err_d:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_err_date) ==0  or X_err_date[-1] != date:\n",
    "            X_err_date.append(date)\n",
    "            y_err_date.append(1)\n",
    "        else:\n",
    "            y_err_date[-1] = y_err_date[-1] +1\n",
    "    plt.plot_date(X_err_date,y_err_date,xdate=True,color='red')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "g_hiddenNodes = int(45)\n",
    "g_alpha = 0 \n",
    "clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_hiddenNodes], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "              learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=1000,early_stopping = False,verbose = 3)\n",
    "mlp = Pipeline([ ('scl', StandardScaler()),('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"start learning\")\n",
    "sys.stdout.flush()\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=mlp, \n",
    "                       X=X, \n",
    "                      y=y, \n",
    "                      train_sizes=np.linspace(0.1, 1.0, 4), \n",
    "                      cv=4,\n",
    "                     n_jobs=1,verbose=3)\n",
    "print(\"finishing\")   \n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "print(test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learningRes = np.vstack([train_sizes,train_mean,train_std,test_mean,test_std]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learningDf = pd.DataFrame(learningRes,columns=['size','train_mean','train_std','test_mean','test_std'])\n",
    "print(learningDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotCurve(train_mean,train_std,test_mean,test_std,train_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate(mlp, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    firstNScores = []\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "\n",
    "        mlp.fit(X[train], y[train])\n",
    "        score = mlp.score(X[test], y[test])\n",
    "        firstNScores.append(firstNScore(2, mlp.predict_proba(X[test]), y[test]))\n",
    "        train_scores.append(mlp.score(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, firstNScores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def lamda_test(mlp, X, y, lamdas):\n",
    "    \n",
    "    train_scores=[]\n",
    "    test_scores=[]\n",
    "    for lamda in lamdas:\n",
    "        clf.set_params(alpha= lamda)\n",
    "        train_s, test_s, firstNScores = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_s, test_s, firstNScores =futureTest(mlp,X,y,numOfWeek=20) \n",
    "        train_scores.append(train_s)\n",
    "        test_scores.append(test_s)\n",
    "        print(\"lamda: {}, train: {}, test: {}\".format(lamda, \n",
    "                    np.mean(train_s), np.mean(test_s)) )\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plotCurve(train_mean,train_std,test_mean,test_std,lamdas)\n",
    "    return np.array(train_scores),np.array(test_scores)\n",
    "\n",
    "l_range = []\n",
    "for i in range(0,50):\n",
    "    l_range.append(2*i)\n",
    "train_scores,test_scores = lamda_test(mlp,X,y,np.array(l_range))\n",
    "#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "alphaRes = np.vstack([l_range,train_mean,train_std,test_mean,test_std]).T\n",
    "alphaDf = pd.DataFrame(alphaRes,columns=['alpha','train_mean','train_std','test_mean','test_std'])\n",
    "print(alphaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testNodeSize(start ,end):\n",
    "    node_range = range(start,end,5)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    for node in node_range:   \n",
    "        print(\"start node:{}\".format(node))\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[node], activation='logistic', algorithm='l-bfgs', alpha=0, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores,first2 = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_scores,test_scores , first2= futureTest(mlp, X,y,numOfWeek = 10)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        print(\"Node {}: train_mean {}  v.s. test_mean {}\".format(node,np.mean(train_scores),np.mean(test_scores)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(node_range))\n",
    "    return node_range, train_means,train_std,test_means,test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node_range, train_means,train_std,test_means,test_std=testNodeSize(1,X.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodeRes = np.vstack([node_range,train_means,train_std,test_means,test_std]).T\n",
    "nodeDf = pd.DataFrame(nodeRes,columns=['nodeNum','train_mean','train_std','test_mean','test_std'])\n",
    "print(nodeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.set_params(alpha=g_alpha)\n",
    "print(clf)\n",
    "train_score, test_score, first2 = futureTest(mlp,X,y,numOfWeek = 30, verbose=True)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testRecentNum(start, end):\n",
    "    recent_range = range(start,end)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    first2_mean=[]\n",
    "    for recent in recent_range:\n",
    "        print(\"start recent:{}\".format(recent))\n",
    "        X,y = c.getH7(recent)\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_alpha], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores, first2 = crossValidate(mlp,X,y,fold=10)\n",
    "        #train_scores,test_scores, first2 = futureTest(mlp, X,y,numOfWeek = 15)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        first2_mean.append(np.mean(first2))\n",
    "        print(\"recent {}: train_mean {}  v.s. test_mean {} , first2_mean {}\".format(\n",
    "                recent,np.mean(train_scores),np.mean(test_scores),np.mean(first2)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(recent_range))\n",
    "    return train_means,train_std,test_means,test_std,first2_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_means,train_std,test_means,test_std,first2_mean=testRecentNum(1 ,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "proba = mlp.predict_proba(X_test)\n",
    "precisionMatrix(proba,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#future\n",
    "mlp.fit(X,y)\n",
    "inData = c.readPredict(\"future.csv\")\n",
    "print(inData)\n",
    "X_in, y_in = c.getH6(5,target=inData)\n",
    "res = mlp.predict(X_in)\n",
    "proba= mlp.predict_proba(X_in)\n",
    "print(mlp.score(X_in,y_in))\n",
    "print (np.hstack([proba,y_in]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "return\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient=True, encode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1358, 29)\n",
      "                home         away        Referee   time HRestTime ARestTime  \\\n",
      "0             Fulham     Man City       M Halsey  15612         7         3   \n",
      "1            Norwich    Liverpool        M Jones  15612         2         2   \n",
      "2            Everton  Southampton      L Probert  15612         3         3   \n",
      "3              Stoke      Swansea         J Moss  15612         7         3   \n",
      "4            Arsenal      Chelsea     M Atkinson  15612         2         3   \n",
      "5         Man United    Tottenham          C Foy  15612         2         2   \n",
      "6        Aston Villa    West Brom       A Taylor  15613         4         3   \n",
      "7                QPR     West Ham  M Clattenburg  15614         4         5   \n",
      "8              Wigan      Everton       K Friend  15619         7         7   \n",
      "9          West Brom          QPR        M Jones  15619         6         4   \n",
      "10          Man City   Sunderland      L Probert  15619         2         7   \n",
      "11           Chelsea      Norwich       A Taylor  15619         3         7   \n",
      "12          West Ham      Arsenal         P Dowd  15619         4         2   \n",
      "13           Swansea      Reading         M Dean  15619         7         7   \n",
      "14       Southampton       Fulham  M Clattenburg  15620         8         8   \n",
      "15         Tottenham  Aston Villa    N Swarbrick  15620         2         7   \n",
      "16         Liverpool        Stoke        L Mason  15620         2         8   \n",
      "17         Newcastle   Man United         H Webb  15620         2         4   \n",
      "18           Norwich      Arsenal      L Probert  15633        14        13   \n",
      "19          West Ham  Southampton    N Swarbrick  15633        13        13   \n",
      "20         West Brom     Man City  M Clattenburg  15633        14        14   \n",
      "21           Swansea        Wigan        M Jones  15633        14        14   \n",
      "22        Man United        Stoke       A Taylor  15633        13        13   \n",
      "23            Fulham  Aston Villa          C Foy  15633        13        13   \n",
      "24         Liverpool      Reading         R East  15633        13        14   \n",
      "25         Tottenham      Chelsea         M Dean  15633        13        14   \n",
      "26               QPR      Everton         J Moss  15634        15        15   \n",
      "27        Sunderland    Newcastle     M Atkinson  15634        15        14   \n",
      "28          Man City      Swansea     M Atkinson  15640         2         7   \n",
      "29           Arsenal          QPR       A Taylor  15640         2         6   \n",
      "...              ...          ...            ...    ...       ...       ...   \n",
      "1328           Stoke    Newcastle    N Swarbrick  16862         4        17   \n",
      "1329      Man United      Watford        M Jones  16862         3         4   \n",
      "1330        West Ham    Tottenham     A Marriner  16862         4         3   \n",
      "1331         Arsenal      Swansea       R Madley  16862         3         3   \n",
      "1332       Liverpool     Man City     M Atkinson  16862         2         2   \n",
      "1333         Chelsea        Stoke  M Clattenburg  16865         3         2   \n",
      "1334     Southampton   Sunderland    N Swarbrick  16865         3         3   \n",
      "1335       Newcastle  Bournemouth      P Tierney  16865         2         3   \n",
      "1336         Everton     West Ham       A Taylor  16865         3         2   \n",
      "1337         Swansea      Norwich       C Pawson  16865         2         3   \n",
      "1338       Tottenham      Arsenal       M Oliver  16865         2         2   \n",
      "1339         Watford    Leicester         J Moss  16865         2         3   \n",
      "1340        Man City  Aston Villa        L Mason  16865         2         3   \n",
      "1341       West Brom   Man United         M Dean  16866         4         3   \n",
      "1342  Crystal Palace    Liverpool     A Marriner  16866         4         3   \n",
      "1343     Bournemouth      Swansea         R East  16872         7         7   \n",
      "1344           Stoke  Southampton        L Mason  16872         7         7   \n",
      "1345         Norwich     Man City         J Moss  16872         7         7   \n",
      "1346     Aston Villa    Tottenham       A Taylor  16873         8         2   \n",
      "1347       Leicester    Newcastle       C Pawson  16874         8         9   \n",
      "1348       West Brom      Norwich       A Taylor  16879        12         7   \n",
      "1349         Watford        Stoke       C Pawson  16879         6         7   \n",
      "1350         Chelsea     West Ham       R Madley  16879         6         5   \n",
      "1351         Everton      Arsenal  M Clattenburg  16879         6         2   \n",
      "1352  Crystal Palace    Leicester        M Jones  16879         7         4   \n",
      "1353         Swansea  Aston Villa         M Dean  16879         7         5   \n",
      "1354       Tottenham  Bournemouth    N Swarbrick  16880         2         8   \n",
      "1355       Newcastle   Sunderland     M Atkinson  16880         5        15   \n",
      "1356        Man City   Man United       M Oliver  16880         4         2   \n",
      "1357     Southampton    Liverpool         R East  16880         8         2   \n",
      "\n",
      "     HS_Acc AS_Acc HST_Acc AST_Acc ... HWin HDraw HLose H goal Diff AWin  \\\n",
      "0        74     79      56      53 ...    3     0     2           5    2   \n",
      "1        55     79      27      40 ...    0     3     2          -6    0   \n",
      "2        94     59      52      31 ...    3     1     1           4    1   \n",
      "3        46     64      24      41 ...    0     4     1          -1    2   \n",
      "4        71     69      33      38 ...    2     3     0           7    4   \n",
      "5        75     85      41      50 ...    4     0     1           6    2   \n",
      "6        55     58      33      35 ...    1     1     3          -4    3   \n",
      "7        54     61      32      37 ...    0     2     3          -8    2   \n",
      "8        55     95      29      54 ...    1     1     3          -4    3   \n",
      "9        57     47      32      30 ...    2     2     1           0    0   \n",
      "10       80     28      50      17 ...    2     3     0           3    1   \n",
      "11       74     61      40      32 ...    4     1     0           6    0   \n",
      "12       70     71      43      34 ...    2     2     1           1    2   \n",
      "13       60     42      33      18 ...    1     1     3          -4    0   \n",
      "14       58     70      27      50 ...    1     0     4          -7    2   \n",
      "15       83     64      50      35 ...    3     2     0           4    1   \n",
      "16       75     50      39      26 ...    1     2     2           0    1   \n",
      "17       62     78      29      42 ...    1     3     1          -1    4   \n",
      "18       55     75      32      42 ...    0     2     3          -7    3   \n",
      "19       70     57      40      24 ...    2     2     1           2    1   \n",
      "20       60     95      35      61 ...    3     1     1           1    3   \n",
      "21       75     53      42      27 ...    0     2     3          -7    0   \n",
      "22       70     48      40      25 ...    4     0     1           8    1   \n",
      "23       70     65      44      37 ...    2     1     2           0    1   \n",
      "24       76     41      36      21 ...    1     2     2           0    0   \n",
      "25       88     71      50      42 ...    4     1     0           6    4   \n",
      "26       54     92      32      54 ...    0     1     4          -5    2   \n",
      "27       33     64      21      27 ...    1     3     1          -2    1   \n",
      "28       97     79      62      43 ...    3     2     0           5    1   \n",
      "29       77     59      42      33 ...    2     1     2           5    0   \n",
      "...     ...    ...     ...     ... ...  ...   ...   ...         ...  ...   \n",
      "1328     46     70      14      26 ...    2     0     3          -6    2   \n",
      "1329     59     52      24      12 ...    2     1     2           2    2   \n",
      "1330     73    107      20      43 ...    2     2     1           2    5   \n",
      "1331     77     67      28      18 ...    2     1     2           1    1   \n",
      "1332     73     80      27      17 ...    2     1     2           4    2   \n",
      "1333     69     52      20      14 ...    3     2     0           6    3   \n",
      "1334     48     69      13      19 ...    2     1     2          -1    1   \n",
      "1335     52     56      19      17 ...    1     0     4          -8    2   \n",
      "1336     96     77      28      20 ...    3     0     2           6    3   \n",
      "1337     71     57      15      12 ...    1     2     2          -1    0   \n",
      "1338     92     86      34      31 ...    4     0     1           5    2   \n",
      "1339     52     69      10      24 ...    1     2     2          -1    3   \n",
      "1340     67     41      15      12 ...    1     1     3          -5    1   \n",
      "1341     51     65      16      26 ...    2     2     1           1    3   \n",
      "1342     68     65      19      30 ...    0     2     3          -3    3   \n",
      "1343     64     63      18      14 ...    2     1     2           0    2   \n",
      "1344     58     50      19      15 ...    3     1     1           1    2   \n",
      "1345     55     73      13      21 ...    0     1     4          -5    2   \n",
      "1346     37    101      11      39 ...    1     0     4         -11    3   \n",
      "1347     70     55      25      20 ...    3     1     1           3    1   \n",
      "1348     43     48      11       9 ...    3     1     1           2    0   \n",
      "1349     51     60      11      18 ...    1     1     3          -2    3   \n",
      "1350     77     69      23      19 ...    3     2     0           6    3   \n",
      "1351     91     75      34      24 ...    3     0     2           6    2   \n",
      "1352     68     66      20      19 ...    0     2     3          -3    3   \n",
      "1353     56     38      15      10 ...    2     0     3          -1    0   \n",
      "1354     94     59      38      18 ...    3     1     1           3    3   \n",
      "1355     61     69      19      20 ...    1     0     4          -7    1   \n",
      "1356     82     57      22      22 ...    1     1     3          -2    2   \n",
      "1357     53     71      17      28 ...    2     1     2          -1    3   \n",
      "\n",
      "     ADraw ALose A goal diff moraldiff + h-a  y  \n",
      "0        3     0           3         -5.4577  A  \n",
      "1        2     3          -6         4.26691  A  \n",
      "2        0     4          -6         20.4973  H  \n",
      "3        1     2           3        -0.98787  H  \n",
      "4        1     0           7         -7.3312  A  \n",
      "5        2     1           2         7.24231  A  \n",
      "6        1     1           3        -16.1433  D  \n",
      "7        2     1           1        -16.4869  A  \n",
      "8        1     1           5        -18.8485  D  \n",
      "9        2     3          -4         17.7318  H  \n",
      "10       4     0           1         5.22302  H  \n",
      "11       3     2          -4         26.8163  H  \n",
      "12       2     1           6         1.33957  A  \n",
      "13       2     3          -5         3.22343  D  \n",
      "14       0     3          -1        -9.28572  D  \n",
      "15       2     2          -3         17.1502  H  \n",
      "16       3     1           1        -5.98228  D  \n",
      "17       0     1           6         -7.8114  A  \n",
      "18       1     1           8        -21.3046  H  \n",
      "19       1     3          -5         12.8002  H  \n",
      "20       2     0           6        -6.20904  A  \n",
      "21       2     3          -6        0.178857  H  \n",
      "22       3     1           1         9.43603  H  \n",
      "23       2     2          -3         5.66887  H  \n",
      "24       2     3          -5         7.88105  H  \n",
      "25       1     0           7        -2.72658  A  \n",
      "26       2     1           3        -23.5168  D  \n",
      "27       3     1          -2       -0.225469  D  \n",
      "28       1     3          -6         24.2804  H  \n",
      "29       2     3          -3         12.0548  H  \n",
      "...    ...   ...         ...             ... ..  \n",
      "1328     0     3          -6         3.14025  H  \n",
      "1329     2     1           1        -4.32401  H  \n",
      "1330     0     0           8        -19.9385  H  \n",
      "1331     2     2          -1         13.1779  A  \n",
      "1332     1     2           2        -2.28404  H  \n",
      "1333     0     2          -2         9.61243  D  \n",
      "1334     2     2          -1         1.78991  D  \n",
      "1335     1     2          -1        -12.8138  A  \n",
      "1336     1     1           3        -10.8407  A  \n",
      "1337     1     4          -7         19.0105  H  \n",
      "1338     1     2           1         13.5843  D  \n",
      "1339     1     1           4        -19.0615  A  \n",
      "1340     0     4          -9         13.3512  H  \n",
      "1341     1     1           4        -4.45653  H  \n",
      "1342     1     1           8        -24.5246  A  \n",
      "1343     1     2           0         1.36626  H  \n",
      "1344     1     2          -1         8.06201  A  \n",
      "1345     0     3          -1        -21.0893  D  \n",
      "1346     1     1           2        -30.6047  A  \n",
      "1347     0     4          -9         31.6266  H  \n",
      "1348     2     3          -3         20.5266  A  \n",
      "1349     1     1           3        -10.9102  A  \n",
      "1350     1     1           2         1.42972  D  \n",
      "1351     1     2           1        -2.52855  A  \n",
      "1352     1     1           2        -33.3654  A  \n",
      "1353     0     5         -15         16.7183  H  \n",
      "1354     1     1           3         3.93801  H  \n",
      "1355     3     1           0        -15.3967  D  \n",
      "1356     1     2           0        -4.08739  A  \n",
      "1357     1     1           8           -9.09  H  \n",
      "\n",
      "[1358 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "df = pd.DataFrame(np.hstack([X,y.reshape(y.shape[0],1)]))\n",
    "\n",
    "df.columns = ['home','away','Referee','time','HRestTime','ARestTime','HS_Acc','AS_Acc','HST_Acc','AST_Acc',\n",
    "                                            'H_Poss_Acc','A_Poss_Acc','H_atkPass_tot_Acc','A_atkPass_tot_Acc'\n",
    "                                             ,'H_atkPass_Ok_Acc','A_atkPass_OK_Acc','H_ins_Acc','A_ins_Acc'\n",
    "              ,'HAccP - AAccP','H/A','HWin','HDraw','HLose','H goal Diff',\n",
    "'AWin','ADraw','ALose','A goal diff','moraldiff + h-a',\n",
    "              'y']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('dataSet/V9.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "def splitData(X,y):\n",
    "    X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_test, X_val, y_test,y_val = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "def createModel(hidSize, inputDim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidSize[0], input_dim=inputDim, init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(hidSize[1], init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(3, init='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd = SGD(lr=1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "    return model\n",
    "earlyCallback = EarlyStopping(patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate2(node_sizes, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    proba_test = []\n",
    "    proba_y=[]\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "        earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "        model = createModel(node_sizes,X.shape[1])\n",
    "        history = model.fit(X[train],y[train],verbose=0,nb_epoch=500, validation_split=0.1,show_accuracy=True, callbacks=[earlyCallback])\n",
    "      #  firstNScores.append(firstNScore(2, model.predict_proba(X[test]), y[test]))\n",
    "        score = model.evaluate(X[test],y[test])\n",
    "        proba_test.append(model.predict_proba(X[test]))\n",
    "        proba_y.append(y[test])\n",
    "        train_scores.append(model.evaluate(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, val_loss: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, proba_test,proba_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNodeNum(X,y, sizes):\n",
    "    train_loss=[] \n",
    "    score_loss=[]\n",
    "    for s in sizes:\n",
    "        train_scores,scores,  proba_test,proba_y= crossValidate2([s,s],X,y,fold=5)\n",
    "        print(\"size:{} , val_loss_mean:{}\".format(s,np.mean(scores)))\n",
    "        train_loss.append(train_scores)\n",
    "        score_loss.append(scores)\n",
    "    return train_loss,score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.996\n",
      "Epoch 00119: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.966\n",
      "Epoch 00078: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.983\n",
      "Epoch 00057: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00045: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.034\n",
      "size:15 , val_loss_mean:0.994878006496512\n",
      "Epoch 00090: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.997\n",
      "Epoch 00064: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.961\n",
      "Epoch 00108: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00056: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.000\n",
      "Epoch 00044: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.032\n",
      "size:20 , val_loss_mean:0.997879133147474\n",
      "Epoch 00057: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.985\n",
      "Epoch 00109: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00106: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00081: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.008\n",
      "Epoch 00044: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.042\n",
      "size:25 , val_loss_mean:1.002979383102754\n",
      "Epoch 00064: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00066: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.957\n",
      "Epoch 00044: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.985\n",
      "Epoch 00053: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.993\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.035\n",
      "size:30 , val_loss_mean:0.9919062859579861\n",
      "Epoch 00047: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.985\n",
      "Epoch 00063: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:35 , val_loss_mean:0.9929705902102708\n",
      "Epoch 00048: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.984\n",
      "Epoch 00098: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.987\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.991\n",
      "Epoch 00042: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.040\n",
      "size:40 , val_loss_mean:0.9931346279183522\n",
      "Epoch 00043: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.984\n",
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00089: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00042: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.033\n",
      "size:45 , val_loss_mean:0.9946483096903833\n",
      "Epoch 00074: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.995\n",
      "Epoch 00091: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.985\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00027: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.038\n",
      "size:50 , val_loss_mean:0.9952394351575709\n",
      "Epoch 00050: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00080: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00035: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00039: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.040\n",
      "size:55 , val_loss_mean:0.9950326937267647\n",
      "Epoch 00070: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00084: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.965\n",
      "Epoch 00066: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.992\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00027: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:60 , val_loss_mean:0.9955750316447294\n",
      "Epoch 00056: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.988\n",
      "Epoch 00040: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00042: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:65 , val_loss_mean:0.9943353890837905\n",
      "Epoch 00072: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00080: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.000\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:70 , val_loss_mean:0.9954743319077854\n",
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.990\n",
      "Epoch 00061: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.966\n",
      "Epoch 00045: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00044: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.034\n",
      "size:75 , val_loss_mean:0.9945887298112925\n",
      "Epoch 00087: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 1.010\n",
      "Epoch 00074: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.972\n",
      "Epoch 00037: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:80 , val_loss_mean:1.000475552932563\n",
      "Epoch 00045: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.990\n",
      "Epoch 00059: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00064: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00035: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.054\n",
      "size:85 , val_loss_mean:0.999098368823273\n",
      "Epoch 00036: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00056: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00040: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.993\n",
      "Epoch 00033: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00036: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.051\n",
      "size:90 , val_loss_mean:0.9974389820081926\n",
      "Epoch 00058: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.999\n",
      "Epoch 00071: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.954\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.987\n",
      "Epoch 00047: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:95 , val_loss_mean:0.9948091308396705\n",
      "Epoch 00057: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.995\n",
      "Epoch 00067: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.965\n",
      "Epoch 00040: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.992\n",
      "Epoch 00032: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00026: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:100 , val_loss_mean:0.997403977341019\n"
     ]
    }
   ],
   "source": [
    "sizes= range(15,X.shape[1],5)\n",
    "train_loss,score_loss= testNodeNum(X_scaled,y,sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(15, 104, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npZSKYpoElFIqiv1/XRn3AdGcjBIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f2f0cbfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 30\n"
     ]
    }
   ],
   "source": [
    "print(sizes)\n",
    "loss = (np.mean(score_loss,axis=1))\n",
    "loss_std = (np.std(score_loss,axis=1))\n",
    "plt.plot(sizes,loss)\n",
    "plt.fill_between(sizes,loss-loss_std,loss+loss_std,alpha=0.3)\n",
    "plt.show()\n",
    "print(\"max: {}\".format(sizes[np.argmin(loss,axis=0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def futureTest2(node_sizes, X,y,ori_dates, numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(ori_dates)\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    model = createModel(node_sizes,X.shape[1])\n",
    "    results = None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            print(\"week{}\".format(w))\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "            history = model.fit(X_train,y_train,verbose=0,nb_epoch=500, validation_split=0.1, callbacks=[earlyCallback])\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = model.predict_proba(X_test)\n",
    "            train_proba =model.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            tresult = np.hstack([np.array([w for i in range(proba.shape[0])]).reshape(proba.shape[0],1),\n",
    "                                 ori_dates[start:last].reshape(proba.shape[0],1),stack,proba,y_test])\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "                results =    tresult\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "                results =  np.vstack([results, tresult])\n",
    "            if verbose == True:\n",
    "                print(\"numOftest {} , loss {}\".format(X_test.shape[0],model.evaluate(X_test,y_test)))               \n",
    "                print (tresult)\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "    start = X.shape[0] +index+1\n",
    "    print(\"start compute precision_mat\")\n",
    "    print(X[0:start,:].shape)\n",
    "    print(y[0:start,:].shape)\n",
    "    _,_, proba_test,proba_y = crossValidate2(node_sizes,X[0:start,:],y[0:start,:],fold=10)\n",
    "    p_matrix = precisionMatrix(np.vstack(proba_test),np.vstack(proba_y))\n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    resultdf= pd.DataFrame(results, columns=['week','DayStamp','HomeTeam','AwayTeam','H_prob','D_prob','A_prob','H','D','A'])\n",
    "    return sum_proba, sum_y,resultdf,p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week0\n",
      "Epoch 00102: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1348/1348 [==============================] - 0s     \n",
      "week1\n",
      "Epoch 00023: early stopping\n",
      "5/5 [==============================] - 0s\n",
      "1343/1343 [==============================] - 0s     \n",
      "week2\n",
      "Epoch 00021: early stopping\n",
      "15/15 [==============================] - 0s\n",
      "1328/1328 [==============================] - 0s     \n",
      "week3\n",
      "Epoch 00023: early stopping\n",
      "13/13 [==============================] - 0s\n",
      "1315/1315 [==============================] - 0s     \n",
      "week4\n",
      "Epoch 00029: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1305/1305 [==============================] - 0s     \n",
      "week5\n",
      "Epoch 00023: early stopping\n",
      "12/12 [==============================] - 0s\n",
      "1293/1293 [==============================] - 0s     \n",
      "week6\n",
      "Epoch 00026: early stopping\n",
      "8/8 [==============================] - 0s\n",
      "1285/1285 [==============================] - 0s     \n",
      "week7\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1275/1275 [==============================] - 0s     \n",
      "week8\n",
      "Epoch 00021: early stopping\n",
      "17/17 [==============================] - 0s\n",
      "1258/1258 [==============================] - 0s     \n",
      "week9\n",
      "Epoch 00021: early stopping\n",
      "3/3 [==============================] - 0s\n",
      "1255/1255 [==============================] - 0s     \n",
      "week10\n",
      "Epoch 00023: early stopping\n",
      "11/11 [==============================] - 0s\n",
      "1244/1244 [==============================] - 0s     \n",
      "week11\n",
      "Epoch 00025: early stopping\n",
      "19/19 [==============================] - 0s\n",
      "1225/1225 [==============================] - 0s     \n",
      "week12\n",
      "Epoch 00023: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1215/1215 [==============================] - 0s     \n",
      "week13\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1205/1205 [==============================] - 0s     \n",
      "week14\n",
      "Epoch 00025: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1195/1195 [==============================] - 0s     \n",
      "week15\n",
      "Epoch 00024: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1185/1185 [==============================] - 0s     \n",
      "week16\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1175/1175 [==============================] - 0s     \n",
      "week17\n",
      "Epoch 00025: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1165/1165 [==============================] - 0s     \n",
      "week18\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1155/1155 [==============================] - 0s     \n",
      "week19\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1145/1145 [==============================] - 0s     \n",
      "week20\n",
      "Epoch 00024: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1135/1135 [==============================] - 0s     \n",
      "week21\n",
      "Epoch 00023: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1125/1125 [==============================] - 0s     \n",
      "week22\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1115/1115 [==============================] - 0s     \n",
      "week23\n",
      "Epoch 00023: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1105/1105 [==============================] - 0s     \n",
      "week24\n",
      "Epoch 00024: early stopping\n",
      "8/8 [==============================] - 0s\n",
      "1097/1097 [==============================] - 0s     \n",
      "start compute precision_mat\n",
      "(1096, 104)\n",
      "(1096, 3)\n",
      "Epoch 00032: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [440 237 308], val_loss: 1.007\n",
      "Epoch 00032: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [440 237 308], val_loss: 0.981\n",
      "Epoch 00039: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [440 237 308], val_loss: 0.932\n",
      "Epoch 00034: early stopping\n",
      "110/110 [==============================] - 0s\n",
      "110/110 [==============================] - 0s\n",
      "986/986 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [440 237 309], val_loss: 0.998\n",
      "Epoch 00033: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [440 238 309], val_loss: 0.944\n",
      "Epoch 00036: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 6, Class dist.: [440 238 309], val_loss: 0.994\n",
      "Epoch 00033: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 7, Class dist.: [440 238 309], val_loss: 0.965\n",
      "Epoch 00032: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 8, Class dist.: [440 238 309], val_loss: 0.991\n",
      "Epoch 00030: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 9, Class dist.: [440 238 309], val_loss: 0.980\n",
      "Epoch 00058: early stopping\n",
      "108/108 [==============================] - 0s\n",
      "108/108 [==============================] - 0s\n",
      "988/988 [==============================] - 0s     \n",
      "Fold: 10, Class dist.: [441 238 309], val_loss: 1.055\n",
      "summary\n",
      "score:\n",
      "0.44061302682\n",
      "2like\n",
      "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
      ">80       0.8     1.0         18        6   0.750000          1        1   \n",
      "60-80     0.6     0.8         12       13   0.480000          2        1   \n",
      "50-60     0.5     0.6         14       12   0.538462          1        2   \n",
      "40-50     0.4     0.5         17       24   0.414634          6       11   \n",
      "30-40     0.3     0.4         29       34   0.460317         16       53   \n",
      "20-30     0.2     0.3          9       22   0.290323         24       51   \n",
      "<20       0.0     0.2         10       41   0.196078         18       74   \n",
      "\n",
      "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
      ">80     0.500000         16        5   0.761905  \n",
      "60-80   0.666667          9       15   0.375000  \n",
      "50-60   0.333333          3        6   0.333333  \n",
      "40-50   0.352941          9       17   0.346154  \n",
      "30-40   0.231884         17       37   0.314815  \n",
      "20-30   0.320000         18       38   0.321429  \n",
      "<20     0.195652         12       59   0.169014  \n",
      "sum precision:[ 0.4962406   0.25        0.44318182]\n"
     ]
    }
   ],
   "source": [
    "sum_proba, sum_y,resultdf,p_matrix= futureTest2([55,55],X_scaled,y,X[:,c.dateColumn],numOfWeek=25,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[lower</th>\n",
       "      <th>upper)</th>\n",
       "      <th>h_Correct</th>\n",
       "      <th>h_Wrong</th>\n",
       "      <th>h_Precent</th>\n",
       "      <th>d_Correct</th>\n",
       "      <th>d_Wrong</th>\n",
       "      <th>d_Precent</th>\n",
       "      <th>a_Correct</th>\n",
       "      <th>a_Wrong</th>\n",
       "      <th>a_Precent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&gt;80</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>181</td>\n",
       "      <td>72</td>\n",
       "      <td>0.715415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-60</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>0.514085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>59</td>\n",
       "      <td>0.628931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-50</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>72</td>\n",
       "      <td>83</td>\n",
       "      <td>0.464516</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>102</td>\n",
       "      <td>0.423729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30-40</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>84</td>\n",
       "      <td>129</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>62</td>\n",
       "      <td>152</td>\n",
       "      <td>0.289720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-30</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>55</td>\n",
       "      <td>166</td>\n",
       "      <td>0.248869</td>\n",
       "      <td>222</td>\n",
       "      <td>623</td>\n",
       "      <td>0.262722</td>\n",
       "      <td>55</td>\n",
       "      <td>154</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>17</td>\n",
       "      <td>85</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>22</td>\n",
       "      <td>170</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>41</td>\n",
       "      <td>278</td>\n",
       "      <td>0.128527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
       ">80       0.8     1.0          7        3   0.700000          0        0   \n",
       "60-80     0.6     0.8        181       72   0.715415          0        0   \n",
       "50-60     0.5     0.6         73       69   0.514085          0        0   \n",
       "40-50     0.4     0.5         72       83   0.464516          0        0   \n",
       "30-40     0.3     0.4         84      129   0.394366         20       39   \n",
       "20-30     0.2     0.3         55      166   0.248869        222      623   \n",
       "<20       0.0     0.2         17       85   0.166667         22      170   \n",
       "\n",
       "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
       ">80          NaN          0        0        NaN  \n",
       "60-80        NaN         10        8   0.555556  \n",
       "50-60        NaN        100       59   0.628931  \n",
       "40-50        NaN         75      102   0.423729  \n",
       "30-40   0.338983         62      152   0.289720  \n",
       "20-30   0.262722         55      154   0.263158  \n",
       "<20     0.114583         41      278   0.128527  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findRecordsBy(self,df):\n",
    "    #print((df['DayStamp']).values)\n",
    "    #date = convertToDate((df['DayStamp']).values)\n",
    "    #df[\"Date\"] = date\n",
    "    \n",
    "    home = df['HomeTeam'].values\n",
    "    away = df['AwayTeam'].values\n",
    "    origin = self.df[[\"Date\",\"HomeTeam\",\"AwayTeam\",\"JocH\",\"JocD\",\"JocA\"]]\n",
    "    origin[\"DayStamp\"]=(pd.to_numeric(origin['Date'])/1e9/24/60/60).values\n",
    "    origin[\"DayStamp\"] = origin[\"DayStamp\"].apply(lambda x: \"%.f\"%(float(x)))\n",
    "    df['DayStamp']=df['DayStamp'].apply(lambda x: \"%.f\"%(float(x)))\n",
    "    return origin.merge(df,left_on=['DayStamp',\"HomeTeam\",\"AwayTeam\"],right_on=[\"DayStamp\",\"HomeTeam\",\"AwayTeam\"],how='inner')\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "withodds = findRecordsBy(c,resultdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def formatMatrixs(oddDf , precisionDf):\n",
    "    proba_mat = oddDf[['H_prob','D_prob','A_prob']].values\n",
    "    def locatePrecision(proba_mat, precisionDf):\n",
    "        precisionMat = precisionDf.values\n",
    "        pre_cols=[4,7,10]\n",
    "        def convert(proba, pre_col = 4):\n",
    "            if proba < 0.2:\n",
    "                return proba\n",
    "            for i in range(precisionMat.shape[0]):\n",
    "                if precisionMat[i,0] <= proba and proba < precisionMat[i,1] :\n",
    "                    if math.isnan(precisionMat[i,pre_col]):\n",
    "                        return proba\n",
    "                    else:\n",
    "                        return precisionMat[i,pre_col]\n",
    "        h_fproba = np.array([ convert(float(proba)) for proba in proba_mat[:,0] ] )\n",
    "        d_fproba = np.array([ convert(float(proba),pre_col=7) for proba in proba_mat[:,1] ] )\n",
    "        a_fproba = np.array([ convert(float(proba),pre_col=10) for proba in proba_mat[:,2] ] )\n",
    "        return h_fproba, d_fproba,a_fproba\n",
    "    h_fproba, d_fproba,a_fproba =locatePrecision(proba_mat,precisionDf)\n",
    "    fproba_mat = np.hstack([h_fproba,d_fproba,a_fproba]).reshape(3,h_fproba.shape[0]).T\n",
    "    odd_mat = oddDf[['JocH','JocD','JocA']].values\n",
    "    win_mat = None\n",
    "    if 'H' in oddDf.columns:\n",
    "        win_mat = oddDf[['H','D','A']].values\n",
    "    return fproba_mat,odd_mat,win_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fproba_mat,odd_mat,win_mat= formatMatrixs(withodds,p_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strategy1(fproba_mat,odd_mat,win_mat):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    print(exp)\n",
    "    maxi = np.argmax(exp,axis=1)\n",
    "    print(maxi)\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    for i in range(maxi.shape[0]):\n",
    "        if exp[i,maxi[i]] > 1:\n",
    "            expectation = expectation+ exp[i,maxi[i]]\n",
    "            spent = spent+1\n",
    "            if maxi[i] == y_true[i]:\n",
    "                income = income + odd_mat[i,maxi[i]]\n",
    "            \n",
    "            \n",
    "        \n",
    "    print(\"Spent:{}, Income:{}, expectation:{}\".format(spent,income,expectation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.03339552e+00   1.04675237e+00   8.94297838e-01]\n",
      " [  8.21849315e-01   9.04600812e-01   6.44336531e-01]\n",
      " [  8.96994536e-01   9.36690647e-01   8.16753927e-01]\n",
      " [  1.00178082e+00   8.27063599e-01   8.51716738e-01]\n",
      " [  6.71511628e-01   1.08776978e+00   1.44656652e+00]\n",
      " [  6.96132597e-01   8.14140731e-01   1.36569106e+00]\n",
      " [  1.71044776e+00   6.39047328e-01   6.70557940e-01]\n",
      " [  7.53767123e-01   9.56292287e-01   8.99754539e-01]\n",
      " [  7.98142077e-01   9.06474820e-01   9.73821990e-01]\n",
      " [  8.80813953e-01   7.88294993e-01   9.32832618e-01]\n",
      " [  9.55161290e-01   3.13147581e-01   4.83380637e-01]\n",
      " [  1.91640922e-01   5.21662380e-01   1.16852630e+00]\n",
      " [  1.21575342e+00   7.62449256e-01   5.33796820e-01]\n",
      " [  1.36740331e+00   1.23884892e+00   6.78901734e-01]\n",
      " [  8.65616438e-01   8.27063599e-01   1.12210300e+00]\n",
      " [  9.94475138e-01   9.51798561e-01   8.56820809e-01]\n",
      " [  8.57734807e-01   8.27063599e-01   9.13005780e-01]\n",
      " [  1.37505661e-01   1.86258842e-01   2.04885686e+00]\n",
      " [  7.98208955e-01   7.75562357e-01   2.19266117e+00]\n",
      " [  9.33606557e-01   1.59804258e+00   4.86921009e-01]\n",
      " [  1.58216418e+00   8.39986468e-01   5.26554066e-01]\n",
      " [  7.73219178e-01   9.69215156e-01   1.24377682e+00]\n",
      " [  5.49180328e-01   1.11798561e+00   1.51416309e+00]\n",
      " [  8.75342466e-01   8.27063599e-01   6.65827831e-01]\n",
      " [  9.69253731e-01   1.05967524e+00   1.22541906e+00]\n",
      " [  8.99657534e-01   8.39986468e-01   6.53730836e-01]\n",
      " [  3.80386740e-01   9.17523681e-01   3.36910569e+00]\n",
      " [  1.05523256e+00   9.66906475e-01   6.94892704e-01]\n",
      " [  6.22465753e-01   1.18890392e+00   1.66541582e+00]\n",
      " [  2.72512452e-01   4.53988700e-01   1.17727273e+00]\n",
      " [  5.89226519e-01   8.01217862e-01   1.26416185e+00]\n",
      " [  1.97704918e+00   9.82138024e-01   4.71204188e-01]\n",
      " [  7.01252885e-01   1.05755396e+00   9.92682927e-01]\n",
      " [  1.07615672e+00   5.64165755e-01   1.51416309e+00]\n",
      " [  9.88524590e-01   9.06474820e-01   7.63350785e-01]\n",
      " [  1.41027397e+00   3.57897199e-01   7.13089005e-01]\n",
      " [  9.34838710e-01   2.44513988e-01   6.28706343e-01]\n",
      " [  2.85179987e-01   2.87325945e-01   1.87727273e+00]\n",
      " [  9.70218579e-01   7.88294993e-01   7.69633508e-01]\n",
      " [  1.09753731e+00   6.86168744e-01   1.48712446e+00]\n",
      " [  9.48387097e-01   1.63693526e-01   2.56565912e-01]\n",
      " [  9.51912568e-01   7.54695535e-01   8.10471204e-01]\n",
      " [  8.36438356e-01   8.78755074e-01   1.13562232e+00]\n",
      " [  1.35410448e+00   3.63497608e-01   9.05793991e-01]\n",
      " [  5.20114092e-01   4.51777467e-01   1.04363636e+00]\n",
      " [  1.12604478e+00   9.82138024e-01   3.13354857e-01]\n",
      " [  3.94726338e-01   6.40660628e-01   1.01818182e+00]\n",
      " [  8.23770492e-01   1.39031401e+00   7.57081545e-01]\n",
      " [  6.85684932e-01   1.04675237e+00   1.25185250e+00]\n",
      " [  3.11647685e-01   4.88010579e-01   2.06818182e+00]\n",
      " [  6.46780822e-01   1.13721245e+00   2.00085837e+00]\n",
      " [  7.02950820e-01   9.66906475e-01   9.59871245e-01]\n",
      " [  7.45856354e-01   9.06474820e-01   7.00523560e-01]\n",
      " [  5.47616327e-01   2.29622126e-01   1.17727273e+00]\n",
      " [  8.37209302e-01   9.36690647e-01   6.95598212e-01]\n",
      " [  7.43224044e-01   9.21582734e-01   1.06806283e+00]\n",
      " [  9.76380597e-01   6.34155348e-01   1.21117437e+00]\n",
      " [  8.96994536e-01   9.66906475e-01   6.89484979e-01]\n",
      " [  6.93313953e-01   8.91677943e-01   1.39248927e+00]\n",
      " [  5.75581395e-01   1.32949640e+00   2.09549356e+00]\n",
      " [  1.03339552e+00   2.56110741e-01   1.51416309e+00]\n",
      " [  9.81104651e-01   7.88294993e-01   7.89527897e-01]\n",
      " [  9.02616279e-01   5.80904518e-01   1.02094241e+00]\n",
      " [  4.20103878e-01   6.18247151e-01   1.14545455e+00]\n",
      " [  1.10466418e+00   1.00798376e+00   7.59614448e-01]\n",
      " [  6.14088398e-01   3.70572278e-01   1.62272727e+00]\n",
      " [  2.45354256e-01   1.08552097e+00   8.90909091e-01]\n",
      " [  7.58132958e-02   5.00004508e-01   1.44688863e+00]\n",
      " [  7.36918605e-01   6.32403348e-01   1.41361257e+00]\n",
      " [  7.87808219e-01   9.04600812e-01   9.16679677e-01]\n",
      " [  1.08328358e+00   6.37020170e-01   8.60793816e-01]\n",
      " [  8.53060109e-01   7.88294993e-01   8.79581152e-01]\n",
      " [  1.12063953e+00   7.36603518e-01   8.41884817e-01]\n",
      " [  7.06395349e-01   9.17523681e-01   1.25729614e+00]\n",
      " [  9.09383562e-01   5.05342014e-01   1.13089005e+00]\n",
      " [  1.21823204e+00   3.89631589e-01   9.73636364e-01]\n",
      " [  8.85174419e-01   7.62449256e-01   1.11518325e+00]\n",
      " [  1.61287121e-01   1.27670658e-01   3.06937829e+00]\n",
      " [  7.60405710e-01   8.52909337e-01   1.08292683e+00]\n",
      " [  8.31575342e-01   8.27063599e-01   1.24377682e+00]\n",
      " [  6.73661202e-01   1.57670450e+00   1.02746781e+00]\n",
      " [  8.45303867e-01   5.70589891e-01   1.19121951e+00]\n",
      " [  7.08563536e-01   1.41272921e+00   6.08369099e-01]\n",
      " [  3.34407225e-01   1.70552871e+00   8.11158798e-01]\n",
      " [  1.99383562e+00   8.78755074e-01   4.70472103e-01]\n",
      " [  9.15697674e-01   8.14140731e-01   8.38197425e-01]\n",
      " [  5.45058140e-01   1.25351827e+00   2.50107296e+00]\n",
      " [  8.71366120e-01   8.27063599e-01   8.23036649e-01]\n",
      " [  5.71147541e-01   1.04244604e+00   1.48712446e+00]\n",
      " [  7.92671233e-01   9.43369418e-01   7.90387785e-01]\n",
      " [  7.76162791e-01   1.01223022e+00   1.06802575e+00]\n",
      " [  9.19365672e-01   2.29741203e-01   1.43122288e+00]\n",
      " [  5.00604312e-01   4.36724567e-01   1.27272727e+00]\n",
      " [  8.75342466e-01   9.82014388e-01   3.37894499e-01]\n",
      " [  9.28064516e-01   9.09550488e-02   2.22816122e-01]\n",
      " [  4.64917127e-01   1.39262190e+00   1.01394850e+00]\n",
      " [  1.93850746e+00   9.82014388e-01   2.00374419e-01]\n",
      " [  5.96775956e-01   1.57006081e+00   8.67056707e-01]\n",
      " [  7.82945205e-01   5.57346869e-01   1.21673820e+00]\n",
      " [  5.71220930e-01   1.11136671e+00   2.27124464e+00]\n",
      " [  4.70169378e-01   4.34346958e-01   1.05000000e+00]\n",
      " [  8.19767442e-01   8.39986468e-01   9.86909871e-01]\n",
      " [  7.45856354e-01   5.67389138e-01   1.32357724e+00]\n",
      " [  1.07703488e+00   1.67862546e+00   2.65069789e-02]\n",
      " [  1.33633880e+00   2.32479954e-01   1.14308943e+00]\n",
      " [  8.16448087e-01   9.21582734e-01   9.26701571e-01]\n",
      " [  5.22346156e-01   3.30033470e-01   1.25917685e+00]\n",
      " [  9.51912568e-01   8.76258993e-01   8.16753927e-01]\n",
      " [  1.63517442e+00   8.27063599e-01   5.87434555e-01]\n",
      " [  6.85684932e-01   4.13146099e-01   1.85214592e+00]\n",
      " [  8.33843284e-01   1.13053545e+00   1.96443340e+00]\n",
      " [  1.21935484e+00   2.90106805e-01   1.78560455e-01]\n",
      " [  1.09836066e+00   8.01217862e-01   6.84816754e-01]\n",
      " [  8.38882059e-01   1.08776978e+00   7.25722543e-01]\n",
      " [  1.01991143e-01   1.24971159e-01   1.59812311e+00]\n",
      " [  1.09011628e+00   5.68221164e-01   8.10471204e-01]\n",
      " [  9.76380597e-01   1.11136671e+00   3.60905512e-01]\n",
      " [  8.60753425e-01   8.52909337e-01   6.56655521e-01]\n",
      " [  8.72093023e-01   5.43427213e-01   1.09947644e+00]\n",
      " [  1.38006874e-02   1.03258871e-02   1.92609039e+00]\n",
      " [  8.85174419e-01   9.21582734e-01   9.19313305e-01]\n",
      " [  1.46075581e+00   7.88294993e-01   5.54291845e-01]\n",
      " [  7.02950820e-01   9.36690647e-01   1.00042918e+00]\n",
      " [  2.64921875e-01   3.27873094e+00   3.53095103e-01]\n",
      " [  1.24720149e+00   1.08776978e+00   9.91997821e-02]\n",
      " [  5.31850800e-01   9.95511957e-02   1.32363636e+00]\n",
      " [  9.81104651e-01   9.51798561e-01   7.65193133e-01]\n",
      " [  4.88372093e-01   1.70581867e+00   3.78540773e+00]\n",
      " [  8.78688525e-01   7.62449256e-01   8.73298429e-01]\n",
      " [  1.24480874e+00   8.52909337e-01   6.06282723e-01]\n",
      " [  8.08011050e-01   7.88294993e-01   1.26341463e+00]\n",
      " [  8.55890411e-01   9.51798561e-01   6.33354516e-01]\n",
      " [  9.55000000e-01   6.93945902e-01   1.33838864e+00]\n",
      " [  9.59302326e-01   8.52909337e-01   8.79581152e-01]\n",
      " [  8.15406977e-01   8.27063599e-01   1.17801047e+00]\n",
      " [  3.36839276e-01   6.08010713e-01   1.15818182e+00]\n",
      " [  5.71823204e-01   9.06474820e-01   9.04712042e-01]\n",
      " [  2.62886292e-01   3.29362514e-01   1.65454545e+00]\n",
      " [  8.19767442e-01   8.39986468e-01   1.14659686e+00]\n",
      " [  9.14516129e-01   4.12867550e-02   1.98349910e-01]\n",
      " [  1.36835821e+00   8.91677943e-01   1.43820427e-01]\n",
      " [  5.88424658e-01   2.09989834e+00   4.77747994e-01]\n",
      " [  8.67704918e-01   9.21582734e-01   8.57591623e-01]\n",
      " [  1.07273224e+00   2.40450630e-01   1.29349593e+00]\n",
      " [  5.62500000e-01   1.13721245e+00   2.40643777e+00]\n",
      " [  9.19365672e-01   7.58491150e-01   1.12789330e+00]\n",
      " [  3.72440835e-02   9.47863490e-03   1.44707633e+00]\n",
      " [  1.74608209e+00   9.21582734e-01   1.20295787e-01]\n",
      " [  1.83761191e-02   2.64767806e+00   3.12102716e-01]\n",
      " [  6.76243094e-01   1.39656712e+00   7.06806283e-01]\n",
      " [  5.88424658e-01   9.80976135e-01   2.70386266e+00]\n",
      " [  1.07703488e+00   5.20977542e-01   1.22670520e+00]\n",
      " [  1.15327869e+00   5.82327513e-01   9.73872832e-01]\n",
      " [  9.14246575e-01   6.15072458e-01   9.46351931e-01]\n",
      " [  9.01644878e-02   1.67874487e-01   1.39238843e+00]\n",
      " [  3.21011487e-01   6.61458778e-01   1.08181818e+00]\n",
      " [  8.50290698e-01   8.39986468e-01   9.19313305e-01]\n",
      " [  4.17907541e-01   2.69245367e+00   1.85551586e-02]\n",
      " [  2.37645349e+00   6.88521758e-01   6.92947977e-01]\n",
      " [  5.14534884e-01   1.69208633e+00   2.11249776e+00]\n",
      " [  8.94193548e-01   9.10416106e-02   1.02643023e+00]\n",
      " [  9.19365672e-01   1.42014388e+00   3.90208983e-01]\n",
      " [  5.19890710e-01   1.19352518e+00   1.99476440e+00]\n",
      " [  8.28488372e-01   8.39986468e-01   9.73390558e-01]\n",
      " [  8.67704918e-01   8.39986468e-01   8.16753927e-01]\n",
      " [  1.46094763e-01   2.29402950e-01   1.35498400e+00]\n",
      " [  6.56506849e-01   7.75505200e-01   1.96030043e+00]\n",
      " [  3.12508962e-01   1.93470579e+00   6.21888412e-01]\n",
      " [  9.95806452e-01   2.20081717e-01   2.70304743e-01]\n",
      " [  1.96451613e+00   2.18196591e-01   2.31163478e-01]\n",
      " [  2.43119490e-01   7.07718101e-01   2.86363636e+00]\n",
      " [  6.33977901e-01   1.71763033e+00   4.54450054e-01]\n",
      " [  1.61620302e-01   7.62449256e-01   1.64181818e+00]\n",
      " [  5.34535519e-01   4.99109919e-01   2.76242775e+00]\n",
      " [  1.01914179e+00   3.03913258e-01   1.62231760e+00]\n",
      " [  8.68865753e-02   2.13363158e-01   1.31686422e+00]\n",
      " [  2.67957864e-01   2.10250926e-01   1.93464269e+00]\n",
      " [  9.22529302e-02   2.10837483e-01   1.61495983e+00]\n",
      " [  1.24645161e+00   1.98217381e-01   1.22647891e-01]\n",
      " [  1.11179104e+00   2.80117504e-01   1.52356021e+00]\n",
      " [  6.36627907e-01   9.69215156e-01   1.94764398e+00]\n",
      " [  1.00258065e+00   2.92420697e-01   5.71834257e-01]\n",
      " [  8.87419355e-01   2.15686502e-01   4.45616917e-02]\n",
      " [  1.24692010e+00   3.63485859e-01   8.52727273e-01]\n",
      " [  7.48904110e-01   6.67449895e-01   1.29785408e+00]\n",
      " [  5.71147541e-01   9.56292287e-01   1.57068063e+00]\n",
      " [  4.50327869e-01   1.22767253e+00   5.15028902e+00]\n",
      " [  1.16712329e+00   1.45483962e+00   4.79532979e-02]\n",
      " [  1.20580110e+00   1.69298437e+00   4.29914163e-01]\n",
      " [  1.33633880e+00   8.39986468e-01   8.80231214e-01]\n",
      " [  1.21156716e+00   3.32882882e-01   5.79300523e-01]\n",
      " [  9.02616279e-01   9.97122302e-01   8.24678112e-01]\n",
      " [  1.01598837e+00   9.66906475e-01   7.30042918e-01]\n",
      " [  4.83278689e-01   1.12428958e+00   2.51308901e+00]\n",
      " [  6.32267442e-01   1.19352518e+00   1.59527897e+00]\n",
      " [  9.34838710e-01   4.73811065e-01   1.60080189e-01]\n",
      " [  9.07458564e-01   1.40130129e+00   5.90575916e-01]\n",
      " [  1.55806452e+00   2.49231873e-04   8.58008229e-04]\n",
      " [  1.60478805e-02   3.16636171e-02   1.55143930e+00]\n",
      " [  5.16695956e-02   3.84867281e-02   2.18171324e+00]\n",
      " [  8.48097015e-01   5.69696929e-01   2.83905579e+00]\n",
      " [  6.95628415e-01   9.82014388e-01   9.73390558e-01]\n",
      " [  1.04068493e+00   9.66906475e-01   2.53780581e-01]\n",
      " [  1.32559701e+00   9.97122302e-01   1.78083093e-01]\n",
      " [  1.30813953e+00   8.39986468e-01   5.73218884e-01]\n",
      " [  8.96994536e-01   9.51798561e-01   6.97596567e-01]\n",
      " [  4.06700091e-01   4.02966645e-01   1.60363636e+00]\n",
      " [  6.83701657e-01   5.09227797e-01   1.52727273e+00]\n",
      " [  6.77322404e-01   9.82014388e-01   1.02746781e+00]\n",
      " [  4.89779006e-01   9.36690647e-01   1.11518325e+00]\n",
      " [  1.35464481e+00   5.12579687e-01   8.42774566e-01]\n",
      " [  1.43962687e+00   1.48283965e-01   8.38197425e-01]\n",
      " [  9.55000000e-01   3.23276963e-01   9.60548416e-01]\n",
      " [  4.62430939e-01   2.23629055e+00   2.62862159e-01]\n",
      " [  1.59193548e+00   4.22450248e-02   2.69058820e-01]\n",
      " [  5.78794826e-03   1.19922328e-02   1.45414041e+00]\n",
      " [  1.58216418e+00   2.27860844e-01   7.62489270e-01]\n",
      " [  1.11774194e+00   7.08793098e-01   7.46402484e-03]\n",
      " [  8.46774194e-01   4.15042508e-03   9.52196633e-02]\n",
      " [  1.28633721e+00   5.80225754e-01   6.81675393e-01]\n",
      " [  8.89931507e-01   1.98124890e-01   1.14659686e+00]\n",
      " [  8.69477612e-01   1.34397835e+00   1.05636127e+00]\n",
      " [  6.88306011e-01   8.01217862e-01   1.20942408e+00]\n",
      " [  3.38475084e-02   2.72890878e-01   2.22618645e+00]\n",
      " [  9.07978142e-01   9.82014388e-01   7.79057592e-01]\n",
      " [  6.51693989e-01   1.02733813e+00   1.22513089e+00]\n",
      " [  7.85806452e-01   5.16097235e-03   8.99702269e-02]\n",
      " [  6.07759563e-01   8.78755074e-01   2.15375723e+00]\n",
      " [  7.23837209e-01   1.45023282e+00   4.35141271e-01]\n",
      " [  4.84806630e-01   1.74854800e+00   5.79523471e-01]\n",
      " [  5.64364641e-01   1.55447647e+00   7.84120172e-01]\n",
      " [  1.28633721e+00   1.58360463e+00   2.37734147e-01]\n",
      " [  1.51089552e+00   4.20896562e-01   4.22614129e-01]\n",
      " [  7.40883978e-01   9.36690647e-01   6.91099476e-01]\n",
      " [  3.29293566e-01   4.52305037e-01   1.84545455e+00]\n",
      " [  1.51939891e+00   7.96070185e-02   1.05636364e+00]\n",
      " [  7.50546448e-01   9.97122302e-01   9.73821990e-01]\n",
      " [  1.77500000e+00   1.46931089e-01   5.68586387e-01]\n",
      " [  1.05523256e+00   9.51798561e-01   4.56004996e-01]\n",
      " [  7.65483871e-01   1.72685177e-01   1.96131371e+00]\n",
      " [  2.22383721e+00   1.82276691e-01   9.14471545e-01]\n",
      " [  5.05245902e-01   1.23884892e+00   2.16753927e+00]\n",
      " [  1.31847015e+00   8.39986468e-01   2.17787395e-01]\n",
      " [  1.09041045e+00   9.56292287e-01   4.14368177e-01]\n",
      " [  1.09836066e+00   9.21582734e-01   6.00257511e-01]\n",
      " [  7.71802326e-01   8.91677943e-01   1.05450644e+00]\n",
      " [  1.62492537e+00   9.97122302e-01   6.86004929e-02]\n",
      " [  7.61530055e-01   9.21582734e-01   1.03664921e+00]\n",
      " [  5.78469945e-01   1.08776978e+00   1.53926702e+00]\n",
      " [  1.80405645e-03   1.83662510e+00   1.47485549e+00]\n",
      " [  8.26716418e-01   8.23329985e-01   3.24463519e+00]\n",
      " [  2.48495970e-02   6.60106134e-04   2.78893182e+00]\n",
      " [  1.65343284e+00   6.15739717e-01   3.40222833e-02]\n",
      " [  1.46100746e+00   2.93872774e-01   9.89528796e-01]\n",
      " [  3.65085643e-01   1.43844364e-01   1.51902822e+00]\n",
      " [  1.16516129e+00   2.90539179e-03   7.48058494e-02]\n",
      " [  4.96400194e-03   5.28669753e-01   1.83152432e+00]\n",
      " [  7.99354839e-01   2.04358448e-04   5.42034303e-03]\n",
      " [  1.40956284e+00   8.65832206e-01   8.42774566e-01]\n",
      " [  1.39110927e-01   1.62740107e+00   8.90406504e-01]\n",
      " [  1.28283582e+00   4.51823559e-01   1.08154506e+00]]\n",
      "[1 1 1 0 2 2 0 1 2 2 0 2 0 0 2 0 2 2 2 1 0 2 2 0 2 0 2 0 2 2 2 0 1 2 0 0 0\n",
      " 2 0 2 0 0 2 0 2 0 2 1 2 2 2 1 1 2 1 2 2 1 2 2 2 0 2 2 0 2 1 2 2 2 0 2 0 2\n",
      " 2 0 2 2 2 2 1 2 1 1 0 0 2 0 2 1 2 2 2 1 0 1 0 1 2 2 2 2 2 1 0 2 2 0 0 2 2\n",
      " 0 0 1 2 0 1 0 2 2 1 0 2 1 0 2 0 2 0 0 2 1 2 0 2 2 1 2 2 0 0 1 1 2 2 2 2 0\n",
      " 1 1 2 2 0 2 2 2 2 1 0 2 2 1 2 2 0 2 2 1 0 0 2 1 2 2 2 2 2 2 0 2 2 0 0 0 2\n",
      " 2 2 1 1 0 0 1 0 2 2 0 1 0 2 2 2 1 0 0 0 1 2 2 2 2 0 0 2 1 0 2 0 0 0 0 2 1\n",
      " 2 2 1 2 0 2 1 1 1 1 0 1 2 0 1 0 0 2 0 2 0 0 0 2 0 2 2 1 2 2 0 0 2 0 2 0 0\n",
      " 1 0]\n",
      "Spent:205, Income:258.19, expectation:322.188139700667\n"
     ]
    }
   ],
   "source": [
    "strategy1(fproba_mat,odd_mat,win_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy3(fproba_mat,odd_mat,win_mat,info, z = 0.5  ):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    total = fproba_mat.shape[0]\n",
    "    withdraw = 0\n",
    "    buy = 0\n",
    "    homeTeam = info[\"HomeTeam\"].values\n",
    "    awayTeam = info[\"AwayTeam\"].values\n",
    "    receipt = []\n",
    "    def chooseFrom(match, chosable):\n",
    "        max_prob = -1\n",
    "        c = 0\n",
    "        for choice in chosable:\n",
    "            if fproba_mat[match,choice] > max_prob:\n",
    "                c = choice\n",
    "                max_prob = fproba_mat[match,choice]\n",
    "        return c\n",
    "    for match in range(exp.shape[0]):\n",
    "        chosable = []\n",
    "        for choose in range(3):\n",
    "            if exp[match,choose]> 1 and fproba_mat[match,choose] >= z:\n",
    "                chosable.append(choose)\n",
    "                \n",
    "        if len(chosable) == 0:\n",
    "            withdraw= withdraw+1\n",
    "            continue\n",
    "        choice = chooseFrom(match,chosable)\n",
    "        expectation = expectation+ exp[match,choice]\n",
    "        spent = spent+1\n",
    "        buy =buy +1 \n",
    "        receipt.append(np.hstack([homeTeam[match],awayTeam[match], odd_mat[match,choice],choice,y_true[match],fproba_mat[match,:]]))\n",
    "        if choice == y_true[match]:\n",
    "            income = income + odd_mat[match,choice]\n",
    "    print(buy)\n",
    "    print(\"Spent:{}, Income:{}, expectation:{}, withdraw:{}(total:{})\".\n",
    "          format(spent,income,expectation,withdraw,total))\n",
    "    receiptDf = pd.DataFrame(receipt,columns=[\"home\",\"away\",\"odd of choice\",\"choice\",\"result\",\"Hp\",\"Dp\",\"Ap\"])\n",
    "    return spent,income,expectation,withdraw,total,receiptDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "Spent:90, Income:91.14999999999998, expectation:139.70355912289673, withdraw:171(total:261)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy3(fproba_mat,odd_mat,win_mat,withodds,z=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "receipt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_range = range(30,80,2)\n",
    "spents = []\n",
    "incomes = []\n",
    "exps = []\n",
    "withs = []\n",
    "for z in z_range:\n",
    "    fz = z/100\n",
    "    print(z)\n",
    "    spent,income,expectation,withdraw,total,_ = strategy3(fproba_mat,odd_mat,win_mat,withodds,z=fz) \n",
    "    spents.append(spent)\n",
    "    incomes.append(income)\n",
    "    exps.append(expectation)\n",
    "    withs.append(withdraw)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(z_range, spents)\n",
    "plt.plot(z_range, incomes,color='green')\n",
    "plt.plot(z_range, np.array(incomes)/np.array(spents)*100,color='red')\n",
    "plt.plot(z_range,exps,color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy4(fproba_mat,odd_mat,win_mat,info, z = 0.2 ,e =1.8 ):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    y_true=None\n",
    "    if win_mat is None:\n",
    "        y_true = np.zeros(shape=(fproba_mat.shape[0],))\n",
    "    else:\n",
    "        y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    total = fproba_mat.shape[0]\n",
    "    withdraw = 0\n",
    "    buy = 0\n",
    "    homeTeam = info[\"HomeTeam\"].values\n",
    "    awayTeam = info[\"AwayTeam\"].values\n",
    "    receipt = []\n",
    "    def chooseFrom(match, chosable):\n",
    "        max_prob = -1\n",
    "        c = 0\n",
    "        for choice in chosable:\n",
    "            if fproba_mat[match,choice] > max_prob:\n",
    "                c = choice\n",
    "                max_prob = fproba_mat[match,choice]\n",
    "        return c\n",
    "    for match in range(exp.shape[0]):\n",
    "        chosable = []\n",
    "        for choose in range(3):\n",
    "            if exp[match,choose]> e and fproba_mat[match,choose] >= z:\n",
    "                chosable.append(choose)\n",
    "                \n",
    "        if len(chosable) == 0:\n",
    "            withdraw= withdraw+1\n",
    "            continue\n",
    "        choice = chooseFrom(match,chosable)\n",
    "        expectation = expectation+ exp[match,choice]\n",
    "        spent = spent+1\n",
    "        buy =buy +1 \n",
    "        receipt.append(np.hstack([homeTeam[match],awayTeam[match], odd_mat[match,choice],choice,y_true[match],fproba_mat[match,:]]))\n",
    "        if choice == y_true[match]:\n",
    "            income = income + odd_mat[match,choice]\n",
    "    #print(buy)\n",
    "    #print(\"Spent:{}, Income:{}, expectation:{}, withdraw:{}(total:{})\".\n",
    "    #      format(spent,income,expectation,withdraw,total))\n",
    "    receiptDf = pd.DataFrame(receipt,columns=[\"home\",\"away\",\"odd of choice\",\"choice\",\"result\",\"Hp\",\"Dp\",\"Ap\"])\n",
    "    return spent,income,expectation,withdraw,total,receiptDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,win_mat,withodds,z=0.2,e=1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_z_range = np.arange(0,1,0.1)\n",
    "g_e_range = np.arange(0,2,0.1)\n",
    "g_incomes = []\n",
    "g_spents =[]\n",
    "g_withdraws=[]\n",
    "g_exps=[]\n",
    "g_total = 0\n",
    "for e in g_e_range:\n",
    "    for z in g_z_range:\n",
    "        spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,win_mat,withodds,z=z,e=e)\n",
    "        g_spents.append(spent)\n",
    "        g_incomes.append(income)\n",
    "        g_withdraws.append(withdraw)\n",
    "        g_exps.append(expectation)\n",
    "        g_total= total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f235cf320>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEKCAYAAADDzOROAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMyuRQ9jMrET/H4RQRFlKR2lvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f24c73048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWMAAAEKCAYAAADHOTRzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f2ea9b630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEKCAYAAADDzOROAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f24621278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f235cf320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "g_n_incomes = np.array(g_incomes).reshape(len(g_e_range),len(g_z_range))\n",
    "g_n_spent = np.array(g_spents).reshape(len(g_e_range),len(g_z_range))\n",
    "g_n_withdraws = np.array(g_withdraws).reshape(len(g_e_range),len(g_z_range))\n",
    "g_n_exps = np.array(g_exps).reshape(len(g_e_range),len(g_z_range))\n",
    "Z, E =np.meshgrid(g_z_range,g_e_range)\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_incomes)\n",
    "plt.title(\"Income\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_incomes-g_n_spent)\n",
    "plt.title(\"Net Income\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_withdraws)\n",
    "plt.title(\"Withdraws\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_exps)\n",
    "plt.title(\"Exps\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_node_sizes=[55,55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPrecisionMaxtrix(node_sizes,c):\n",
    "    X, y = c.getH7(removeInsufficient=True)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    _,_, proba_test,proba_y = crossValidate2(node_sizes,X_scaled,y,fold=10)\n",
    "    p_matrix = precisionMatrix(np.vstack(proba_test),np.vstack(proba_y)) \n",
    "    return p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "Epoch 00087: early stopping\n",
      "137/137 [==============================] - 0s     \n",
      "137/137 [==============================] - 0s     \n",
      "1221/1221 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [539 298 384], val_loss: 1.028\n",
      "Epoch 00101: early stopping\n",
      "137/137 [==============================] - 0s     \n",
      "137/137 [==============================] - 0s     \n",
      "1221/1221 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [539 298 384], val_loss: 1.030\n",
      "Epoch 00087: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [539 299 384], val_loss: 0.948\n",
      "Epoch 00090: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [539 299 384], val_loss: 0.963\n",
      "Epoch 00097: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [539 299 384], val_loss: 1.015\n",
      "Epoch 00074: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 6, Class dist.: [539 299 384], val_loss: 1.001\n",
      "Epoch 00046: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 7, Class dist.: [539 299 384], val_loss: 0.984\n",
      "Epoch 00060: early stopping\n",
      "135/135 [==============================] - 0s     \n",
      "135/135 [==============================] - 0s     \n",
      "1223/1223 [==============================] - 0s     \n",
      "Fold: 8, Class dist.: [539 299 385], val_loss: 1.052\n",
      "Epoch 00037: early stopping\n",
      "135/135 [==============================] - 0s     \n",
      "135/135 [==============================] - 0s     \n",
      "1223/1223 [==============================] - 0s     \n",
      "Fold: 9, Class dist.: [539 299 385], val_loss: 1.034\n",
      "Epoch 00027: early stopping\n",
      "134/134 [==============================] - 0s     \n",
      "134/134 [==============================] - 0s     \n",
      "1224/1224 [==============================] - 0s     \n",
      "Fold: 10, Class dist.: [540 299 385], val_loss: 1.027\n"
     ]
    }
   ],
   "source": [
    "g_p_matrix= getPrecisionMaxtrix(g_node_sizes,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainedModel(node_sizes,c):\n",
    "    X, y = c.getH7(removeInsufficient=True)\n",
    "    X_scaled = StandardScaler().fit_transform(X) \n",
    "    model = createModel(node_sizes,X_scaled.shape[1])\n",
    "    earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "    history = model.fit(X_scaled,y,verbose=0,nb_epoch=500, validation_split=0.1, callbacks=[earlyCallback])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "Epoch 00092: early stopping\n"
     ]
    }
   ],
   "source": [
    "g_model = getTrainedModel(g_node_sizes,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def predictFuture(node_sizes,c,p_matrix,model):\n",
    "    X_test,y_test = c.getH7(removeInsufficient = True,future=True)\n",
    "    X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "    proba_y = model.predict_proba(X_test_scaled)\n",
    "   \n",
    "    def getTestDf (X_test,proba_y,c):\n",
    "        decoded = oneHotDecode(c, X_test)\n",
    "        homeNames = c.inverseTeamMapping(decoded[:,0])\n",
    "        awayNames = c.inverseTeamMapping(decoded[:,1])\n",
    "        names = np.array([homeNames,awayNames]).T\n",
    "        return pd.DataFrame(np.hstack([names,proba_y]),columns=['HomeTeam','AwayTeam','H_prob','D_prob','A_prob'])\n",
    "   \n",
    "    test_df = getTestDf(X_test,proba_y,c)\n",
    "    test_df = test_df.sort(columns=\"HomeTeam\")\n",
    "    originDf = c.df[c.df[\"Future\"]==1].sort(columns=\"HomeTeam\")\n",
    "    test_df['JocH']=originDf['JocH'].values\n",
    "    test_df['JocD']=originDf['JocD'].values\n",
    "    test_df['JocA']=originDf['JocA'].values\n",
    "    fproba_mat,odd_mat,_= formatMatrixs(test_df,p_matrix)\n",
    "    return fproba_mat,odd_mat,test_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "7/7 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:15: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:16: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "fproba_mat,odd_mat,test_df=predictFuture(g_node_sizes,c,g_p_matrix,g_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Spent:1, Income:0, expectation:1.6114285714285712, withdraw:6(total:7)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,None,test_df,z=0.2,e=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "      <th>odd of choice</th>\n",
       "      <th>choice</th>\n",
       "      <th>result</th>\n",
       "      <th>Hp</th>\n",
       "      <th>Dp</th>\n",
       "      <th>Ap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2436548223350254</td>\n",
       "      <td>0.24871794871794872</td>\n",
       "      <td>0.5714285714285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        home       away odd of choice choice result                  Hp  \\\n",
       "0  Liverpool  Tottenham          2.82      2    0.0  0.2436548223350254   \n",
       "\n",
       "                    Dp                  Ap  \n",
       "0  0.24871794871794872  0.5714285714285714  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipt.to_csv(\"2016-4-3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
