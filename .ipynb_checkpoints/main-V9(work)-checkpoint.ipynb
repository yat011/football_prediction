{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    from IPython.core.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "import sys\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotCurve(train_mean, train_std,test_mean,test_std,sizes):\n",
    "    plt.plot(sizes, train_mean, \n",
    "            color='blue', marker='o', \n",
    "            markersize=5, \n",
    "            label='training accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                  train_mean + train_std,\n",
    "                   train_mean - train_std, alpha=0.15, color='blue')\n",
    "\n",
    "    plt.plot(sizes, test_mean, \n",
    "              color='green', linestyle='--', \n",
    "              marker='s', markersize=5, \n",
    "             label='validation accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                      test_mean + test_std,\n",
    "                     test_mean - test_std, \n",
    "                    alpha=0.15, color='green')\n",
    "    plt.xlabel('x_range')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "def likehoodScore(proba,y):\n",
    "    return np.sum(proba * y)/proba.shape[0]\n",
    "\n",
    "def firstNScore(n, pred, y):\n",
    "    backup = np.array(pred, copy =True)\n",
    "    for r in range(pred.shape[0]):\n",
    "        row = backup[r]\n",
    "        s = np.sort(row)\n",
    "        for c in range(pred.shape[1]):\n",
    "            temp = backup[r][c]\n",
    "            backup[r][c] = False\n",
    "            for j in range(1,n+1):\n",
    "                if temp == s[-j]:\n",
    "                    backup[r][c] = True\n",
    "                    break\n",
    "    res = np.sum(np.logical_and(backup,y))/pred.shape[0]\n",
    "    return res               \n",
    "\n",
    "def oneHotDecode(self, X_sample):\n",
    "    result=None\n",
    "    fiPos = 0\n",
    "    colIndex = 0\n",
    "    while colIndex < X_sample.shape[1]:\n",
    "        if fiPos < len(self.ohe.n_values_) and colIndex == self.ohe.feature_indices_[fiPos]:                \n",
    "            start = self.ohe.feature_indices_[fiPos]\n",
    "            end_ = start+ self.ohe.n_values_[fiPos]\n",
    "            #print(\"start{} end{}\".format(start,end_))\n",
    "            classes = np.argmax(X_sample[:,start:end_],axis=1).reshape(X_sample.shape[0],1)\n",
    "            if result is None:\n",
    "                result = classes\n",
    "            else:\n",
    "                result=np.hstack([result,classes])\n",
    "            colIndex = end_\n",
    "            fiPos = fiPos +1\n",
    "        else:\n",
    "            if result is None:\n",
    "                result = X_sample[:,colIndex:colIndex+1]\n",
    "            else:\n",
    "                result=np.hstack([result, X_sample[:,colIndex:colIndex+1]])\n",
    "            colIndex = colIndex +1\n",
    "        \n",
    "    return result \n",
    "def convertToDate(dayStamps):\n",
    "    res = [] \n",
    "    for v in dayStamps:\n",
    "        res.append(datetime.datetime.fromtimestamp(float(v)*24*60*60))\n",
    "    return res\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def precisionMatrix(proba, y):\n",
    "    def _precisionClassify(df,proba, wins, c =0 ):\n",
    "        for indx, v in enumerate(proba):\n",
    "            row = 0\n",
    "            col = 0\n",
    "            if wins[indx] == c:\n",
    "                col = 0\n",
    "            else:\n",
    "                col =1\n",
    "            if v <0.2:\n",
    "                row =6 \n",
    "            elif v < 0.3 and  v >=0.2:\n",
    "                row =5 \n",
    "            elif v < 0.4 and v >= 0.3:\n",
    "                row = 4 \n",
    "            elif v < 0.5 and v >= 0.4:\n",
    "                row = 3 \n",
    "            elif v < 0.6 and v >= 0.5:\n",
    "                row = 2 \n",
    "            elif v < 0.8 and v >= 0.6:\n",
    "                row = 1\n",
    "            df.iloc[row,col] = df.iloc[row,col]+1 \n",
    "        df[df.columns[2]] = df[df.columns[0]] /(df[df.columns[1]] + df[df.columns[0]])\n",
    "        return df\n",
    "    rowHeader = ['>80','60-80','50-60','40-50','30-40','20-30','<20']\n",
    "    df = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['h_Correct', 'h_Wrong','h_Precent'])\n",
    "    hproba = proba[:,0]\n",
    "    wins = np.argmax(y,axis=1)\n",
    "    df = _precisionClassify(df,hproba,wins)\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['d_Correct', 'd_Wrong','d_Precent'])\n",
    "    dproba = proba[:,1]\n",
    "    df = df.join(_precisionClassify(temp,dproba,wins,c=1))\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['a_Correct', 'a_Wrong','a_Precent'])\n",
    "    aproba = proba[:,2]\n",
    "    df = df.join(_precisionClassify(temp,aproba,wins,c=2))\n",
    "    \n",
    "    bound = pd.DataFrame(np.array([[0.8,1.0],[0.6,0.8],[0.5,0.6],[0.4,0.5],[0.3,0.4],[0.2,0.3],[0,0.2]] )\n",
    "                                ,index=rowHeader, columns=['[lower', 'upper)'])\n",
    "            \n",
    "    return bound.join(df)\n",
    "       \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "def futureTest(mlp, X,y,numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(X[:,c.dateColumn])\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            mlp.fit(X_train,y_train)\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = mlp.predict_proba(X_test)\n",
    "            train_proba =mlp.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "            if verbose == True:\n",
    "                print(\"week{}\".format(w))\n",
    "                print(\"numOftest {} , score {}\".format(X_test.shape[0],mlp.score(X_test,y_test)))\n",
    "                print(np.hstack([stack[errorIndx],proba[errorIndx],y_test[errorIndx]]))\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "        \n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    return firstNScore(1,sum_train_proba,sum_train_y), score, like2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/scikit-learn/sklearn/cross_validation.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "class FootballDataHelper:\n",
    "    def __init__ (self, recentNum=5):\n",
    "        self.win_mapping = {'H':0, 'D':1,'A':2}\n",
    "        self.recentNum = recentNum\n",
    "        self.df = None\n",
    "        self.teamsData={}\n",
    "        self.session = 0\n",
    "        #self.hiddensCount = 2\n",
    "       \n",
    "    def addColumns(self,df, addition):    \n",
    "        dates = df[\"Date\"].drop_duplicates().values\n",
    "        col_adds = []\n",
    "        for colAdd in addition.columns:\n",
    "             if colAdd not in df.columns:\n",
    "                    df[colAdd]=np.zeros(shape=(df.shape[0],))\n",
    "                    col_adds.append(colAdd)\n",
    "        for date in dates:\n",
    "            dateAddition= addition[addition['Date'] == date].sort(columns='HomeTeam')\n",
    "            dateDf  = df [df['Date']==date].sort(columns='HomeTeam')\n",
    "            for col in col_adds:\n",
    "                dateDf[col] = dateAddition[col].values\n",
    "            df.update(dateDf)\n",
    "        return df\n",
    "            \n",
    "    def saveDf(self,filename):\n",
    "        self.df.to_csv(filename,index=False)\n",
    "    def loadDf(self,filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])    \n",
    "        self.df = df\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "    def readFootBallData(self,year): \n",
    "        filename = \"dataSet/E{}.csv\".format(year)\n",
    "        df = pd.read_csv(filename)\n",
    "        #df = df.drop(df.columns[range(23,df.shape[1])], axis=1)\n",
    "        #df = df.drop(\"Div\",axis=1)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "        df['session'] = pd.Series(np.ones(shape=(df.shape[0],))*self.session, index=df.index)\n",
    "        self.session = self.session +1\n",
    "        \n",
    "        matchDetail = pd.read_csv(\"dataSet/match{}.csv\".format(year))\n",
    "        matchDetail['Date'] =pd.to_datetime(matchDetail['Date'])\n",
    "        df = self.addColumns(df,matchDetail)\n",
    "        \n",
    "        df[\"Future\"] = np.zeros(shape=(df.shape[0],))\n",
    "        \n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = pd.concat([self.df,df])\n",
    "            \n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "    def readFuture(self):\n",
    "        filename = \"dataSet/future.csv\"\n",
    "        df = pd.read_csv(filename)\n",
    "        #df = df.drop(df.columns[range(23,df.shape[1])], axis=1)\n",
    "        #df = df.drop(\"Div\",axis=1)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df[\"Future\"] = np.ones(shape=(df.shape[0],))\n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = pd.concat([self.df,df])\n",
    "            \n",
    "        self.df['HTR']=self.df['HTR'].fillna('D')\n",
    "        self.df = self.df.fillna(0)\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "        \n",
    "    def getTeam(self,dataFrame, teamName):       \n",
    "        return dataFrame[(dataFrame[\"HomeTeam\"] == teamName) | (dataFrame[\"AwayTeam\"] == teamName)]\n",
    " \n",
    "        \n",
    "    def previousRecords(self,team, date , recentNum):\n",
    "        prev = team[( team[\"Date\"] < date)]\n",
    "        \n",
    "        if prev.shape[0] < recentNum :\n",
    "            #print(\"less than min Num\")\n",
    "            return None\n",
    "        else:\n",
    "            return prev.iloc[-recentNum:]\n",
    "    def readPredict (self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "  \n",
    "    def inverseTeamMapping (self, col):\n",
    "        inverseMap ={}\n",
    "        for name in self.teamsMap.keys():        \n",
    "            inverseMap[self.teamsMap[name]] = name\n",
    "        res =[]\n",
    "        for idex, v in enumerate(col):\n",
    "            res.append(inverseMap[v])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def readTeamMatch(self, teamName):\n",
    "        df = pd.read_csv('teams/'+teamName+'.csv')\n",
    "        df['1'] = pd.to_datetime(df['1'],yearfirst=True)\n",
    "        #df['1']= (pd.to_numeric(df['1'])/1e9/24/60/60)\n",
    "        self.teamsData[teamName]=df.sort(['1'],ascending=[False])\n",
    "        self.teamsById[self.teamsMap[teamName]]=self.teamsData[teamName]\n",
    "    \n",
    "    def commonMapping(self, X):\n",
    "        X['HomeTeam'] = X['HomeTeam'].map(self.teamsMap)\n",
    "        X['AwayTeam'] = X['AwayTeam'].map(self.teamsMap)\n",
    "        X['Referee']=X['Referee'].map(self.refereesMap).fillna(0)\n",
    "        X['HTR'] = X['HTR'].map(self.win_mapping)\n",
    "        X['FTR'] = X['FTR'].map(self.win_mapping)\n",
    "        return X\n",
    "    def initData(self, X, target,encode):\n",
    "        X  = X.sort_values(by=\"Date\")\n",
    "        isInput = False\n",
    "        if target is None:\n",
    "            target =X      \n",
    "        else:\n",
    "            if self.ohe is None:\n",
    "                raise Exception(\"Not yet get train data\")\n",
    "            isInput = True\n",
    "            if encode == True:\n",
    "                target = self.commonMapping(target)\n",
    "        y=None\n",
    "        if encode == True:    \n",
    "            X =self.commonMapping(X)\n",
    "            y = []\n",
    "            for v in target['FTR']:\n",
    "                y.append(range(3)==v)\n",
    "        else:\n",
    "            y = target['FTR'].values\n",
    "        target_date = (pd.to_numeric(target['Date'])/1e9/24/60/60).values\n",
    "        return isInput, X,y, target, target_date\n",
    "   \n",
    "    def aggregate(self,recents,nonExpand,isInput,encode):\n",
    "        res =None\n",
    "        if encode == True:\n",
    "            if isInput==False:\n",
    "                self.ohe = OneHotEncoder(categorical_features='all')\n",
    "                self.ohe.fit(recents)\n",
    "            res = self.ohe.transform(recents).toarray()\n",
    "        else:\n",
    "            res = np.array(recents)\n",
    "        self.dateColumn = res.shape[1]\n",
    "        res = np.hstack([res,nonExpand])\n",
    "        return res\n",
    "  \n",
    "    def getH7(self,removeInsufficient=False, target=None,encode = True,future =0):\n",
    "        #Simple recent win,draw, lose \n",
    "        df = self.df\n",
    "        if removeInsufficient == True:\n",
    "            df= df[df['Sufficient'] == 1]\n",
    "        df=df[df['Future']==future]\n",
    "        \n",
    "        isInput, X, y,target, target_date = self.initData(df,target,encode)\n",
    "        resy=[]\n",
    "        resx=[]\n",
    "        print(\"start format\")\n",
    "        recents = X[['HomeTeam','AwayTeam','Referee']].values\n",
    "        haccp = X['HAccP'].values.reshape(X.shape[0],1)\n",
    "        aaccp = X['AAccP'].values.reshape(X.shape[0],1)\n",
    "        homeRecent = np.hstack([X[['HWin','HDraw','HLose']].values,\n",
    "                                (X['HScore'].values - X['HConcede'].values).reshape(X.shape[0],1)])\n",
    "        awayRecent = np.hstack([X[['AWin','ADraw','ALose']].values,\n",
    "                                (X['AScore'].values - X['AConcede'].values).reshape(X.shape[0],1)])\n",
    "        homeMoral = X['HMoral'].values.reshape(X.shape[0],1)\n",
    "        awayMoral = X['AMoral'].values.reshape(X.shape[0],1)\n",
    "        target_date = target_date.reshape(X.shape[0],1)\n",
    "     \n",
    "        nonExpand =np.hstack([target_date,X[['HRestDay','ARestDay','HS_Acc','AS_Acc','HST_Acc','AST_Acc',\n",
    "                                            'H_poss_Acc','A_poss_Acc','H_atk_3rd_tot_Acc','A_atk_3rd_tot_Acc',\n",
    "                                             'H_atk_3rd_Acc','A_atk_3rdk_Acc','H_interceptions_Acc','A_interceptions_Acc'\n",
    "                                            ]].values,haccp-aaccp,(haccp+1)/(aaccp+1),\n",
    "                                homeRecent,awayRecent, homeMoral - awayMoral + haccp - aaccp])\n",
    "        res = self.aggregate(recents,nonExpand,future,encode)\n",
    "        print(\"finish\")\n",
    "        sys.stdout.flush()\n",
    "        return res, np.array(y)\n",
    "    def _getRank(self,x, X,teamName,recentNum):\n",
    "        team = self.getTeam(X,teamName)\n",
    "        prev = team[team['Date'] < x['Date']].values      \n",
    "        for i in range(recentNum):\n",
    "            pass\n",
    "    def initRanking(self, n = 20):\n",
    "        defaultPt = 1\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HPoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"APoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        hpoints= df[\"HPoints\"].values\n",
    "        apoints=df[\"APoints\"].values\n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hpoints[i] = 3\n",
    "                apoints[i] = 0\n",
    "            elif ftr[i] == 'D':\n",
    "                hpoints[i] = 1\n",
    "                apoints[i] = 1\n",
    "            else :\n",
    "                hpoints[i] = 0\n",
    "                apoints[i] = 3\n",
    "        df[\"HPoints\"]=hpoints\n",
    "        df[\"APoints\"]=apoints\n",
    "        for teamName in self.teamsMap.keys():\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hpoints = team['HPoints'].values\n",
    "            apoints = team['APoints'].values\n",
    "            psum = 0\n",
    "            haccp = team['HAccP'].values\n",
    "            aaccp = team['AAccP'].values\n",
    "        \n",
    "            for  i in range(0,n):\n",
    "                if i < hpoints.shape[0]:\n",
    "                    psum = psum + (hpoints[i] if hometeam[i] == teamName else apoints[i] ) \n",
    "                else:\n",
    "                    psum = psum + defaultPt        \n",
    "                    \n",
    "        \n",
    "            for j in range(team.shape[0]):\n",
    "\n",
    "                if j+n < hpoints.shape[0]:                     \n",
    "                    psum = psum + (hpoints[j+n] if hometeam[j+n]==teamName else apoints[j+n])\n",
    "                else:\n",
    "                    psum = psum + defaultPt \n",
    "                \n",
    "                psum = psum - (hpoints[j] if hometeam[j]==teamName else apoints[j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    haccp[j]=psum\n",
    "                else:\n",
    "                    aaccp[j]=psum\n",
    "            team['HAccp']=haccp\n",
    "            team['AAccP']=aaccp\n",
    "            #print(team[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "            df.update(team)\n",
    "            \n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initRecentData(self, n =5):\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HDraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ADraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HLose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ALose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "                \n",
    "        df[\"HScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HRestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ARestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        #general\n",
    "        generalList = ['HS','AS','HST','AST','H_poss' ,'A_poss' ,'H_atk_3rd', 'A_atk_3rd','H_atk_3rd_tot','A_atk_3rd_tot'\n",
    "                       ,'H_interceptions' ,'A_interceptions']\n",
    "        generalOutput = []\n",
    "        for attr in generalList:\n",
    "            temp = attr+'_Acc'\n",
    "            df[temp]=pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "            generalOutput.append(temp)\n",
    "        #\n",
    "        df[\"Sufficient\"] = pd.Series(np.ones(shape=(df.shape[0],)))\n",
    "        \n",
    "      \n",
    "            \n",
    "        \n",
    "        hscore = df['FTHG'].values\n",
    "        ascore = df['FTAG'].values\n",
    "        hconcede = df ['FTAG'].values\n",
    "        aconcede = df['FTHG'].values\n",
    "               \n",
    "        hwin = df['HWin'].values\n",
    "        awin = df['AWin'].values\n",
    "        hlose = df['HLose'].values\n",
    "        alose = df['ALose'].values\n",
    "        hdraw = df['HDraw'].values\n",
    "        adraw = df['ADraw'].values\n",
    "        hmoral = df['HMoral'].values\n",
    "        amoral = df['AMoral'].values\n",
    "        \n",
    "        rankRatio = (df['HAccP'].values+1) / (df['AAccP'].values +1)\n",
    "        \n",
    "        \n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hwin[i] = 1\n",
    "                hmoral[i] = 3 * 1/rankRatio[i]\n",
    "                alose[i]= 1\n",
    "                amoral[i] = -3 * 1/rankRatio[i]\n",
    "            elif ftr[i] == 'D':\n",
    "                hdraw[i] = 1\n",
    "                hmoral[i] = 1 * 1/rankRatio[i]\n",
    "                adraw[i] = 1\n",
    "                amoral[i] = 1 * rankRatio[i]\n",
    "            else :\n",
    "                hlose[i] = 1\n",
    "                hmoral[i] = -3*rankRatio[i]\n",
    "                awin [i] = 1\n",
    "                amoral[i] = 3*rankRatio[i]\n",
    "        \n",
    "        \n",
    "        df[\"HWin\"]=hwin\n",
    "        df[\"AWin\"]=awin\n",
    "        df[\"HDraw\"]=hdraw\n",
    "        df[\"ADraw\"]=adraw\n",
    "        df[\"HLose\"]=hlose\n",
    "        df[\"ALose\"]=alose\n",
    "        df[\"HScore\"]=hscore\n",
    "        df[\"AScore\"]=ascore\n",
    "        df[\"HConcede\"]=hconcede\n",
    "        df[\"AConcede\"]=aconcede\n",
    "        df[\"HMoral\"] = hmoral\n",
    "        df[\"AMoral\"] = amoral\n",
    "        \n",
    "        \n",
    "        \n",
    "        for teamName in self.teamsMap.keys():\n",
    "            print(teamName)\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hwin = team[\"HWin\"].values\n",
    "            awin = team[\"AWin\"].values\n",
    "            hlose= team[\"HLose\"].values\n",
    "            alose = team[\"ALose\"].values\n",
    "            hdraw = team[\"HDraw\"].values\n",
    "            adraw = team[\"ADraw\"].values\n",
    "            hscore = team[\"HScore\"].values\n",
    "            ascore = team[\"AScore\"].values\n",
    "            hconcede = team[\"HConcede\"].values\n",
    "            aconcede = team[\"AConcede\"].values\n",
    "            hmoral = team[\"HMoral\"].values\n",
    "            amoral = team[\"AMoral\"].values\n",
    "            hrestday = team[\"HRestDay\"].values\n",
    "            arestday = team[\"ARestDay\"].values\n",
    "            \n",
    "            #general\n",
    "            hs= team[\"HS\"].values\n",
    "            as_ = team[\"AS\"].values\n",
    "            hst = team[\"HST\"].values\n",
    "            ast = team[\"AST\"].values\n",
    "            original_list =[]\n",
    "            output_list=[]\n",
    "            for indx, o_attr in enumerate(generalList):\n",
    "                original_list.append(team[o_attr].values)\n",
    "                output_list.append(team[generalOutput[indx]].values)\n",
    "            \n",
    "            \n",
    "            matchDate =team['Date'].values\n",
    "            sufficient = team['Sufficient'].values\n",
    "            teamMatchesDate = self.teamsData[teamName].sort('1',ascending=False)['1'].values\n",
    "            \n",
    "            restday = 0\n",
    "            winsum =0 \n",
    "            losesum=0\n",
    "            drawsum=0\n",
    "            scoresum =0\n",
    "            concedesum=0\n",
    "            moralsum = 0\n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            teamAttrSum_list=[0 for i in range(int(len(original_list)/2))]\n",
    "            for  i in range(0,n):\n",
    "                if i < team.shape[0]:\n",
    "                    scoresum = scoresum + (hscore[i] if hometeam[i] == teamName else ascore[i])\n",
    "                    winsum = winsum + (hwin[i] if hometeam[i] == teamName else awin[i])\n",
    "                    losesum= losesum + (hlose[i] if hometeam[i] == teamName else alose[i])\n",
    "                    drawsum= drawsum + (hdraw[i] if hometeam[i] == teamName else adraw[i])\n",
    "                    concedesum = concedesum+ (hconcede[i] if hometeam[i] == teamName else aconcede[i])\n",
    "                    moralsum= moralsum+ (hmoral[i] if hometeam[i] == teamName else amoral[i])\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        teamAttrSum_list[attrIndx] +=(original_list[2*attrIndx][i] if hometeam[i] == teamName else original_list[2*attrIndx+1][i])\n",
    "                else:\n",
    "                    # + 0\n",
    "                    pass\n",
    "            dateIndx = 0\n",
    "            for j in range(team.shape[0]):\n",
    "                while True:\n",
    "                    if dateIndx >= teamMatchesDate.shape[0]:\n",
    "                        sufficient[j] = False\n",
    "                        break\n",
    "                    if teamMatchesDate[dateIndx] < matchDate[j] :\n",
    "                        restday = (matchDate[j] - teamMatchesDate[dateIndx])/np.timedelta64(1,'D')\n",
    "                        break\n",
    "                    else:\n",
    "                        dateIndx = dateIndx + 1\n",
    "                \n",
    "                if j+n < team.shape[0]:                     \n",
    "                    scoresum = scoresum + (hscore[j+n] if hometeam[j+n] == teamName else ascore[j+n])\n",
    "                    winsum = winsum + (hwin[j+n] if hometeam[j+n] == teamName else awin[j+n])\n",
    "                    losesum= losesum + (hlose[j+n] if hometeam[j+n] == teamName else alose[j+n])\n",
    "                    drawsum= drawsum + (hdraw[j+n] if hometeam[j+n] == teamName else adraw[j+n])\n",
    "                    concedesum = concedesum+ (hconcede[j+n] if hometeam[j+n] == teamName else aconcede[j+n])\n",
    "                    moralsum= moralsum+ (hmoral[j+n] if hometeam[j+n] == teamName else amoral[j+n])\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        teamAttrSum_list[attrIndx] += (original_list[2*attrIndx][j+n] if hometeam[j+n] == teamName else original_list[2*attrIndx+1][j+n])\n",
    "                else:\n",
    "                    sufficient[j] = False\n",
    "                    \n",
    "                \n",
    "                scoresum = scoresum - (hscore[j] if hometeam[j] == teamName else ascore[j])\n",
    "                winsum = winsum - (hwin[j] if hometeam[j] == teamName else awin[j])\n",
    "                losesum= losesum - (hlose[j] if hometeam[j] == teamName else alose[j])\n",
    "                drawsum= drawsum - (hdraw[j] if hometeam[j] == teamName else adraw[j])\n",
    "                concedesum = concedesum - (hconcede[j] if hometeam[j] == teamName else aconcede[j])\n",
    "                moralsum= moralsum - (hmoral[j] if hometeam[j] == teamName else amoral[j])\n",
    "                for attrIndx in range(len(teamAttrSum_list)):\n",
    "                    teamAttrSum_list[attrIndx] -=  (original_list[2*attrIndx][j] if hometeam[j] == teamName else original_list[2*attrIndx+1][j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    hscore[j] = scoresum\n",
    "                    hwin[j] = winsum\n",
    "                    hlose[j] = losesum\n",
    "                    hdraw[j] = drawsum\n",
    "                    hconcede[j] = concedesum\n",
    "                    hmoral[j] = moralsum\n",
    "                    hrestday[j] = restday\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        output_list[2*attrIndx][j] = teamAttrSum_list[attrIndx]\n",
    "                else:\n",
    "                    ascore[j] = scoresum\n",
    "                    awin[j] = winsum\n",
    "                    alose[j] = losesum\n",
    "                    adraw[j] = drawsum\n",
    "                    aconcede[j] = concedesum\n",
    "                    amoral[j] = moralsum\n",
    "                    arestday[j] = restday\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        output_list[2*attrIndx+1][j] = teamAttrSum_list[attrIndx]\n",
    "            team[\"HWin\"]=hwin\n",
    "            team[\"AWin\"]=awin\n",
    "            team[\"HDraw\"]=hdraw\n",
    "            team[\"ADraw\"]=adraw\n",
    "            team[\"HLose\"]=hlose\n",
    "            team[\"ALose\"]=alose\n",
    "            team[\"HScore\"]=hscore\n",
    "            team[\"AScore\"]=ascore\n",
    "            team[\"HConcede\"]=hconcede\n",
    "            team[\"AConcede\"]=aconcede\n",
    "            team[\"HMoral\"] = hmoral\n",
    "            team[\"AMoral\"] = amoral\n",
    "            team['Sufficient'] = sufficient\n",
    "            for indx in range(len(output_list)):\n",
    "                team[generalOutput[indx]]= output_list[indx]\n",
    "        \n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            df.update(team)\n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initTeamData(self):\n",
    "        self.teamsData={}\n",
    "        self.teamsById={}\n",
    "        for name in self.teamsMap.keys():\n",
    "            self.readTeamMatch(name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "c = FootballDataHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:23: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:24: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#c.readFootBallData(\"E0_1112.csv\")\n",
    "c.readFootBallData(2012)\n",
    "c.readFootBallData(2013)\n",
    "c.readFootBallData(2014)\n",
    "c.readFootBallData(2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>A_Poss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>37.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Man City</td>\n",
       "      <td>45.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>46.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>44.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Everton</td>\n",
       "      <td>46.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>40.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Watford</td>\n",
       "      <td>47.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>49.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>50.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>2016-03-13</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>62.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Norwich</td>\n",
       "      <td>66.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>53.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>61.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>51.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Everton</td>\n",
       "      <td>58.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Man City</td>\n",
       "      <td>28.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>50.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>44.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Watford</td>\n",
       "      <td>49.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>65.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>44.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Man United</td>\n",
       "      <td>38.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>37.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2012-09-15</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>48.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>31.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>55.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>38.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Man City</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>51.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>66.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>37.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Norwich</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Man United</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>47.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>60.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012-08-22</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>28.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012-08-20</td>\n",
       "      <td>Everton</td>\n",
       "      <td>69.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-08-19</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012-08-19</td>\n",
       "      <td>Man City</td>\n",
       "      <td>35.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>48.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Reading</td>\n",
       "      <td>47.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>QPR</td>\n",
       "      <td>50.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>59.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>65.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>29.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1443 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        HomeTeam  A_Poss\n",
       "1442 2016-03-20       Tottenham    37.2\n",
       "1441 2016-03-20     Southampton    50.8\n",
       "1440 2016-03-20       Newcastle    40.1\n",
       "1439 2016-03-20        Man City    45.5\n",
       "1436 2016-03-19         Swansea    46.2\n",
       "1434 2016-03-19  Crystal Palace    44.1\n",
       "1435 2016-03-19         Everton    46.1\n",
       "1433 2016-03-19         Chelsea    40.6\n",
       "1437 2016-03-19         Watford    47.9\n",
       "1438 2016-03-19       West Brom    49.9\n",
       "1432 2016-03-14       Leicester    50.1\n",
       "1431 2016-03-13     Aston Villa    62.2\n",
       "1430 2016-03-12           Stoke    39.4\n",
       "1429 2016-03-12         Norwich    66.1\n",
       "1428 2016-03-12     Bournemouth    56.0\n",
       "1427 2016-03-06       West Brom    53.4\n",
       "1426 2016-03-06  Crystal Palace    61.5\n",
       "1422 2016-03-05     Southampton    34.9\n",
       "1418 2016-03-05         Chelsea    51.3\n",
       "1419 2016-03-05         Everton    58.5\n",
       "1420 2016-03-05        Man City    28.8\n",
       "1421 2016-03-05       Newcastle    50.7\n",
       "1423 2016-03-05         Swansea    44.2\n",
       "1424 2016-03-05       Tottenham    48.2\n",
       "1425 2016-03-05         Watford    49.8\n",
       "1417 2016-03-02        West Ham    65.1\n",
       "1416 2016-03-02           Stoke    44.7\n",
       "1415 2016-03-02      Man United    38.8\n",
       "1414 2016-03-02       Liverpool    50.5\n",
       "1413 2016-03-02         Arsenal    37.4\n",
       "...         ...             ...     ...\n",
       "36   2012-09-15      Sunderland    65.9\n",
       "28   2012-09-02     Southampton    54.9\n",
       "27   2012-09-02       Newcastle    48.3\n",
       "26   2012-09-02       Liverpool    47.0\n",
       "25   2012-09-01           Wigan    31.3\n",
       "24   2012-09-01        West Ham    54.9\n",
       "23   2012-09-01       West Brom    55.7\n",
       "22   2012-09-01       Tottenham    38.4\n",
       "21   2012-09-01         Swansea    36.2\n",
       "20   2012-09-01        Man City    40.1\n",
       "18   2012-08-26       Liverpool    51.3\n",
       "19   2012-08-26           Stoke    66.7\n",
       "17   2012-08-25       Tottenham    41.0\n",
       "16   2012-08-25         Swansea    37.8\n",
       "15   2012-08-25     Southampton    50.8\n",
       "14   2012-08-25         Norwich    44.9\n",
       "13   2012-08-25      Man United    40.1\n",
       "12   2012-08-25         Chelsea    47.5\n",
       "11   2012-08-25     Aston Villa    60.8\n",
       "10   2012-08-22         Chelsea    28.4\n",
       "9    2012-08-20         Everton    69.2\n",
       "8    2012-08-19           Wigan    48.0\n",
       "7    2012-08-19        Man City    35.9\n",
       "2    2012-08-18       Newcastle    48.1\n",
       "4    2012-08-18         Reading    47.4\n",
       "3    2012-08-18             QPR    50.1\n",
       "1    2012-08-18          Fulham    40.2\n",
       "5    2012-08-18       West Brom    59.5\n",
       "6    2012-08-18        West Ham    65.8\n",
       "0    2012-08-18         Arsenal    29.9\n",
       "\n",
       "[1443 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.df[['Date','HomeTeam','A_Poss']].sort(columns='Date',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "c.readFuture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:140: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c.initTeamData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:224: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df = c.initRanking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:284: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:400: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:479: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:480: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:481: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:484: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:485: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:486: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:487: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:488: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:489: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:492: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burnley\n",
      "West Ham\n",
      "West Brom\n",
      "Aston Villa\n",
      "Arsenal\n",
      "Crystal Palace\n",
      "Norwich\n",
      "Man United\n",
      "Hull\n",
      "Chelsea\n",
      "QPR\n",
      "Watford\n",
      "Cardiff\n",
      "Fulham\n",
      "Everton\n",
      "Sunderland\n",
      "Swansea\n",
      "Reading\n",
      "Man City\n",
      "Leicester\n",
      "Southampton\n",
      "Bournemouth\n",
      "Stoke\n",
      "Tottenham\n",
      "Newcastle\n",
      "Wigan\n",
      "Liverpool\n"
     ]
    }
   ],
   "source": [
    "df=c.initRecentData(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.saveDf('dataSet/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.loadDf('dataSet/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y=c.getH7(removeInsufficient=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "def plotErrorDate(X_test, X_err, dateCol = 10):\n",
    "    X_test_date =np.sort(X_test[:,c.dateColumn])\n",
    "    X_date=[]\n",
    "    y_date=[]\n",
    "    for v in X_test_date:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_date) ==0  or X_date[-1] != date:\n",
    "            X_date.append(date)\n",
    "            y_date.append(1)\n",
    "        else:\n",
    "            y_date[-1] = y_date[-1] +1\n",
    "    plt.plot_date(X_date,y_date,xdate=True)\n",
    "    X_err_d = np.sort(X_err[:,c.dateColumn])\n",
    "    X_err_date=[]\n",
    "    y_err_date = []\n",
    "    for v in X_err_d:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_err_date) ==0  or X_err_date[-1] != date:\n",
    "            X_err_date.append(date)\n",
    "            y_err_date.append(1)\n",
    "        else:\n",
    "            y_err_date[-1] = y_err_date[-1] +1\n",
    "    plt.plot_date(X_err_date,y_err_date,xdate=True,color='red')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "g_hiddenNodes = int(45)\n",
    "g_alpha = 0 \n",
    "clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_hiddenNodes], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "              learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=1000,early_stopping = False,verbose = 3)\n",
    "mlp = Pipeline([ ('scl', StandardScaler()),('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"start learning\")\n",
    "sys.stdout.flush()\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=mlp, \n",
    "                       X=X, \n",
    "                      y=y, \n",
    "                      train_sizes=np.linspace(0.1, 1.0, 4), \n",
    "                      cv=4,\n",
    "                     n_jobs=1,verbose=3)\n",
    "print(\"finishing\")   \n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "print(test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learningRes = np.vstack([train_sizes,train_mean,train_std,test_mean,test_std]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learningDf = pd.DataFrame(learningRes,columns=['size','train_mean','train_std','test_mean','test_std'])\n",
    "print(learningDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotCurve(train_mean,train_std,test_mean,test_std,train_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate(mlp, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    firstNScores = []\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "\n",
    "        mlp.fit(X[train], y[train])\n",
    "        score = mlp.score(X[test], y[test])\n",
    "        firstNScores.append(firstNScore(2, mlp.predict_proba(X[test]), y[test]))\n",
    "        train_scores.append(mlp.score(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, firstNScores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def lamda_test(mlp, X, y, lamdas):\n",
    "    \n",
    "    train_scores=[]\n",
    "    test_scores=[]\n",
    "    for lamda in lamdas:\n",
    "        clf.set_params(alpha= lamda)\n",
    "        train_s, test_s, firstNScores = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_s, test_s, firstNScores =futureTest(mlp,X,y,numOfWeek=20) \n",
    "        train_scores.append(train_s)\n",
    "        test_scores.append(test_s)\n",
    "        print(\"lamda: {}, train: {}, test: {}\".format(lamda, \n",
    "                    np.mean(train_s), np.mean(test_s)) )\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plotCurve(train_mean,train_std,test_mean,test_std,lamdas)\n",
    "    return np.array(train_scores),np.array(test_scores)\n",
    "\n",
    "l_range = []\n",
    "for i in range(0,50):\n",
    "    l_range.append(2*i)\n",
    "train_scores,test_scores = lamda_test(mlp,X,y,np.array(l_range))\n",
    "#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "alphaRes = np.vstack([l_range,train_mean,train_std,test_mean,test_std]).T\n",
    "alphaDf = pd.DataFrame(alphaRes,columns=['alpha','train_mean','train_std','test_mean','test_std'])\n",
    "print(alphaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testNodeSize(start ,end):\n",
    "    node_range = range(start,end,5)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    for node in node_range:   \n",
    "        print(\"start node:{}\".format(node))\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[node], activation='logistic', algorithm='l-bfgs', alpha=0, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores,first2 = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_scores,test_scores , first2= futureTest(mlp, X,y,numOfWeek = 10)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        print(\"Node {}: train_mean {}  v.s. test_mean {}\".format(node,np.mean(train_scores),np.mean(test_scores)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(node_range))\n",
    "    return node_range, train_means,train_std,test_means,test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node_range, train_means,train_std,test_means,test_std=testNodeSize(1,X.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodeRes = np.vstack([node_range,train_means,train_std,test_means,test_std]).T\n",
    "nodeDf = pd.DataFrame(nodeRes,columns=['nodeNum','train_mean','train_std','test_mean','test_std'])\n",
    "print(nodeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.set_params(alpha=g_alpha)\n",
    "print(clf)\n",
    "train_score, test_score, first2 = futureTest(mlp,X,y,numOfWeek = 30, verbose=True)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testRecentNum(start, end):\n",
    "    recent_range = range(start,end)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    first2_mean=[]\n",
    "    for recent in recent_range:\n",
    "        print(\"start recent:{}\".format(recent))\n",
    "        X,y = c.getH7(recent)\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_alpha], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores, first2 = crossValidate(mlp,X,y,fold=10)\n",
    "        #train_scores,test_scores, first2 = futureTest(mlp, X,y,numOfWeek = 15)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        first2_mean.append(np.mean(first2))\n",
    "        print(\"recent {}: train_mean {}  v.s. test_mean {} , first2_mean {}\".format(\n",
    "                recent,np.mean(train_scores),np.mean(test_scores),np.mean(first2)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(recent_range))\n",
    "    return train_means,train_std,test_means,test_std,first2_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_means,train_std,test_means,test_std,first2_mean=testRecentNum(1 ,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "proba = mlp.predict_proba(X_test)\n",
    "precisionMatrix(proba,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#future\n",
    "mlp.fit(X,y)\n",
    "inData = c.readPredict(\"future.csv\")\n",
    "print(inData)\n",
    "X_in, y_in = c.getH6(5,target=inData)\n",
    "res = mlp.predict(X_in)\n",
    "proba= mlp.predict_proba(X_in)\n",
    "print(mlp.score(X_in,y_in))\n",
    "print (np.hstack([proba,y_in]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "return\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient=True, encode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1358, 29)\n",
      "                home         away        Referee   time HRestTime ARestTime  \\\n",
      "0             Fulham     Man City       M Halsey  15612         7         3   \n",
      "1            Norwich    Liverpool        M Jones  15612         2         2   \n",
      "2            Everton  Southampton      L Probert  15612         3         3   \n",
      "3              Stoke      Swansea         J Moss  15612         7         3   \n",
      "4            Arsenal      Chelsea     M Atkinson  15612         2         3   \n",
      "5         Man United    Tottenham          C Foy  15612         2         2   \n",
      "6        Aston Villa    West Brom       A Taylor  15613         4         3   \n",
      "7                QPR     West Ham  M Clattenburg  15614         4         5   \n",
      "8              Wigan      Everton       K Friend  15619         7         7   \n",
      "9          West Brom          QPR        M Jones  15619         6         4   \n",
      "10          Man City   Sunderland      L Probert  15619         2         7   \n",
      "11           Chelsea      Norwich       A Taylor  15619         3         7   \n",
      "12          West Ham      Arsenal         P Dowd  15619         4         2   \n",
      "13           Swansea      Reading         M Dean  15619         7         7   \n",
      "14       Southampton       Fulham  M Clattenburg  15620         8         8   \n",
      "15         Tottenham  Aston Villa    N Swarbrick  15620         2         7   \n",
      "16         Liverpool        Stoke        L Mason  15620         2         8   \n",
      "17         Newcastle   Man United         H Webb  15620         2         4   \n",
      "18           Norwich      Arsenal      L Probert  15633        14        13   \n",
      "19          West Ham  Southampton    N Swarbrick  15633        13        13   \n",
      "20         West Brom     Man City  M Clattenburg  15633        14        14   \n",
      "21           Swansea        Wigan        M Jones  15633        14        14   \n",
      "22        Man United        Stoke       A Taylor  15633        13        13   \n",
      "23            Fulham  Aston Villa          C Foy  15633        13        13   \n",
      "24         Liverpool      Reading         R East  15633        13        14   \n",
      "25         Tottenham      Chelsea         M Dean  15633        13        14   \n",
      "26               QPR      Everton         J Moss  15634        15        15   \n",
      "27        Sunderland    Newcastle     M Atkinson  15634        15        14   \n",
      "28          Man City      Swansea     M Atkinson  15640         2         7   \n",
      "29           Arsenal          QPR       A Taylor  15640         2         6   \n",
      "...              ...          ...            ...    ...       ...       ...   \n",
      "1328           Stoke    Newcastle    N Swarbrick  16862         4        17   \n",
      "1329      Man United      Watford        M Jones  16862         3         4   \n",
      "1330        West Ham    Tottenham     A Marriner  16862         4         3   \n",
      "1331         Arsenal      Swansea       R Madley  16862         3         3   \n",
      "1332       Liverpool     Man City     M Atkinson  16862         2         2   \n",
      "1333         Chelsea        Stoke  M Clattenburg  16865         3         2   \n",
      "1334     Southampton   Sunderland    N Swarbrick  16865         3         3   \n",
      "1335       Newcastle  Bournemouth      P Tierney  16865         2         3   \n",
      "1336         Everton     West Ham       A Taylor  16865         3         2   \n",
      "1337         Swansea      Norwich       C Pawson  16865         2         3   \n",
      "1338       Tottenham      Arsenal       M Oliver  16865         2         2   \n",
      "1339         Watford    Leicester         J Moss  16865         2         3   \n",
      "1340        Man City  Aston Villa        L Mason  16865         2         3   \n",
      "1341       West Brom   Man United         M Dean  16866         4         3   \n",
      "1342  Crystal Palace    Liverpool     A Marriner  16866         4         3   \n",
      "1343     Bournemouth      Swansea         R East  16872         7         7   \n",
      "1344           Stoke  Southampton        L Mason  16872         7         7   \n",
      "1345         Norwich     Man City         J Moss  16872         7         7   \n",
      "1346     Aston Villa    Tottenham       A Taylor  16873         8         2   \n",
      "1347       Leicester    Newcastle       C Pawson  16874         8         9   \n",
      "1348       West Brom      Norwich       A Taylor  16879        12         7   \n",
      "1349         Watford        Stoke       C Pawson  16879         6         7   \n",
      "1350         Chelsea     West Ham       R Madley  16879         6         5   \n",
      "1351         Everton      Arsenal  M Clattenburg  16879         6         2   \n",
      "1352  Crystal Palace    Leicester        M Jones  16879         7         4   \n",
      "1353         Swansea  Aston Villa         M Dean  16879         7         5   \n",
      "1354       Tottenham  Bournemouth    N Swarbrick  16880         2         8   \n",
      "1355       Newcastle   Sunderland     M Atkinson  16880         5        15   \n",
      "1356        Man City   Man United       M Oliver  16880         4         2   \n",
      "1357     Southampton    Liverpool         R East  16880         8         2   \n",
      "\n",
      "     HS_Acc AS_Acc HST_Acc AST_Acc ... HWin HDraw HLose H goal Diff AWin  \\\n",
      "0        74     79      56      53 ...    3     0     2           5    2   \n",
      "1        55     79      27      40 ...    0     3     2          -6    0   \n",
      "2        94     59      52      31 ...    3     1     1           4    1   \n",
      "3        46     64      24      41 ...    0     4     1          -1    2   \n",
      "4        71     69      33      38 ...    2     3     0           7    4   \n",
      "5        75     85      41      50 ...    4     0     1           6    2   \n",
      "6        55     58      33      35 ...    1     1     3          -4    3   \n",
      "7        54     61      32      37 ...    0     2     3          -8    2   \n",
      "8        55     95      29      54 ...    1     1     3          -4    3   \n",
      "9        57     47      32      30 ...    2     2     1           0    0   \n",
      "10       80     28      50      17 ...    2     3     0           3    1   \n",
      "11       74     61      40      32 ...    4     1     0           6    0   \n",
      "12       70     71      43      34 ...    2     2     1           1    2   \n",
      "13       60     42      33      18 ...    1     1     3          -4    0   \n",
      "14       58     70      27      50 ...    1     0     4          -7    2   \n",
      "15       83     64      50      35 ...    3     2     0           4    1   \n",
      "16       75     50      39      26 ...    1     2     2           0    1   \n",
      "17       62     78      29      42 ...    1     3     1          -1    4   \n",
      "18       55     75      32      42 ...    0     2     3          -7    3   \n",
      "19       70     57      40      24 ...    2     2     1           2    1   \n",
      "20       60     95      35      61 ...    3     1     1           1    3   \n",
      "21       75     53      42      27 ...    0     2     3          -7    0   \n",
      "22       70     48      40      25 ...    4     0     1           8    1   \n",
      "23       70     65      44      37 ...    2     1     2           0    1   \n",
      "24       76     41      36      21 ...    1     2     2           0    0   \n",
      "25       88     71      50      42 ...    4     1     0           6    4   \n",
      "26       54     92      32      54 ...    0     1     4          -5    2   \n",
      "27       33     64      21      27 ...    1     3     1          -2    1   \n",
      "28       97     79      62      43 ...    3     2     0           5    1   \n",
      "29       77     59      42      33 ...    2     1     2           5    0   \n",
      "...     ...    ...     ...     ... ...  ...   ...   ...         ...  ...   \n",
      "1328     46     70      14      26 ...    2     0     3          -6    2   \n",
      "1329     59     52      24      12 ...    2     1     2           2    2   \n",
      "1330     73    107      20      43 ...    2     2     1           2    5   \n",
      "1331     77     67      28      18 ...    2     1     2           1    1   \n",
      "1332     73     80      27      17 ...    2     1     2           4    2   \n",
      "1333     69     52      20      14 ...    3     2     0           6    3   \n",
      "1334     48     69      13      19 ...    2     1     2          -1    1   \n",
      "1335     52     56      19      17 ...    1     0     4          -8    2   \n",
      "1336     96     77      28      20 ...    3     0     2           6    3   \n",
      "1337     71     57      15      12 ...    1     2     2          -1    0   \n",
      "1338     92     86      34      31 ...    4     0     1           5    2   \n",
      "1339     52     69      10      24 ...    1     2     2          -1    3   \n",
      "1340     67     41      15      12 ...    1     1     3          -5    1   \n",
      "1341     51     65      16      26 ...    2     2     1           1    3   \n",
      "1342     68     65      19      30 ...    0     2     3          -3    3   \n",
      "1343     64     63      18      14 ...    2     1     2           0    2   \n",
      "1344     58     50      19      15 ...    3     1     1           1    2   \n",
      "1345     55     73      13      21 ...    0     1     4          -5    2   \n",
      "1346     37    101      11      39 ...    1     0     4         -11    3   \n",
      "1347     70     55      25      20 ...    3     1     1           3    1   \n",
      "1348     43     48      11       9 ...    3     1     1           2    0   \n",
      "1349     51     60      11      18 ...    1     1     3          -2    3   \n",
      "1350     77     69      23      19 ...    3     2     0           6    3   \n",
      "1351     91     75      34      24 ...    3     0     2           6    2   \n",
      "1352     68     66      20      19 ...    0     2     3          -3    3   \n",
      "1353     56     38      15      10 ...    2     0     3          -1    0   \n",
      "1354     94     59      38      18 ...    3     1     1           3    3   \n",
      "1355     61     69      19      20 ...    1     0     4          -7    1   \n",
      "1356     82     57      22      22 ...    1     1     3          -2    2   \n",
      "1357     53     71      17      28 ...    2     1     2          -1    3   \n",
      "\n",
      "     ADraw ALose A goal diff moraldiff + h-a  y  \n",
      "0        3     0           3         -5.4577  A  \n",
      "1        2     3          -6         4.26691  A  \n",
      "2        0     4          -6         20.4973  H  \n",
      "3        1     2           3        -0.98787  H  \n",
      "4        1     0           7         -7.3312  A  \n",
      "5        2     1           2         7.24231  A  \n",
      "6        1     1           3        -16.1433  D  \n",
      "7        2     1           1        -16.4869  A  \n",
      "8        1     1           5        -18.8485  D  \n",
      "9        2     3          -4         17.7318  H  \n",
      "10       4     0           1         5.22302  H  \n",
      "11       3     2          -4         26.8163  H  \n",
      "12       2     1           6         1.33957  A  \n",
      "13       2     3          -5         3.22343  D  \n",
      "14       0     3          -1        -9.28572  D  \n",
      "15       2     2          -3         17.1502  H  \n",
      "16       3     1           1        -5.98228  D  \n",
      "17       0     1           6         -7.8114  A  \n",
      "18       1     1           8        -21.3046  H  \n",
      "19       1     3          -5         12.8002  H  \n",
      "20       2     0           6        -6.20904  A  \n",
      "21       2     3          -6        0.178857  H  \n",
      "22       3     1           1         9.43603  H  \n",
      "23       2     2          -3         5.66887  H  \n",
      "24       2     3          -5         7.88105  H  \n",
      "25       1     0           7        -2.72658  A  \n",
      "26       2     1           3        -23.5168  D  \n",
      "27       3     1          -2       -0.225469  D  \n",
      "28       1     3          -6         24.2804  H  \n",
      "29       2     3          -3         12.0548  H  \n",
      "...    ...   ...         ...             ... ..  \n",
      "1328     0     3          -6         3.14025  H  \n",
      "1329     2     1           1        -4.32401  H  \n",
      "1330     0     0           8        -19.9385  H  \n",
      "1331     2     2          -1         13.1779  A  \n",
      "1332     1     2           2        -2.28404  H  \n",
      "1333     0     2          -2         9.61243  D  \n",
      "1334     2     2          -1         1.78991  D  \n",
      "1335     1     2          -1        -12.8138  A  \n",
      "1336     1     1           3        -10.8407  A  \n",
      "1337     1     4          -7         19.0105  H  \n",
      "1338     1     2           1         13.5843  D  \n",
      "1339     1     1           4        -19.0615  A  \n",
      "1340     0     4          -9         13.3512  H  \n",
      "1341     1     1           4        -4.45653  H  \n",
      "1342     1     1           8        -24.5246  A  \n",
      "1343     1     2           0         1.36626  H  \n",
      "1344     1     2          -1         8.06201  A  \n",
      "1345     0     3          -1        -21.0893  D  \n",
      "1346     1     1           2        -30.6047  A  \n",
      "1347     0     4          -9         31.6266  H  \n",
      "1348     2     3          -3         20.5266  A  \n",
      "1349     1     1           3        -10.9102  A  \n",
      "1350     1     1           2         1.42972  D  \n",
      "1351     1     2           1        -2.52855  A  \n",
      "1352     1     1           2        -33.3654  A  \n",
      "1353     0     5         -15         16.7183  H  \n",
      "1354     1     1           3         3.93801  H  \n",
      "1355     3     1           0        -15.3967  D  \n",
      "1356     1     2           0        -4.08739  A  \n",
      "1357     1     1           8           -9.09  H  \n",
      "\n",
      "[1358 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "df = pd.DataFrame(np.hstack([X,y.reshape(y.shape[0],1)]))\n",
    "\n",
    "df.columns = ['home','away','Referee','time','HRestTime','ARestTime','HS_Acc','AS_Acc','HST_Acc','AST_Acc',\n",
    "                                            'H_Poss_Acc','A_Poss_Acc','H_atkPass_tot_Acc','A_atkPass_tot_Acc'\n",
    "                                             ,'H_atkPass_Ok_Acc','A_atkPass_OK_Acc','H_ins_Acc','A_ins_Acc'\n",
    "              ,'HAccP - AAccP','H/A','HWin','HDraw','HLose','H goal Diff',\n",
    "'AWin','ADraw','ALose','A goal diff','moraldiff + h-a',\n",
    "              'y']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('dataSet/V9.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "def splitData(X,y):\n",
    "    X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_test, X_val, y_test,y_val = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "def createModel(hidSize, inputDim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidSize[0], input_dim=inputDim, init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(hidSize[1], init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(3, init='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd = SGD(lr=1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "    return model\n",
    "earlyCallback = EarlyStopping(patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate2(node_sizes, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    proba_test = []\n",
    "    proba_y=[]\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "        earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "        model = createModel(node_sizes,X.shape[1])\n",
    "        history = model.fit(X[train],y[train],verbose=0,nb_epoch=500, validation_split=0.1,show_accuracy=True, callbacks=[earlyCallback])\n",
    "      #  firstNScores.append(firstNScore(2, model.predict_proba(X[test]), y[test]))\n",
    "        score = model.evaluate(X[test],y[test])\n",
    "        proba_test.append(model.predict_proba(X[test]))\n",
    "        proba_y.append(y[test])\n",
    "        train_scores.append(model.evaluate(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, val_loss: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, proba_test,proba_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNodeNum(X,y, sizes):\n",
    "    train_loss=[] \n",
    "    score_loss=[]\n",
    "    for s in sizes:\n",
    "        train_scores,scores,  proba_test,proba_y= crossValidate2([s,s],X,y,fold=5)\n",
    "        print(\"size:{} , val_loss_mean:{}\".format(s,np.mean(scores)))\n",
    "        train_loss.append(train_scores)\n",
    "        score_loss.append(scores)\n",
    "    return train_loss,score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.996\n",
      "Epoch 00119: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.966\n",
      "Epoch 00078: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.983\n",
      "Epoch 00057: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00045: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.034\n",
      "size:15 , val_loss_mean:0.994878006496512\n",
      "Epoch 00090: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.997\n",
      "Epoch 00064: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.961\n",
      "Epoch 00108: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00056: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.000\n",
      "Epoch 00044: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.032\n",
      "size:20 , val_loss_mean:0.997879133147474\n",
      "Epoch 00057: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.985\n",
      "Epoch 00109: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00106: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00081: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.008\n",
      "Epoch 00044: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.042\n",
      "size:25 , val_loss_mean:1.002979383102754\n",
      "Epoch 00064: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00066: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.957\n",
      "Epoch 00044: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.985\n",
      "Epoch 00053: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.993\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.035\n",
      "size:30 , val_loss_mean:0.9919062859579861\n",
      "Epoch 00047: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.985\n",
      "Epoch 00063: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:35 , val_loss_mean:0.9929705902102708\n",
      "Epoch 00048: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.984\n",
      "Epoch 00098: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.987\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.991\n",
      "Epoch 00042: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.040\n",
      "size:40 , val_loss_mean:0.9931346279183522\n",
      "Epoch 00043: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.984\n",
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00089: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00042: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.033\n",
      "size:45 , val_loss_mean:0.9946483096903833\n",
      "Epoch 00074: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.995\n",
      "Epoch 00091: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.985\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00027: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.038\n",
      "size:50 , val_loss_mean:0.9952394351575709\n",
      "Epoch 00050: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00080: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00035: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00039: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.040\n",
      "size:55 , val_loss_mean:0.9950326937267647\n",
      "Epoch 00070: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00084: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.965\n",
      "Epoch 00066: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.992\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00027: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:60 , val_loss_mean:0.9955750316447294\n",
      "Epoch 00056: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.988\n",
      "Epoch 00040: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00042: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:65 , val_loss_mean:0.9943353890837905\n",
      "Epoch 00072: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00080: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.000\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:70 , val_loss_mean:0.9954743319077854\n",
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.990\n",
      "Epoch 00061: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.966\n",
      "Epoch 00045: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00044: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.034\n",
      "size:75 , val_loss_mean:0.9945887298112925\n",
      "Epoch 00087: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 1.010\n",
      "Epoch 00074: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.972\n",
      "Epoch 00037: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:80 , val_loss_mean:1.000475552932563\n",
      "Epoch 00045: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.990\n",
      "Epoch 00059: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00064: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00035: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.054\n",
      "size:85 , val_loss_mean:0.999098368823273\n",
      "Epoch 00036: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00056: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00040: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.993\n",
      "Epoch 00033: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00036: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.051\n",
      "size:90 , val_loss_mean:0.9974389820081926\n",
      "Epoch 00058: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.999\n",
      "Epoch 00071: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.954\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.987\n",
      "Epoch 00047: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:95 , val_loss_mean:0.9948091308396705\n",
      "Epoch 00057: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.995\n",
      "Epoch 00067: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.965\n",
      "Epoch 00040: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.992\n",
      "Epoch 00032: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00026: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:100 , val_loss_mean:0.997403977341019\n"
     ]
    }
   ],
   "source": [
    "sizes= range(15,X.shape[1],5)\n",
    "train_loss,score_loss= testNodeNum(X_scaled,y,sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(15, 104, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npZSKYpoElFIqiv1/XRn3AdGcjBIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f2f0cbfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 30\n"
     ]
    }
   ],
   "source": [
    "print(sizes)\n",
    "loss = (np.mean(score_loss,axis=1))\n",
    "loss_std = (np.std(score_loss,axis=1))\n",
    "plt.plot(sizes,loss)\n",
    "plt.fill_between(sizes,loss-loss_std,loss+loss_std,alpha=0.3)\n",
    "plt.show()\n",
    "print(\"max: {}\".format(sizes[np.argmin(loss,axis=0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def futureTest2(node_sizes, X,y,ori_dates, numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(ori_dates)\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    model = createModel(node_sizes,X.shape[1])\n",
    "    results = None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            print(\"week{}\".format(w))\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "            history = model.fit(X_train,y_train,verbose=0,nb_epoch=500, validation_split=0.1, callbacks=[earlyCallback])\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = model.predict_proba(X_test)\n",
    "            train_proba =model.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            tresult = np.hstack([np.array([w for i in range(proba.shape[0])]).reshape(proba.shape[0],1),\n",
    "                                 ori_dates[start:last].reshape(proba.shape[0],1),stack,proba,y_test])\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "                results =    tresult\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "                results =  np.vstack([results, tresult])\n",
    "            if verbose == True:\n",
    "                print(\"numOftest {} , loss {}\".format(X_test.shape[0],model.evaluate(X_test,y_test)))               \n",
    "                print (tresult)\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "    start = X.shape[0] +index+1\n",
    "    print(\"start compute precision_mat\")\n",
    "    print(X[0:start,:].shape)\n",
    "    print(y[0:start,:].shape)\n",
    "    _,_, proba_test,proba_y = crossValidate2(node_sizes,X[0:start,:],y[0:start,:],fold=10)\n",
    "    p_matrix = precisionMatrix(np.vstack(proba_test),np.vstack(proba_y))\n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    resultdf= pd.DataFrame(results, columns=['week','DayStamp','HomeTeam','AwayTeam','H_prob','D_prob','A_prob','H','D','A'])\n",
    "    return sum_proba, sum_y,resultdf,p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week0\n",
      "Epoch 00102: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1348/1348 [==============================] - 0s     \n",
      "week1\n",
      "Epoch 00023: early stopping\n",
      "5/5 [==============================] - 0s\n",
      "1343/1343 [==============================] - 0s     \n",
      "week2\n",
      "Epoch 00021: early stopping\n",
      "15/15 [==============================] - 0s\n",
      "1328/1328 [==============================] - 0s     \n",
      "week3\n",
      "Epoch 00023: early stopping\n",
      "13/13 [==============================] - 0s\n",
      "1315/1315 [==============================] - 0s     \n",
      "week4\n",
      "Epoch 00029: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1305/1305 [==============================] - 0s     \n",
      "week5\n",
      "Epoch 00023: early stopping\n",
      "12/12 [==============================] - 0s\n",
      "1293/1293 [==============================] - 0s     \n",
      "week6\n",
      "Epoch 00026: early stopping\n",
      "8/8 [==============================] - 0s\n",
      "1285/1285 [==============================] - 0s     \n",
      "week7\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1275/1275 [==============================] - 0s     \n",
      "week8\n",
      "Epoch 00021: early stopping\n",
      "17/17 [==============================] - 0s\n",
      "1258/1258 [==============================] - 0s     \n",
      "week9\n",
      "Epoch 00021: early stopping\n",
      "3/3 [==============================] - 0s\n",
      "1255/1255 [==============================] - 0s     \n",
      "week10\n",
      "Epoch 00023: early stopping\n",
      "11/11 [==============================] - 0s\n",
      "1244/1244 [==============================] - 0s     \n",
      "week11\n",
      "Epoch 00025: early stopping\n",
      "19/19 [==============================] - 0s\n",
      "1225/1225 [==============================] - 0s     \n",
      "week12\n",
      "Epoch 00023: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1215/1215 [==============================] - 0s     \n",
      "week13\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1205/1205 [==============================] - 0s     \n",
      "week14\n",
      "Epoch 00025: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1195/1195 [==============================] - 0s     \n",
      "week15\n",
      "Epoch 00024: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1185/1185 [==============================] - 0s     \n",
      "week16\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1175/1175 [==============================] - 0s     \n",
      "week17\n",
      "Epoch 00025: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1165/1165 [==============================] - 0s     \n",
      "week18\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1155/1155 [==============================] - 0s     \n",
      "week19\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1145/1145 [==============================] - 0s     \n",
      "week20\n",
      "Epoch 00024: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1135/1135 [==============================] - 0s     \n",
      "week21\n",
      "Epoch 00023: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1125/1125 [==============================] - 0s     \n",
      "week22\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1115/1115 [==============================] - 0s     \n",
      "week23\n",
      "Epoch 00023: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1105/1105 [==============================] - 0s     \n",
      "week24\n",
      "Epoch 00024: early stopping\n",
      "8/8 [==============================] - 0s\n",
      "1097/1097 [==============================] - 0s     \n",
      "start compute precision_mat\n",
      "(1096, 104)\n",
      "(1096, 3)\n",
      "Epoch 00032: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [440 237 308], val_loss: 1.007\n",
      "Epoch 00032: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [440 237 308], val_loss: 0.981\n",
      "Epoch 00039: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [440 237 308], val_loss: 0.932\n",
      "Epoch 00034: early stopping\n",
      "110/110 [==============================] - 0s\n",
      "110/110 [==============================] - 0s\n",
      "986/986 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [440 237 309], val_loss: 0.998\n",
      "Epoch 00033: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [440 238 309], val_loss: 0.944\n",
      "Epoch 00036: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 6, Class dist.: [440 238 309], val_loss: 0.994\n",
      "Epoch 00033: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 7, Class dist.: [440 238 309], val_loss: 0.965\n",
      "Epoch 00032: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 8, Class dist.: [440 238 309], val_loss: 0.991\n",
      "Epoch 00030: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 9, Class dist.: [440 238 309], val_loss: 0.980\n",
      "Epoch 00058: early stopping\n",
      "108/108 [==============================] - 0s\n",
      "108/108 [==============================] - 0s\n",
      "988/988 [==============================] - 0s     \n",
      "Fold: 10, Class dist.: [441 238 309], val_loss: 1.055\n",
      "summary\n",
      "score:\n",
      "0.44061302682\n",
      "2like\n",
      "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
      ">80       0.8     1.0         18        6   0.750000          1        1   \n",
      "60-80     0.6     0.8         12       13   0.480000          2        1   \n",
      "50-60     0.5     0.6         14       12   0.538462          1        2   \n",
      "40-50     0.4     0.5         17       24   0.414634          6       11   \n",
      "30-40     0.3     0.4         29       34   0.460317         16       53   \n",
      "20-30     0.2     0.3          9       22   0.290323         24       51   \n",
      "<20       0.0     0.2         10       41   0.196078         18       74   \n",
      "\n",
      "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
      ">80     0.500000         16        5   0.761905  \n",
      "60-80   0.666667          9       15   0.375000  \n",
      "50-60   0.333333          3        6   0.333333  \n",
      "40-50   0.352941          9       17   0.346154  \n",
      "30-40   0.231884         17       37   0.314815  \n",
      "20-30   0.320000         18       38   0.321429  \n",
      "<20     0.195652         12       59   0.169014  \n",
      "sum precision:[ 0.4962406   0.25        0.44318182]\n"
     ]
    }
   ],
   "source": [
    "sum_proba, sum_y,resultdf,p_matrix= futureTest2([55,55],X_scaled,y,X[:,c.dateColumn],numOfWeek=25,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[lower</th>\n",
       "      <th>upper)</th>\n",
       "      <th>h_Correct</th>\n",
       "      <th>h_Wrong</th>\n",
       "      <th>h_Precent</th>\n",
       "      <th>d_Correct</th>\n",
       "      <th>d_Wrong</th>\n",
       "      <th>d_Precent</th>\n",
       "      <th>a_Correct</th>\n",
       "      <th>a_Wrong</th>\n",
       "      <th>a_Precent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&gt;80</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>181</td>\n",
       "      <td>72</td>\n",
       "      <td>0.715415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-60</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>0.514085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>59</td>\n",
       "      <td>0.628931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-50</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>72</td>\n",
       "      <td>83</td>\n",
       "      <td>0.464516</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>102</td>\n",
       "      <td>0.423729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30-40</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>84</td>\n",
       "      <td>129</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>62</td>\n",
       "      <td>152</td>\n",
       "      <td>0.289720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-30</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>55</td>\n",
       "      <td>166</td>\n",
       "      <td>0.248869</td>\n",
       "      <td>222</td>\n",
       "      <td>623</td>\n",
       "      <td>0.262722</td>\n",
       "      <td>55</td>\n",
       "      <td>154</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>17</td>\n",
       "      <td>85</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>22</td>\n",
       "      <td>170</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>41</td>\n",
       "      <td>278</td>\n",
       "      <td>0.128527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
       ">80       0.8     1.0          7        3   0.700000          0        0   \n",
       "60-80     0.6     0.8        181       72   0.715415          0        0   \n",
       "50-60     0.5     0.6         73       69   0.514085          0        0   \n",
       "40-50     0.4     0.5         72       83   0.464516          0        0   \n",
       "30-40     0.3     0.4         84      129   0.394366         20       39   \n",
       "20-30     0.2     0.3         55      166   0.248869        222      623   \n",
       "<20       0.0     0.2         17       85   0.166667         22      170   \n",
       "\n",
       "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
       ">80          NaN          0        0        NaN  \n",
       "60-80        NaN         10        8   0.555556  \n",
       "50-60        NaN        100       59   0.628931  \n",
       "40-50        NaN         75      102   0.423729  \n",
       "30-40   0.338983         62      152   0.289720  \n",
       "20-30   0.262722         55      154   0.263158  \n",
       "<20     0.114583         41      278   0.128527  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findRecordsBy(self,df):\n",
    "    #print((df['DayStamp']).values)\n",
    "    #date = convertToDate((df['DayStamp']).values)\n",
    "    #df[\"Date\"] = date\n",
    "    \n",
    "    home = df['HomeTeam'].values\n",
    "    away = df['AwayTeam'].values\n",
    "    origin = self.df[[\"Date\",\"HomeTeam\",\"AwayTeam\",\"JocH\",\"JocD\",\"JocA\"]]\n",
    "    origin[\"DayStamp\"]=(pd.to_numeric(origin['Date'])/1e9/24/60/60).values\n",
    "    origin[\"DayStamp\"] = origin[\"DayStamp\"].apply(lambda x: \"%.f\"%(float(x)))\n",
    "    df['DayStamp']=df['DayStamp'].apply(lambda x: \"%.f\"%(float(x)))\n",
    "    return origin.merge(df,left_on=['DayStamp',\"HomeTeam\",\"AwayTeam\"],right_on=[\"DayStamp\",\"HomeTeam\",\"AwayTeam\"],how='inner')\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "withodds = findRecordsBy(c,resultdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def formatMatrixs(oddDf , precisionDf):\n",
    "    proba_mat = oddDf[['H_prob','D_prob','A_prob']].values\n",
    "    def locatePrecision(proba_mat, precisionDf):\n",
    "        precisionMat = precisionDf.values\n",
    "        pre_cols=[4,7,10]\n",
    "        def convert(proba, pre_col = 4):\n",
    "            if proba < 0.2:\n",
    "                return proba\n",
    "            for i in range(precisionMat.shape[0]):\n",
    "                if precisionMat[i,0] <= proba and proba < precisionMat[i,1] :\n",
    "                    if math.isnan(precisionMat[i,pre_col]):\n",
    "                        return proba\n",
    "                    else:\n",
    "                        return precisionMat[i,pre_col]\n",
    "        h_fproba = np.array([ convert(float(proba)) for proba in proba_mat[:,0] ] )\n",
    "        d_fproba = np.array([ convert(float(proba),pre_col=7) for proba in proba_mat[:,1] ] )\n",
    "        a_fproba = np.array([ convert(float(proba),pre_col=10) for proba in proba_mat[:,2] ] )\n",
    "        return h_fproba, d_fproba,a_fproba\n",
    "    h_fproba, d_fproba,a_fproba =locatePrecision(proba_mat,precisionDf)\n",
    "    fproba_mat = np.hstack([h_fproba,d_fproba,a_fproba]).reshape(3,h_fproba.shape[0]).T\n",
    "    odd_mat = oddDf[['JocH','JocD','JocA']].values\n",
    "    win_mat = None\n",
    "    if 'H' in oddDf.columns:\n",
    "        win_mat = oddDf[['H','D','A']].values\n",
    "    return fproba_mat,odd_mat,win_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fproba_mat,odd_mat,win_mat= formatMatrixs(withodds,p_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strategy1(fproba_mat,odd_mat,win_mat):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    print(exp)\n",
    "    maxi = np.argmax(exp,axis=1)\n",
    "    print(maxi)\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    for i in range(maxi.shape[0]):\n",
    "        if exp[i,maxi[i]] > 1:\n",
    "            expectation = expectation+ exp[i,maxi[i]]\n",
    "            spent = spent+1\n",
    "            if maxi[i] == y_true[i]:\n",
    "                income = income + odd_mat[i,maxi[i]]\n",
    "            \n",
    "            \n",
    "        \n",
    "    print(\"Spent:{}, Income:{}, expectation:{}\".format(spent,income,expectation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.03339552e+00   1.04675237e+00   8.94297838e-01]\n",
      " [  8.21849315e-01   9.04600812e-01   6.44336531e-01]\n",
      " [  8.96994536e-01   9.36690647e-01   8.16753927e-01]\n",
      " [  1.00178082e+00   8.27063599e-01   8.51716738e-01]\n",
      " [  6.71511628e-01   1.08776978e+00   1.44656652e+00]\n",
      " [  6.96132597e-01   8.14140731e-01   1.36569106e+00]\n",
      " [  1.71044776e+00   6.39047328e-01   6.70557940e-01]\n",
      " [  7.53767123e-01   9.56292287e-01   8.99754539e-01]\n",
      " [  7.98142077e-01   9.06474820e-01   9.73821990e-01]\n",
      " [  8.80813953e-01   7.88294993e-01   9.32832618e-01]\n",
      " [  9.55161290e-01   3.13147581e-01   4.83380637e-01]\n",
      " [  1.91640922e-01   5.21662380e-01   1.16852630e+00]\n",
      " [  1.21575342e+00   7.62449256e-01   5.33796820e-01]\n",
      " [  1.36740331e+00   1.23884892e+00   6.78901734e-01]\n",
      " [  8.65616438e-01   8.27063599e-01   1.12210300e+00]\n",
      " [  9.94475138e-01   9.51798561e-01   8.56820809e-01]\n",
      " [  8.57734807e-01   8.27063599e-01   9.13005780e-01]\n",
      " [  1.37505661e-01   1.86258842e-01   2.04885686e+00]\n",
      " [  7.98208955e-01   7.75562357e-01   2.19266117e+00]\n",
      " [  9.33606557e-01   1.59804258e+00   4.86921009e-01]\n",
      " [  1.58216418e+00   8.39986468e-01   5.26554066e-01]\n",
      " [  7.73219178e-01   9.69215156e-01   1.24377682e+00]\n",
      " [  5.49180328e-01   1.11798561e+00   1.51416309e+00]\n",
      " [  8.75342466e-01   8.27063599e-01   6.65827831e-01]\n",
      " [  9.69253731e-01   1.05967524e+00   1.22541906e+00]\n",
      " [  8.99657534e-01   8.39986468e-01   6.53730836e-01]\n",
      " [  3.80386740e-01   9.17523681e-01   3.36910569e+00]\n",
      " [  1.05523256e+00   9.66906475e-01   6.94892704e-01]\n",
      " [  6.22465753e-01   1.18890392e+00   1.66541582e+00]\n",
      " [  2.72512452e-01   4.53988700e-01   1.17727273e+00]\n",
      " [  5.89226519e-01   8.01217862e-01   1.26416185e+00]\n",
      " [  1.97704918e+00   9.82138024e-01   4.71204188e-01]\n",
      " [  7.01252885e-01   1.05755396e+00   9.92682927e-01]\n",
      " [  1.07615672e+00   5.64165755e-01   1.51416309e+00]\n",
      " [  9.88524590e-01   9.06474820e-01   7.63350785e-01]\n",
      " [  1.41027397e+00   3.57897199e-01   7.13089005e-01]\n",
      " [  9.34838710e-01   2.44513988e-01   6.28706343e-01]\n",
      " [  2.85179987e-01   2.87325945e-01   1.87727273e+00]\n",
      " [  9.70218579e-01   7.88294993e-01   7.69633508e-01]\n",
      " [  1.09753731e+00   6.86168744e-01   1.48712446e+00]\n",
      " [  9.48387097e-01   1.63693526e-01   2.56565912e-01]\n",
      " [  9.51912568e-01   7.54695535e-01   8.10471204e-01]\n",
      " [  8.36438356e-01   8.78755074e-01   1.13562232e+00]\n",
      " [  1.35410448e+00   3.63497608e-01   9.05793991e-01]\n",
      " [  5.20114092e-01   4.51777467e-01   1.04363636e+00]\n",
      " [  1.12604478e+00   9.82138024e-01   3.13354857e-01]\n",
      " [  3.94726338e-01   6.40660628e-01   1.01818182e+00]\n",
      " [  8.23770492e-01   1.39031401e+00   7.57081545e-01]\n",
      " [  6.85684932e-01   1.04675237e+00   1.25185250e+00]\n",
      " [  3.11647685e-01   4.88010579e-01   2.06818182e+00]\n",
      " [  6.46780822e-01   1.13721245e+00   2.00085837e+00]\n",
      " [  7.02950820e-01   9.66906475e-01   9.59871245e-01]\n",
      " [  7.45856354e-01   9.06474820e-01   7.00523560e-01]\n",
      " [  5.47616327e-01   2.29622126e-01   1.17727273e+00]\n",
      " [  8.37209302e-01   9.36690647e-01   6.95598212e-01]\n",
      " [  7.43224044e-01   9.21582734e-01   1.06806283e+00]\n",
      " [  9.76380597e-01   6.34155348e-01   1.21117437e+00]\n",
      " [  8.96994536e-01   9.66906475e-01   6.89484979e-01]\n",
      " [  6.93313953e-01   8.91677943e-01   1.39248927e+00]\n",
      " [  5.75581395e-01   1.32949640e+00   2.09549356e+00]\n",
      " [  1.03339552e+00   2.56110741e-01   1.51416309e+00]\n",
      " [  9.81104651e-01   7.88294993e-01   7.89527897e-01]\n",
      " [  9.02616279e-01   5.80904518e-01   1.02094241e+00]\n",
      " [  4.20103878e-01   6.18247151e-01   1.14545455e+00]\n",
      " [  1.10466418e+00   1.00798376e+00   7.59614448e-01]\n",
      " [  6.14088398e-01   3.70572278e-01   1.62272727e+00]\n",
      " [  2.45354256e-01   1.08552097e+00   8.90909091e-01]\n",
      " [  7.58132958e-02   5.00004508e-01   1.44688863e+00]\n",
      " [  7.36918605e-01   6.32403348e-01   1.41361257e+00]\n",
      " [  7.87808219e-01   9.04600812e-01   9.16679677e-01]\n",
      " [  1.08328358e+00   6.37020170e-01   8.60793816e-01]\n",
      " [  8.53060109e-01   7.88294993e-01   8.79581152e-01]\n",
      " [  1.12063953e+00   7.36603518e-01   8.41884817e-01]\n",
      " [  7.06395349e-01   9.17523681e-01   1.25729614e+00]\n",
      " [  9.09383562e-01   5.05342014e-01   1.13089005e+00]\n",
      " [  1.21823204e+00   3.89631589e-01   9.73636364e-01]\n",
      " [  8.85174419e-01   7.62449256e-01   1.11518325e+00]\n",
      " [  1.61287121e-01   1.27670658e-01   3.06937829e+00]\n",
      " [  7.60405710e-01   8.52909337e-01   1.08292683e+00]\n",
      " [  8.31575342e-01   8.27063599e-01   1.24377682e+00]\n",
      " [  6.73661202e-01   1.57670450e+00   1.02746781e+00]\n",
      " [  8.45303867e-01   5.70589891e-01   1.19121951e+00]\n",
      " [  7.08563536e-01   1.41272921e+00   6.08369099e-01]\n",
      " [  3.34407225e-01   1.70552871e+00   8.11158798e-01]\n",
      " [  1.99383562e+00   8.78755074e-01   4.70472103e-01]\n",
      " [  9.15697674e-01   8.14140731e-01   8.38197425e-01]\n",
      " [  5.45058140e-01   1.25351827e+00   2.50107296e+00]\n",
      " [  8.71366120e-01   8.27063599e-01   8.23036649e-01]\n",
      " [  5.71147541e-01   1.04244604e+00   1.48712446e+00]\n",
      " [  7.92671233e-01   9.43369418e-01   7.90387785e-01]\n",
      " [  7.76162791e-01   1.01223022e+00   1.06802575e+00]\n",
      " [  9.19365672e-01   2.29741203e-01   1.43122288e+00]\n",
      " [  5.00604312e-01   4.36724567e-01   1.27272727e+00]\n",
      " [  8.75342466e-01   9.82014388e-01   3.37894499e-01]\n",
      " [  9.28064516e-01   9.09550488e-02   2.22816122e-01]\n",
      " [  4.64917127e-01   1.39262190e+00   1.01394850e+00]\n",
      " [  1.93850746e+00   9.82014388e-01   2.00374419e-01]\n",
      " [  5.96775956e-01   1.57006081e+00   8.67056707e-01]\n",
      " [  7.82945205e-01   5.57346869e-01   1.21673820e+00]\n",
      " [  5.71220930e-01   1.11136671e+00   2.27124464e+00]\n",
      " [  4.70169378e-01   4.34346958e-01   1.05000000e+00]\n",
      " [  8.19767442e-01   8.39986468e-01   9.86909871e-01]\n",
      " [  7.45856354e-01   5.67389138e-01   1.32357724e+00]\n",
      " [  1.07703488e+00   1.67862546e+00   2.65069789e-02]\n",
      " [  1.33633880e+00   2.32479954e-01   1.14308943e+00]\n",
      " [  8.16448087e-01   9.21582734e-01   9.26701571e-01]\n",
      " [  5.22346156e-01   3.30033470e-01   1.25917685e+00]\n",
      " [  9.51912568e-01   8.76258993e-01   8.16753927e-01]\n",
      " [  1.63517442e+00   8.27063599e-01   5.87434555e-01]\n",
      " [  6.85684932e-01   4.13146099e-01   1.85214592e+00]\n",
      " [  8.33843284e-01   1.13053545e+00   1.96443340e+00]\n",
      " [  1.21935484e+00   2.90106805e-01   1.78560455e-01]\n",
      " [  1.09836066e+00   8.01217862e-01   6.84816754e-01]\n",
      " [  8.38882059e-01   1.08776978e+00   7.25722543e-01]\n",
      " [  1.01991143e-01   1.24971159e-01   1.59812311e+00]\n",
      " [  1.09011628e+00   5.68221164e-01   8.10471204e-01]\n",
      " [  9.76380597e-01   1.11136671e+00   3.60905512e-01]\n",
      " [  8.60753425e-01   8.52909337e-01   6.56655521e-01]\n",
      " [  8.72093023e-01   5.43427213e-01   1.09947644e+00]\n",
      " [  1.38006874e-02   1.03258871e-02   1.92609039e+00]\n",
      " [  8.85174419e-01   9.21582734e-01   9.19313305e-01]\n",
      " [  1.46075581e+00   7.88294993e-01   5.54291845e-01]\n",
      " [  7.02950820e-01   9.36690647e-01   1.00042918e+00]\n",
      " [  2.64921875e-01   3.27873094e+00   3.53095103e-01]\n",
      " [  1.24720149e+00   1.08776978e+00   9.91997821e-02]\n",
      " [  5.31850800e-01   9.95511957e-02   1.32363636e+00]\n",
      " [  9.81104651e-01   9.51798561e-01   7.65193133e-01]\n",
      " [  4.88372093e-01   1.70581867e+00   3.78540773e+00]\n",
      " [  8.78688525e-01   7.62449256e-01   8.73298429e-01]\n",
      " [  1.24480874e+00   8.52909337e-01   6.06282723e-01]\n",
      " [  8.08011050e-01   7.88294993e-01   1.26341463e+00]\n",
      " [  8.55890411e-01   9.51798561e-01   6.33354516e-01]\n",
      " [  9.55000000e-01   6.93945902e-01   1.33838864e+00]\n",
      " [  9.59302326e-01   8.52909337e-01   8.79581152e-01]\n",
      " [  8.15406977e-01   8.27063599e-01   1.17801047e+00]\n",
      " [  3.36839276e-01   6.08010713e-01   1.15818182e+00]\n",
      " [  5.71823204e-01   9.06474820e-01   9.04712042e-01]\n",
      " [  2.62886292e-01   3.29362514e-01   1.65454545e+00]\n",
      " [  8.19767442e-01   8.39986468e-01   1.14659686e+00]\n",
      " [  9.14516129e-01   4.12867550e-02   1.98349910e-01]\n",
      " [  1.36835821e+00   8.91677943e-01   1.43820427e-01]\n",
      " [  5.88424658e-01   2.09989834e+00   4.77747994e-01]\n",
      " [  8.67704918e-01   9.21582734e-01   8.57591623e-01]\n",
      " [  1.07273224e+00   2.40450630e-01   1.29349593e+00]\n",
      " [  5.62500000e-01   1.13721245e+00   2.40643777e+00]\n",
      " [  9.19365672e-01   7.58491150e-01   1.12789330e+00]\n",
      " [  3.72440835e-02   9.47863490e-03   1.44707633e+00]\n",
      " [  1.74608209e+00   9.21582734e-01   1.20295787e-01]\n",
      " [  1.83761191e-02   2.64767806e+00   3.12102716e-01]\n",
      " [  6.76243094e-01   1.39656712e+00   7.06806283e-01]\n",
      " [  5.88424658e-01   9.80976135e-01   2.70386266e+00]\n",
      " [  1.07703488e+00   5.20977542e-01   1.22670520e+00]\n",
      " [  1.15327869e+00   5.82327513e-01   9.73872832e-01]\n",
      " [  9.14246575e-01   6.15072458e-01   9.46351931e-01]\n",
      " [  9.01644878e-02   1.67874487e-01   1.39238843e+00]\n",
      " [  3.21011487e-01   6.61458778e-01   1.08181818e+00]\n",
      " [  8.50290698e-01   8.39986468e-01   9.19313305e-01]\n",
      " [  4.17907541e-01   2.69245367e+00   1.85551586e-02]\n",
      " [  2.37645349e+00   6.88521758e-01   6.92947977e-01]\n",
      " [  5.14534884e-01   1.69208633e+00   2.11249776e+00]\n",
      " [  8.94193548e-01   9.10416106e-02   1.02643023e+00]\n",
      " [  9.19365672e-01   1.42014388e+00   3.90208983e-01]\n",
      " [  5.19890710e-01   1.19352518e+00   1.99476440e+00]\n",
      " [  8.28488372e-01   8.39986468e-01   9.73390558e-01]\n",
      " [  8.67704918e-01   8.39986468e-01   8.16753927e-01]\n",
      " [  1.46094763e-01   2.29402950e-01   1.35498400e+00]\n",
      " [  6.56506849e-01   7.75505200e-01   1.96030043e+00]\n",
      " [  3.12508962e-01   1.93470579e+00   6.21888412e-01]\n",
      " [  9.95806452e-01   2.20081717e-01   2.70304743e-01]\n",
      " [  1.96451613e+00   2.18196591e-01   2.31163478e-01]\n",
      " [  2.43119490e-01   7.07718101e-01   2.86363636e+00]\n",
      " [  6.33977901e-01   1.71763033e+00   4.54450054e-01]\n",
      " [  1.61620302e-01   7.62449256e-01   1.64181818e+00]\n",
      " [  5.34535519e-01   4.99109919e-01   2.76242775e+00]\n",
      " [  1.01914179e+00   3.03913258e-01   1.62231760e+00]\n",
      " [  8.68865753e-02   2.13363158e-01   1.31686422e+00]\n",
      " [  2.67957864e-01   2.10250926e-01   1.93464269e+00]\n",
      " [  9.22529302e-02   2.10837483e-01   1.61495983e+00]\n",
      " [  1.24645161e+00   1.98217381e-01   1.22647891e-01]\n",
      " [  1.11179104e+00   2.80117504e-01   1.52356021e+00]\n",
      " [  6.36627907e-01   9.69215156e-01   1.94764398e+00]\n",
      " [  1.00258065e+00   2.92420697e-01   5.71834257e-01]\n",
      " [  8.87419355e-01   2.15686502e-01   4.45616917e-02]\n",
      " [  1.24692010e+00   3.63485859e-01   8.52727273e-01]\n",
      " [  7.48904110e-01   6.67449895e-01   1.29785408e+00]\n",
      " [  5.71147541e-01   9.56292287e-01   1.57068063e+00]\n",
      " [  4.50327869e-01   1.22767253e+00   5.15028902e+00]\n",
      " [  1.16712329e+00   1.45483962e+00   4.79532979e-02]\n",
      " [  1.20580110e+00   1.69298437e+00   4.29914163e-01]\n",
      " [  1.33633880e+00   8.39986468e-01   8.80231214e-01]\n",
      " [  1.21156716e+00   3.32882882e-01   5.79300523e-01]\n",
      " [  9.02616279e-01   9.97122302e-01   8.24678112e-01]\n",
      " [  1.01598837e+00   9.66906475e-01   7.30042918e-01]\n",
      " [  4.83278689e-01   1.12428958e+00   2.51308901e+00]\n",
      " [  6.32267442e-01   1.19352518e+00   1.59527897e+00]\n",
      " [  9.34838710e-01   4.73811065e-01   1.60080189e-01]\n",
      " [  9.07458564e-01   1.40130129e+00   5.90575916e-01]\n",
      " [  1.55806452e+00   2.49231873e-04   8.58008229e-04]\n",
      " [  1.60478805e-02   3.16636171e-02   1.55143930e+00]\n",
      " [  5.16695956e-02   3.84867281e-02   2.18171324e+00]\n",
      " [  8.48097015e-01   5.69696929e-01   2.83905579e+00]\n",
      " [  6.95628415e-01   9.82014388e-01   9.73390558e-01]\n",
      " [  1.04068493e+00   9.66906475e-01   2.53780581e-01]\n",
      " [  1.32559701e+00   9.97122302e-01   1.78083093e-01]\n",
      " [  1.30813953e+00   8.39986468e-01   5.73218884e-01]\n",
      " [  8.96994536e-01   9.51798561e-01   6.97596567e-01]\n",
      " [  4.06700091e-01   4.02966645e-01   1.60363636e+00]\n",
      " [  6.83701657e-01   5.09227797e-01   1.52727273e+00]\n",
      " [  6.77322404e-01   9.82014388e-01   1.02746781e+00]\n",
      " [  4.89779006e-01   9.36690647e-01   1.11518325e+00]\n",
      " [  1.35464481e+00   5.12579687e-01   8.42774566e-01]\n",
      " [  1.43962687e+00   1.48283965e-01   8.38197425e-01]\n",
      " [  9.55000000e-01   3.23276963e-01   9.60548416e-01]\n",
      " [  4.62430939e-01   2.23629055e+00   2.62862159e-01]\n",
      " [  1.59193548e+00   4.22450248e-02   2.69058820e-01]\n",
      " [  5.78794826e-03   1.19922328e-02   1.45414041e+00]\n",
      " [  1.58216418e+00   2.27860844e-01   7.62489270e-01]\n",
      " [  1.11774194e+00   7.08793098e-01   7.46402484e-03]\n",
      " [  8.46774194e-01   4.15042508e-03   9.52196633e-02]\n",
      " [  1.28633721e+00   5.80225754e-01   6.81675393e-01]\n",
      " [  8.89931507e-01   1.98124890e-01   1.14659686e+00]\n",
      " [  8.69477612e-01   1.34397835e+00   1.05636127e+00]\n",
      " [  6.88306011e-01   8.01217862e-01   1.20942408e+00]\n",
      " [  3.38475084e-02   2.72890878e-01   2.22618645e+00]\n",
      " [  9.07978142e-01   9.82014388e-01   7.79057592e-01]\n",
      " [  6.51693989e-01   1.02733813e+00   1.22513089e+00]\n",
      " [  7.85806452e-01   5.16097235e-03   8.99702269e-02]\n",
      " [  6.07759563e-01   8.78755074e-01   2.15375723e+00]\n",
      " [  7.23837209e-01   1.45023282e+00   4.35141271e-01]\n",
      " [  4.84806630e-01   1.74854800e+00   5.79523471e-01]\n",
      " [  5.64364641e-01   1.55447647e+00   7.84120172e-01]\n",
      " [  1.28633721e+00   1.58360463e+00   2.37734147e-01]\n",
      " [  1.51089552e+00   4.20896562e-01   4.22614129e-01]\n",
      " [  7.40883978e-01   9.36690647e-01   6.91099476e-01]\n",
      " [  3.29293566e-01   4.52305037e-01   1.84545455e+00]\n",
      " [  1.51939891e+00   7.96070185e-02   1.05636364e+00]\n",
      " [  7.50546448e-01   9.97122302e-01   9.73821990e-01]\n",
      " [  1.77500000e+00   1.46931089e-01   5.68586387e-01]\n",
      " [  1.05523256e+00   9.51798561e-01   4.56004996e-01]\n",
      " [  7.65483871e-01   1.72685177e-01   1.96131371e+00]\n",
      " [  2.22383721e+00   1.82276691e-01   9.14471545e-01]\n",
      " [  5.05245902e-01   1.23884892e+00   2.16753927e+00]\n",
      " [  1.31847015e+00   8.39986468e-01   2.17787395e-01]\n",
      " [  1.09041045e+00   9.56292287e-01   4.14368177e-01]\n",
      " [  1.09836066e+00   9.21582734e-01   6.00257511e-01]\n",
      " [  7.71802326e-01   8.91677943e-01   1.05450644e+00]\n",
      " [  1.62492537e+00   9.97122302e-01   6.86004929e-02]\n",
      " [  7.61530055e-01   9.21582734e-01   1.03664921e+00]\n",
      " [  5.78469945e-01   1.08776978e+00   1.53926702e+00]\n",
      " [  1.80405645e-03   1.83662510e+00   1.47485549e+00]\n",
      " [  8.26716418e-01   8.23329985e-01   3.24463519e+00]\n",
      " [  2.48495970e-02   6.60106134e-04   2.78893182e+00]\n",
      " [  1.65343284e+00   6.15739717e-01   3.40222833e-02]\n",
      " [  1.46100746e+00   2.93872774e-01   9.89528796e-01]\n",
      " [  3.65085643e-01   1.43844364e-01   1.51902822e+00]\n",
      " [  1.16516129e+00   2.90539179e-03   7.48058494e-02]\n",
      " [  4.96400194e-03   5.28669753e-01   1.83152432e+00]\n",
      " [  7.99354839e-01   2.04358448e-04   5.42034303e-03]\n",
      " [  1.40956284e+00   8.65832206e-01   8.42774566e-01]\n",
      " [  1.39110927e-01   1.62740107e+00   8.90406504e-01]\n",
      " [  1.28283582e+00   4.51823559e-01   1.08154506e+00]]\n",
      "[1 1 1 0 2 2 0 1 2 2 0 2 0 0 2 0 2 2 2 1 0 2 2 0 2 0 2 0 2 2 2 0 1 2 0 0 0\n",
      " 2 0 2 0 0 2 0 2 0 2 1 2 2 2 1 1 2 1 2 2 1 2 2 2 0 2 2 0 2 1 2 2 2 0 2 0 2\n",
      " 2 0 2 2 2 2 1 2 1 1 0 0 2 0 2 1 2 2 2 1 0 1 0 1 2 2 2 2 2 1 0 2 2 0 0 2 2\n",
      " 0 0 1 2 0 1 0 2 2 1 0 2 1 0 2 0 2 0 0 2 1 2 0 2 2 1 2 2 0 0 1 1 2 2 2 2 0\n",
      " 1 1 2 2 0 2 2 2 2 1 0 2 2 1 2 2 0 2 2 1 0 0 2 1 2 2 2 2 2 2 0 2 2 0 0 0 2\n",
      " 2 2 1 1 0 0 1 0 2 2 0 1 0 2 2 2 1 0 0 0 1 2 2 2 2 0 0 2 1 0 2 0 0 0 0 2 1\n",
      " 2 2 1 2 0 2 1 1 1 1 0 1 2 0 1 0 0 2 0 2 0 0 0 2 0 2 2 1 2 2 0 0 2 0 2 0 0\n",
      " 1 0]\n",
      "Spent:205, Income:258.19, expectation:322.188139700667\n"
     ]
    }
   ],
   "source": [
    "strategy1(fproba_mat,odd_mat,win_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy3(fproba_mat,odd_mat,win_mat,info, z = 0.5  ):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    total = fproba_mat.shape[0]\n",
    "    withdraw = 0\n",
    "    buy = 0\n",
    "    homeTeam = info[\"HomeTeam\"].values\n",
    "    awayTeam = info[\"AwayTeam\"].values\n",
    "    receipt = []\n",
    "    def chooseFrom(match, chosable):\n",
    "        max_prob = -1\n",
    "        c = 0\n",
    "        for choice in chosable:\n",
    "            if fproba_mat[match,choice] > max_prob:\n",
    "                c = choice\n",
    "                max_prob = fproba_mat[match,choice]\n",
    "        return c\n",
    "    for match in range(exp.shape[0]):\n",
    "        chosable = []\n",
    "        for choose in range(3):\n",
    "            if exp[match,choose]> 1 and fproba_mat[match,choose] >= z:\n",
    "                chosable.append(choose)\n",
    "                \n",
    "        if len(chosable) == 0:\n",
    "            withdraw= withdraw+1\n",
    "            continue\n",
    "        choice = chooseFrom(match,chosable)\n",
    "        expectation = expectation+ exp[match,choice]\n",
    "        spent = spent+1\n",
    "        buy =buy +1 \n",
    "        receipt.append(np.hstack([homeTeam[match],awayTeam[match], odd_mat[match,choice],choice,y_true[match],fproba_mat[match,:]]))\n",
    "        if choice == y_true[match]:\n",
    "            income = income + odd_mat[match,choice]\n",
    "    print(buy)\n",
    "    print(\"Spent:{}, Income:{}, expectation:{}, withdraw:{}(total:{})\".\n",
    "          format(spent,income,expectation,withdraw,total))\n",
    "    receiptDf = pd.DataFrame(receipt,columns=[\"home\",\"away\",\"odd of choice\",\"choice\",\"result\",\"Hp\",\"Dp\",\"Ap\"])\n",
    "    return spent,income,expectation,withdraw,total,receiptDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "Spent:90, Income:91.14999999999998, expectation:139.70355912289673, withdraw:171(total:261)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy3(fproba_mat,odd_mat,win_mat,withodds,z=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "receipt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_range = range(30,80,2)\n",
    "spents = []\n",
    "incomes = []\n",
    "exps = []\n",
    "withs = []\n",
    "for z in z_range:\n",
    "    fz = z/100\n",
    "    print(z)\n",
    "    spent,income,expectation,withdraw,total,_ = strategy3(fproba_mat,odd_mat,win_mat,withodds,z=fz) \n",
    "    spents.append(spent)\n",
    "    incomes.append(income)\n",
    "    exps.append(expectation)\n",
    "    withs.append(withdraw)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(z_range, spents)\n",
    "plt.plot(z_range, incomes,color='green')\n",
    "plt.plot(z_range, np.array(incomes)/np.array(spents)*100,color='red')\n",
    "plt.plot(z_range,exps,color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy4(fproba_mat,odd_mat,win_mat,info, z = 0.2 ,e =1.8 ):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    y_true=None\n",
    "    if win_mat is None:\n",
    "        y_true = np.zeros(shape=(fproba_mat.shape[0],))\n",
    "    else:\n",
    "        y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    total = fproba_mat.shape[0]\n",
    "    withdraw = 0\n",
    "    buy = 0\n",
    "    homeTeam = info[\"HomeTeam\"].values\n",
    "    awayTeam = info[\"AwayTeam\"].values\n",
    "    receipt = []\n",
    "    def chooseFrom(match, chosable):\n",
    "        max_prob = -1\n",
    "        c = 0\n",
    "        for choice in chosable:\n",
    "            if fproba_mat[match,choice] > max_prob:\n",
    "                c = choice\n",
    "                max_prob = fproba_mat[match,choice]\n",
    "        return c\n",
    "    for match in range(exp.shape[0]):\n",
    "        chosable = []\n",
    "        for choose in range(3):\n",
    "            if exp[match,choose]> e and fproba_mat[match,choose] >= z:\n",
    "                chosable.append(choose)\n",
    "                \n",
    "        if len(chosable) == 0:\n",
    "            withdraw= withdraw+1\n",
    "            continue\n",
    "        choice = chooseFrom(match,chosable)\n",
    "        expectation = expectation+ exp[match,choice]\n",
    "        spent = spent+1\n",
    "        buy =buy +1 \n",
    "        receipt.append(np.hstack([homeTeam[match],awayTeam[match], odd_mat[match,choice],choice,y_true[match],fproba_mat[match,:]]))\n",
    "        if choice == y_true[match]:\n",
    "            income = income + odd_mat[match,choice]\n",
    "    #print(buy)\n",
    "    #print(\"Spent:{}, Income:{}, expectation:{}, withdraw:{}(total:{})\".\n",
    "    #      format(spent,income,expectation,withdraw,total))\n",
    "    receiptDf = pd.DataFrame(receipt,columns=[\"home\",\"away\",\"odd of choice\",\"choice\",\"result\",\"Hp\",\"Dp\",\"Ap\"])\n",
    "    return spent,income,expectation,withdraw,total,receiptDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,win_mat,withodds,z=0.2,e=1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_z_range = np.arange(0,1,0.05)\n",
    "g_e_range = np.arange(1,2,0.05)\n",
    "g_incomes = []\n",
    "g_spents =[]\n",
    "g_withdraws=[]\n",
    "g_exps=[]\n",
    "g_total = 0\n",
    "for e in g_e_range:\n",
    "    for z in g_z_range:\n",
    "        spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,win_mat,withodds,z=z,e=e)\n",
    "        g_spents.append(spent)\n",
    "        g_incomes.append(income)\n",
    "        g_withdraws.append(withdraw)\n",
    "        g_exps.append(expectation)\n",
    "        g_total= total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f2dc5a588>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEKCAYAAADDzOROAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZjlyCJuZ5cghbGaWo/8PWr9YbLo49QMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f23729710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAEKCAYAAABT352BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f2644dac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEKCAYAAADDzOROAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1ee6rxGOIIUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f2b357080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWMAAAEKCAYAAADHOTRzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzKwC/j8VS0+cAAAABElEQVR3TFx0JC2s+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f2c426ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f2dc5a588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "g_n_incomes = np.array(g_incomes).reshape(len(g_e_range),len(g_z_range))\n",
    "g_n_spent = np.array(g_spents).reshape(len(g_e_range),len(g_z_range))\n",
    "g_n_withdraws = np.array(g_withdraws).reshape(len(g_e_range),len(g_z_range))\n",
    "g_n_exps = np.array(g_exps).reshape(len(g_e_range),len(g_z_range))\n",
    "Z, E =np.meshgrid(g_z_range,g_e_range)\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_incomes)\n",
    "plt.title(\"Income\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_incomes-g_n_spent)\n",
    "plt.title(\"Net Income\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_withdraws)\n",
    "plt.title(\"Withdraws\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_exps - g_n_incomes)\n",
    "plt.title(\"Exps - income\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_node_sizes=[55,55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPrecisionMaxtrix(node_sizes,c):\n",
    "    X, y = c.getH7(removeInsufficient=True)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    _,_, proba_test,proba_y = crossValidate2(node_sizes,X_scaled,y,fold=10)\n",
    "    p_matrix = precisionMatrix(np.vstack(proba_test),np.vstack(proba_y)) \n",
    "    return p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "Epoch 00087: early stopping\n",
      "137/137 [==============================] - 0s     \n",
      "137/137 [==============================] - 0s     \n",
      "1221/1221 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [539 298 384], val_loss: 1.028\n",
      "Epoch 00101: early stopping\n",
      "137/137 [==============================] - 0s     \n",
      "137/137 [==============================] - 0s     \n",
      "1221/1221 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [539 298 384], val_loss: 1.030\n",
      "Epoch 00087: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [539 299 384], val_loss: 0.948\n",
      "Epoch 00090: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [539 299 384], val_loss: 0.963\n",
      "Epoch 00097: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [539 299 384], val_loss: 1.015\n",
      "Epoch 00074: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 6, Class dist.: [539 299 384], val_loss: 1.001\n",
      "Epoch 00046: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 7, Class dist.: [539 299 384], val_loss: 0.984\n",
      "Epoch 00060: early stopping\n",
      "135/135 [==============================] - 0s     \n",
      "135/135 [==============================] - 0s     \n",
      "1223/1223 [==============================] - 0s     \n",
      "Fold: 8, Class dist.: [539 299 385], val_loss: 1.052\n",
      "Epoch 00037: early stopping\n",
      "135/135 [==============================] - 0s     \n",
      "135/135 [==============================] - 0s     \n",
      "1223/1223 [==============================] - 0s     \n",
      "Fold: 9, Class dist.: [539 299 385], val_loss: 1.034\n",
      "Epoch 00027: early stopping\n",
      "134/134 [==============================] - 0s     \n",
      "134/134 [==============================] - 0s     \n",
      "1224/1224 [==============================] - 0s     \n",
      "Fold: 10, Class dist.: [540 299 385], val_loss: 1.027\n"
     ]
    }
   ],
   "source": [
    "g_p_matrix= getPrecisionMaxtrix(g_node_sizes,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainedModel(node_sizes,c):\n",
    "    X, y = c.getH7(removeInsufficient=True)\n",
    "    X_scaled = StandardScaler().fit_transform(X) \n",
    "    model = createModel(node_sizes,X_scaled.shape[1])\n",
    "    earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "    history = model.fit(X_scaled,y,verbose=0,nb_epoch=500, validation_split=0.1, callbacks=[earlyCallback])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "Epoch 00092: early stopping\n"
     ]
    }
   ],
   "source": [
    "g_model = getTrainedModel(g_node_sizes,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def predictFuture(node_sizes,c,p_matrix,model):\n",
    "    X_test,y_test = c.getH7(removeInsufficient = True,future=True)\n",
    "    X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "    proba_y = model.predict_proba(X_test_scaled)\n",
    "   \n",
    "    def getTestDf (X_test,proba_y,c):\n",
    "        decoded = oneHotDecode(c, X_test)\n",
    "        homeNames = c.inverseTeamMapping(decoded[:,0])\n",
    "        awayNames = c.inverseTeamMapping(decoded[:,1])\n",
    "        names = np.array([homeNames,awayNames]).T\n",
    "        return pd.DataFrame(np.hstack([names,proba_y]),columns=['HomeTeam','AwayTeam','H_prob','D_prob','A_prob'])\n",
    "   \n",
    "    test_df = getTestDf(X_test,proba_y,c)\n",
    "    test_df = test_df.sort(columns=\"HomeTeam\")\n",
    "    originDf = c.df[c.df[\"Future\"]==1].sort(columns=\"HomeTeam\")\n",
    "    test_df['JocH']=originDf['JocH'].values\n",
    "    test_df['JocD']=originDf['JocD'].values\n",
    "    test_df['JocA']=originDf['JocA'].values\n",
    "    fproba_mat,odd_mat,_= formatMatrixs(test_df,p_matrix)\n",
    "    return fproba_mat,odd_mat,test_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "7/7 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:15: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:16: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "fproba_mat,odd_mat,test_df=predictFuture(g_node_sizes,c,g_p_matrix,g_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Spent:1, Income:0, expectation:1.6114285714285712, withdraw:6(total:7)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,None,test_df,z=0.2,e=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "      <th>odd of choice</th>\n",
       "      <th>choice</th>\n",
       "      <th>result</th>\n",
       "      <th>Hp</th>\n",
       "      <th>Dp</th>\n",
       "      <th>Ap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2436548223350254</td>\n",
       "      <td>0.24871794871794872</td>\n",
       "      <td>0.5714285714285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        home       away odd of choice choice result                  Hp  \\\n",
       "0  Liverpool  Tottenham          2.82      2    0.0  0.2436548223350254   \n",
       "\n",
       "                    Dp                  Ap  \n",
       "0  0.24871794871794872  0.5714285714285714  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipt.to_csv(\"2016-4-3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
