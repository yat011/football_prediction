{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    from IPython.core.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "import sys\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotCurve(train_mean, train_std,test_mean,test_std,sizes):\n",
    "    plt.plot(sizes, train_mean, \n",
    "            color='blue', marker='o', \n",
    "            markersize=5, \n",
    "            label='training accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                  train_mean + train_std,\n",
    "                   train_mean - train_std, alpha=0.15, color='blue')\n",
    "\n",
    "    plt.plot(sizes, test_mean, \n",
    "              color='green', linestyle='--', \n",
    "              marker='s', markersize=5, \n",
    "             label='validation accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                      test_mean + test_std,\n",
    "                     test_mean - test_std, \n",
    "                    alpha=0.15, color='green')\n",
    "    plt.xlabel('x_range')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "def likehoodScore(proba,y):\n",
    "    return np.sum(proba * y)/proba.shape[0]\n",
    "\n",
    "def firstNScore(n, pred, y):\n",
    "    backup = np.array(pred, copy =True)\n",
    "    for r in range(pred.shape[0]):\n",
    "        row = backup[r]\n",
    "        s = np.sort(row)\n",
    "        for c in range(pred.shape[1]):\n",
    "            temp = backup[r][c]\n",
    "            backup[r][c] = False\n",
    "            for j in range(1,n+1):\n",
    "                if temp == s[-j]:\n",
    "                    backup[r][c] = True\n",
    "                    break\n",
    "    res = np.sum(np.logical_and(backup,y))/pred.shape[0]\n",
    "    return res               \n",
    "\n",
    "def oneHotDecode(self, X_sample):\n",
    "    result=None\n",
    "    fiPos = 0\n",
    "    colIndex = 0\n",
    "    while colIndex < X_sample.shape[1]:\n",
    "        if fiPos < len(self.ohe.n_values_) and colIndex == self.ohe.feature_indices_[fiPos]:                \n",
    "            start = self.ohe.feature_indices_[fiPos]\n",
    "            end_ = start+ self.ohe.n_values_[fiPos]\n",
    "            #print(\"start{} end{}\".format(start,end_))\n",
    "            classes = np.argmax(X_sample[:,start:end_],axis=1).reshape(X_sample.shape[0],1)\n",
    "            if result is None:\n",
    "                result = classes\n",
    "            else:\n",
    "                result=np.hstack([result,classes])\n",
    "            colIndex = end_\n",
    "            fiPos = fiPos +1\n",
    "        else:\n",
    "            if result is None:\n",
    "                result = X_sample[:,colIndex:colIndex+1]\n",
    "            else:\n",
    "                result=np.hstack([result, X_sample[:,colIndex:colIndex+1]])\n",
    "            colIndex = colIndex +1\n",
    "        \n",
    "    return result \n",
    "def convertToDate(dayStamps):\n",
    "    res = [] \n",
    "    for v in dayStamps:\n",
    "        res.append(datetime.datetime.fromtimestamp(float(v)*24*60*60))\n",
    "    return res\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def precisionMatrix(proba, y):\n",
    "    def _precisionClassify(df,proba, wins, c =0 ):\n",
    "        for indx, v in enumerate(proba):\n",
    "            row = 0\n",
    "            col = 0\n",
    "            if wins[indx] == c:\n",
    "                col = 0\n",
    "            else:\n",
    "                col =1\n",
    "            if v <0.2:\n",
    "                row =6 \n",
    "            elif v < 0.3 and  v >=0.2:\n",
    "                row =5 \n",
    "            elif v < 0.4 and v >= 0.3:\n",
    "                row = 4 \n",
    "            elif v < 0.5 and v >= 0.4:\n",
    "                row = 3 \n",
    "            elif v < 0.6 and v >= 0.5:\n",
    "                row = 2 \n",
    "            elif v < 0.8 and v >= 0.6:\n",
    "                row = 1\n",
    "            df.iloc[row,col] = df.iloc[row,col]+1 \n",
    "        df[df.columns[2]] = df[df.columns[0]] /(df[df.columns[1]] + df[df.columns[0]])\n",
    "        return df\n",
    "    rowHeader = ['>80','60-80','50-60','40-50','30-40','20-30','<20']\n",
    "    df = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['h_Correct', 'h_Wrong','h_Precent'])\n",
    "    hproba = proba[:,0]\n",
    "    wins = np.argmax(y,axis=1)\n",
    "    df = _precisionClassify(df,hproba,wins)\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['d_Correct', 'd_Wrong','d_Precent'])\n",
    "    dproba = proba[:,1]\n",
    "    df = df.join(_precisionClassify(temp,dproba,wins,c=1))\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['a_Correct', 'a_Wrong','a_Precent'])\n",
    "    aproba = proba[:,2]\n",
    "    df = df.join(_precisionClassify(temp,aproba,wins,c=2))\n",
    "    \n",
    "    bound = pd.DataFrame(np.array([[0.8,1.0],[0.6,0.8],[0.5,0.6],[0.4,0.5],[0.3,0.4],[0.2,0.3],[0,0.2]] )\n",
    "                                ,index=rowHeader, columns=['[lower', 'upper)'])\n",
    "            \n",
    "    return bound.join(df)\n",
    "       \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "def futureTest(mlp, X,y,numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(X[:,c.dateColumn])\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            mlp.fit(X_train,y_train)\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = mlp.predict_proba(X_test)\n",
    "            train_proba =mlp.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "            if verbose == True:\n",
    "                print(\"week{}\".format(w))\n",
    "                print(\"numOftest {} , score {}\".format(X_test.shape[0],mlp.score(X_test,y_test)))\n",
    "                print(np.hstack([stack[errorIndx],proba[errorIndx],y_test[errorIndx]]))\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "        \n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    return firstNScore(1,sum_train_proba,sum_train_y), score, like2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "class FootballDataHelper:\n",
    "    def __init__ (self, recentNum=5):\n",
    "        self.win_mapping = {'H':0, 'D':1,'A':2}\n",
    "        self.recentNum = recentNum\n",
    "        self.df = None\n",
    "        self.teamsData={}\n",
    "        self.session = 0\n",
    "        #self.hiddensCount = 2\n",
    "       \n",
    "    def addColumns(self,df, addition):    \n",
    "        dates = df[\"Date\"].drop_duplicates().values\n",
    "        col_adds = []\n",
    "        for colAdd in addition.columns:\n",
    "             if colAdd not in df.columns:\n",
    "                    df[colAdd]=np.zeros(shape=(df.shape[0],))\n",
    "                    col_adds.append(colAdd)\n",
    "        for date in dates:\n",
    "            dateAddition= addition[addition['Date'] == date].sort(columns='HomeTeam')\n",
    "            dateDf  = df [df['Date']==date].sort(columns='HomeTeam')\n",
    "            for col in col_adds:\n",
    "                dateDf[col] = dateAddition[col].values\n",
    "            df.update(dateDf)\n",
    "        return df\n",
    "            \n",
    "            \n",
    "    def readFootBallData(self,year): \n",
    "        filename = \"dataSet/E{}.csv\".format(year)\n",
    "        df = pd.read_csv(filename)\n",
    "        #df = df.drop(df.columns[range(23,df.shape[1])], axis=1)\n",
    "        #df = df.drop(\"Div\",axis=1)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "        df['session'] = pd.Series(np.ones(shape=(df.shape[0],))*self.session, index=df.index)\n",
    "        self.session = self.session +1\n",
    "        \n",
    "        matchDetail = pd.read_csv(\"dataSet/match{}.csv\".format(year))\n",
    "        matchDetail['Date'] =pd.to_datetime(matchDetail['Date'])\n",
    "        df = self.addColumns(df,matchDetail)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = pd.concat([self.df,df])\n",
    "            \n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    " \n",
    "    def getTeam(self,dataFrame, teamName):       \n",
    "        return dataFrame[(dataFrame[\"HomeTeam\"] == teamName) | (dataFrame[\"AwayTeam\"] == teamName)]\n",
    " \n",
    "        \n",
    "    def previousRecords(self,team, date , recentNum):\n",
    "        prev = team[( team[\"Date\"] < date)]\n",
    "        \n",
    "        if prev.shape[0] < recentNum :\n",
    "            #print(\"less than min Num\")\n",
    "            return None\n",
    "        else:\n",
    "            return prev.iloc[-recentNum:]\n",
    "    def readPredict (self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "  \n",
    "    def inverseTeamMapping (self, col):\n",
    "        inverseMap ={}\n",
    "        for name in self.teamsMap.keys():        \n",
    "            inverseMap[self.teamsMap[name]] = name\n",
    "        res =[]\n",
    "        for idex, v in enumerate(col):\n",
    "            res.append(inverseMap[v])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def readTeamMatch(self, teamName):\n",
    "        df = pd.read_csv('teams/'+teamName+'.csv')\n",
    "        df['1'] = pd.to_datetime(df['1'],yearfirst=True)\n",
    "        #df['1']= (pd.to_numeric(df['1'])/1e9/24/60/60)\n",
    "        self.teamsData[teamName]=df.sort(['1'],ascending=[False])\n",
    "        self.teamsById[self.teamsMap[teamName]]=self.teamsData[teamName]\n",
    "    \n",
    "    def commonMapping(self, X):\n",
    "        X['HomeTeam'] = X['HomeTeam'].map(self.teamsMap)\n",
    "        X['AwayTeam'] = X['AwayTeam'].map(self.teamsMap)\n",
    "        X['Referee']=X['Referee'].map(self.refereesMap).fillna(0)\n",
    "        X['HTR'] = X['HTR'].map(self.win_mapping)\n",
    "        X['FTR'] = X['FTR'].map(self.win_mapping)\n",
    "        return X\n",
    "    def initData(self, X, target,encode):\n",
    "        X  = X.sort_values(by=\"Date\")\n",
    "        isInput = False\n",
    "        if target is None:\n",
    "            target =X      \n",
    "        else:\n",
    "            if self.ohe is None:\n",
    "                raise Exception(\"Not yet get train data\")\n",
    "            isInput = True\n",
    "            if encode == True:\n",
    "                target = self.commonMapping(target)\n",
    "        y=None\n",
    "        if encode == True:    \n",
    "            X =self.commonMapping(X)\n",
    "            y = []\n",
    "            for v in target['FTR']:\n",
    "                y.append(range(3)==v)\n",
    "        else:\n",
    "            y = target['FTR'].values\n",
    "        target_date = (pd.to_numeric(target['Date'])/1e9/24/60/60).values\n",
    "        return isInput, X,y, target, target_date\n",
    "   \n",
    "    def aggregate(self,recents,nonExpand,isInput,encode):\n",
    "        res =None\n",
    "        if encode == True:\n",
    "            if isInput==False:\n",
    "                self.ohe = OneHotEncoder(categorical_features='all')\n",
    "                self.ohe.fit(recents)\n",
    "            res = self.ohe.transform(recents).toarray()\n",
    "        else:\n",
    "            res = np.array(recents)\n",
    "        self.dateColumn = res.shape[1]\n",
    "        res = np.hstack([res,nonExpand])\n",
    "        return res\n",
    "  \n",
    "    def getH7(self,removeInsufficient=False, target=None,encode = True):\n",
    "        #Simple recent win,draw, lose \n",
    "        df = self.df\n",
    "        if removeInsufficient == True:\n",
    "            df= df[df['Sufficient'] == 1]\n",
    "        \n",
    "        \n",
    "        isInput, X, y,target, target_date = self.initData(df,target,encode)\n",
    "        resy=[]\n",
    "        resx=[]\n",
    "        print(\"start format\")\n",
    "        recents = X[['HomeTeam','AwayTeam','Referee']].values\n",
    "        haccp = X['HAccP'].values.reshape(X.shape[0],1)\n",
    "        aaccp = X['AAccP'].values.reshape(X.shape[0],1)\n",
    "        homeRecent = np.hstack([X[['HWin','HDraw','HLose']].values,\n",
    "                                (X['HScore'].values - X['HConcede'].values).reshape(X.shape[0],1)])\n",
    "        awayRecent = np.hstack([X[['AWin','ADraw','ALose']].values,\n",
    "                                (X['AScore'].values - X['AConcede'].values).reshape(X.shape[0],1)])\n",
    "        homeMoral = X['HMoral'].values.reshape(X.shape[0],1)\n",
    "        awayMoral = X['AMoral'].values.reshape(X.shape[0],1)\n",
    "        target_date = target_date.reshape(X.shape[0],1)\n",
    "        nonExpand =np.hstack([target_date,X[['HRestDay','ARestDay','HS_Acc','AS_Acc','HST_Acc','AST_Acc']].values,haccp-aaccp,(haccp+1)/(aaccp+1),\n",
    "                                homeRecent,awayRecent, homeMoral - awayMoral + haccp - aaccp])\n",
    "        res = self.aggregate(recents,nonExpand,isInput,encode)\n",
    "        print(\"finish\")\n",
    "        sys.stdout.flush()\n",
    "        return res, np.array(y)\n",
    "    def _getRank(self,x, X,teamName,recentNum):\n",
    "        team = self.getTeam(X,teamName)\n",
    "        prev = team[team['Date'] < x['Date']].values      \n",
    "        for i in range(recentNum):\n",
    "            pass\n",
    "    def initRanking(self, n = 20):\n",
    "        defaultPt = 1\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HPoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"APoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        hpoints= df[\"HPoints\"].values\n",
    "        apoints=df[\"APoints\"].values\n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hpoints[i] = 3\n",
    "                apoints[i] = 0\n",
    "            elif ftr[i] == 'D':\n",
    "                hpoints[i] = 1\n",
    "                apoints[i] = 1\n",
    "            else :\n",
    "                hpoints[i] = 0\n",
    "                apoints[i] = 3\n",
    "        df[\"HPoints\"]=hpoints\n",
    "        df[\"APoints\"]=apoints\n",
    "        for teamName in self.teamsMap.keys():\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hpoints = team['HPoints'].values\n",
    "            apoints = team['APoints'].values\n",
    "            psum = 0\n",
    "            haccp = team['HAccP'].values\n",
    "            aaccp = team['AAccP'].values\n",
    "        \n",
    "            for  i in range(0,n):\n",
    "                if i < hpoints.shape[0]:\n",
    "                    psum = psum + (hpoints[i] if hometeam[i] == teamName else apoints[i] ) \n",
    "                else:\n",
    "                    psum = psum + defaultPt        \n",
    "                    \n",
    "        \n",
    "            for j in range(team.shape[0]):\n",
    "\n",
    "                if j+n < hpoints.shape[0]:                     \n",
    "                    psum = psum + (hpoints[j+n] if hometeam[j+n]==teamName else apoints[j+n])\n",
    "                else:\n",
    "                    psum = psum + defaultPt \n",
    "                \n",
    "                psum = psum - (hpoints[j] if hometeam[j]==teamName else apoints[j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    haccp[j]=psum\n",
    "                else:\n",
    "                    aaccp[j]=psum\n",
    "            team['HAccp']=haccp\n",
    "            team['AAccP']=aaccp\n",
    "            #print(team[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "            df.update(team)\n",
    "            \n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initRecentData(self, n =5):\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HDraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ADraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HLose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ALose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "                \n",
    "        df[\"HScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HRestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ARestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        #general\n",
    "        generalList = ['HS','AS','HST','AST','H_Poss','A_Poss','H_tkPass_OK','A_atkPass_OK',\n",
    "                       'H_atkPass_tot','A_atkPass_tot','H_ins','A_ins']\n",
    "        generalOutput = []\n",
    "        for attr in generalList:\n",
    "            temp = attr+'_Acc'\n",
    "            df[temp]=pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "            generalOutput.append(temp)\n",
    "        #\n",
    "        df[\"Sufficient\"] = pd.Series(np.ones(shape=(df.shape[0],)))\n",
    "        \n",
    "      \n",
    "            \n",
    "        \n",
    "        hscore = df['FTHG'].values\n",
    "        ascore = df['FTAG'].values\n",
    "        hconcede = df ['FTAG'].values\n",
    "        aconcede = df['FTHG'].values\n",
    "               \n",
    "        hwin = df['HWin'].values\n",
    "        awin = df['AWin'].values\n",
    "        hlose = df['HLose'].values\n",
    "        alose = df['ALose'].values\n",
    "        hdraw = df['HDraw'].values\n",
    "        adraw = df['ADraw'].values\n",
    "        hmoral = df['HMoral'].values\n",
    "        amoral = df['AMoral'].values\n",
    "        \n",
    "        rankRatio = (df['HAccP'].values+1) / (df['AAccP'].values +1)\n",
    "        \n",
    "        \n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hwin[i] = 1\n",
    "                hmoral[i] = 3 * 1/rankRatio[i]\n",
    "                alose[i]= 1\n",
    "                amoral[i] = -3 * 1/rankRatio[i]\n",
    "            elif ftr[i] == 'D':\n",
    "                hdraw[i] = 1\n",
    "                hmoral[i] = 1 * 1/rankRatio[i]\n",
    "                adraw[i] = 1\n",
    "                amoral[i] = 1 * rankRatio[i]\n",
    "            else :\n",
    "                hlose[i] = 1\n",
    "                hmoral[i] = -3*rankRatio[i]\n",
    "                awin [i] = 1\n",
    "                amoral[i] = 3*rankRatio[i]\n",
    "        \n",
    "        \n",
    "        df[\"HWin\"]=hwin\n",
    "        df[\"AWin\"]=awin\n",
    "        df[\"HDraw\"]=hdraw\n",
    "        df[\"ADraw\"]=adraw\n",
    "        df[\"HLose\"]=hlose\n",
    "        df[\"ALose\"]=alose\n",
    "        df[\"HScore\"]=hscore\n",
    "        df[\"AScore\"]=ascore\n",
    "        df[\"HConcede\"]=hconcede\n",
    "        df[\"AConcede\"]=aconcede\n",
    "        df[\"HMoral\"] = hmoral\n",
    "        df[\"AMoral\"] = amoral\n",
    "        \n",
    "        \n",
    "        \n",
    "        for teamName in self.teamsMap.keys():\n",
    "            print(teamName)\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hwin = team[\"HWin\"].values\n",
    "            awin = team[\"AWin\"].values\n",
    "            hlose= team[\"HLose\"].values\n",
    "            alose = team[\"ALose\"].values\n",
    "            hdraw = team[\"HDraw\"].values\n",
    "            adraw = team[\"ADraw\"].values\n",
    "            hscore = team[\"HScore\"].values\n",
    "            ascore = team[\"AScore\"].values\n",
    "            hconcede = team[\"HConcede\"].values\n",
    "            aconcede = team[\"AConcede\"].values\n",
    "            hmoral = team[\"HMoral\"].values\n",
    "            amoral = team[\"AMoral\"].values\n",
    "            hrestday = team[\"HRestDay\"].values\n",
    "            arestday = team[\"ARestDay\"].values\n",
    "            \n",
    "            #general\n",
    "            hs= team[\"HS\"].values\n",
    "            as_ = team[\"AS\"].values\n",
    "            hst = team[\"HST\"].values\n",
    "            ast = team[\"AST\"].values\n",
    "            original_list =[]\n",
    "            output_list=[]\n",
    "            for indx, o_attr in enumerate(generalList):\n",
    "                original_list.append(team[o_attr].values)\n",
    "                output_list.append(team[generalOutput[indx]].values)\n",
    "            \n",
    "            \n",
    "            matchDate =team['Date'].values\n",
    "            sufficient = team['Sufficient'].values\n",
    "            teamMatchesDate = self.teamsData[teamName].sort('1',ascending=False)['1'].values\n",
    "            \n",
    "            restday = 0\n",
    "            winsum =0 \n",
    "            losesum=0\n",
    "            drawsum=0\n",
    "            scoresum =0\n",
    "            concedesum=0\n",
    "            moralsum = 0\n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            teamAttrSum_list=[0 for i in range(int(len(original_list)/2))]\n",
    "            for  i in range(0,n):\n",
    "                if i < team.shape[0]:\n",
    "                    scoresum = scoresum + (hscore[i] if hometeam[i] == teamName else ascore[i])\n",
    "                    winsum = winsum + (hwin[i] if hometeam[i] == teamName else awin[i])\n",
    "                    losesum= losesum + (hlose[i] if hometeam[i] == teamName else alose[i])\n",
    "                    drawsum= drawsum + (hdraw[i] if hometeam[i] == teamName else adraw[i])\n",
    "                    concedesum = concedesum+ (hconcede[i] if hometeam[i] == teamName else aconcede[i])\n",
    "                    moralsum= moralsum+ (hmoral[i] if hometeam[i] == teamName else amoral[i])\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        teamAttrSum_list[attrIndx] +=(original_list[2*attrIndx][i] if hometeam[i] == teamName else original_list[2*attrIndx+1][i])\n",
    "                else:\n",
    "                    # + 0\n",
    "                    pass\n",
    "            dateIndx = 0\n",
    "            for j in range(team.shape[0]):\n",
    "                while True:\n",
    "                    if dateIndx >= teamMatchesDate.shape[0]:\n",
    "                        sufficient[j] = False\n",
    "                        break\n",
    "                    if teamMatchesDate[dateIndx] < matchDate[j] :\n",
    "                        restday = (matchDate[j] - teamMatchesDate[dateIndx])/np.timedelta64(1,'D')\n",
    "                        break\n",
    "                    else:\n",
    "                        dateIndx = dateIndx + 1\n",
    "                \n",
    "                if j+n < team.shape[0]:                     \n",
    "                    scoresum = scoresum + (hscore[j+n] if hometeam[j+n] == teamName else ascore[j+n])\n",
    "                    winsum = winsum + (hwin[j+n] if hometeam[j+n] == teamName else awin[j+n])\n",
    "                    losesum= losesum + (hlose[j+n] if hometeam[j+n] == teamName else alose[j+n])\n",
    "                    drawsum= drawsum + (hdraw[j+n] if hometeam[j+n] == teamName else adraw[j+n])\n",
    "                    concedesum = concedesum+ (hconcede[j+n] if hometeam[j+n] == teamName else aconcede[j+n])\n",
    "                    moralsum= moralsum+ (hmoral[j+n] if hometeam[j+n] == teamName else amoral[j+n])\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        teamAttrSum_list[attrIndx] += (original_list[2*attrIndx][j+n] if hometeam[j+n] == teamName else original_list[2*attrIndx+1][j+n])\n",
    "                else:\n",
    "                    sufficient[j] = False\n",
    "                    \n",
    "                \n",
    "                scoresum = scoresum - (hscore[j] if hometeam[j] == teamName else ascore[j])\n",
    "                winsum = winsum - (hwin[j] if hometeam[j] == teamName else awin[j])\n",
    "                losesum= losesum - (hlose[j] if hometeam[j] == teamName else alose[j])\n",
    "                drawsum= drawsum - (hdraw[j] if hometeam[j] == teamName else adraw[j])\n",
    "                concedesum = concedesum - (hconcede[j] if hometeam[j] == teamName else aconcede[j])\n",
    "                moralsum= moralsum - (hmoral[j] if hometeam[j] == teamName else amoral[j])\n",
    "                for attrIndx in range(len(teamAttrSum_list)):\n",
    "                    teamAttrSum_list[attrIndx] -=  (original_list[2*attrIndx][j] if hometeam[j] == teamName else original_list[2*attrIndx+1][j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    hscore[j] = scoresum\n",
    "                    hwin[j] = winsum\n",
    "                    hlose[j] = losesum\n",
    "                    hdraw[j] = drawsum\n",
    "                    hconcede[j] = concedesum\n",
    "                    hmoral[j] = moralsum\n",
    "                    hrestday[j] = restday\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        output_list[2*attrIndx][j] = teamAttrSum_list[attrIndx]\n",
    "                else:\n",
    "                    ascore[j] = scoresum\n",
    "                    awin[j] = winsum\n",
    "                    alose[j] = losesum\n",
    "                    adraw[j] = drawsum\n",
    "                    aconcede[j] = concedesum\n",
    "                    amoral[j] = moralsum\n",
    "                    arestday[j] = restday\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        output_list[2*attrIndx+1][j] = teamAttrSum_list[attrIndx]\n",
    "            team[\"HWin\"]=hwin\n",
    "            team[\"AWin\"]=awin\n",
    "            team[\"HDraw\"]=hdraw\n",
    "            team[\"ADraw\"]=adraw\n",
    "            team[\"HLose\"]=hlose\n",
    "            team[\"ALose\"]=alose\n",
    "            team[\"HScore\"]=hscore\n",
    "            team[\"AScore\"]=ascore\n",
    "            team[\"HConcede\"]=hconcede\n",
    "            team[\"AConcede\"]=aconcede\n",
    "            team[\"HMoral\"] = hmoral\n",
    "            team[\"AMoral\"] = amoral\n",
    "            team['Sufficient'] = sufficient\n",
    "            for indx in range(len(output_list)):\n",
    "                team[generalList[indx]]= output_list[indx]\n",
    "        \n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            df.update(team)\n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initTeamData(self):\n",
    "        self.teamsData={}\n",
    "        self.teamsById={}\n",
    "        for name in self.teamsMap.keys():\n",
    "            self.readTeamMatch(name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:23: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:24: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:98: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "c = FootballDataHelper()\n",
    "#c.readFootBallData(\"E0_1112.csv\")\n",
    "c.readFootBallData(2012)\n",
    "c.readFootBallData(2013)\n",
    "c.readFootBallData(2014)\n",
    "c.readFootBallData(2015)\n",
    "c.initTeamData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#c.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1442"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:177: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:228: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:229: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df = c.initRanking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1442Leicester\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:237: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'A_atkPass_OK'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-ef239affba4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitRecentData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-54aa123d0e6e>\u001b[0m in \u001b[0;36minitRecentData\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[0moutput_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo_attr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneralList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m                 \u001b[0moriginal_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo_attr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m                 \u001b[0moutput_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgeneralOutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1967\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1968\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1969\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1971\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1974\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1975\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1976\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1978\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/anaconda3/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3210\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3211\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3212\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3213\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/anaconda3/lib/python3.5/site-packages/pandas/core/index.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1757\u001b[0m                                  'backfill or nearest lookups')\n\u001b[0;32m   1758\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1759\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m         indexer = self.get_indexer([key], method=method,\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3979)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3843)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12265)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12216)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'A_atkPass_OK'"
     ]
    }
   ],
   "source": [
    "df=c.initRecentData(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1442     49\n",
       "1441    111\n",
       "1440     72\n",
       "1439     68\n",
       "1436    100\n",
       "1434     98\n",
       "1435    126\n",
       "1433     58\n",
       "1437     78\n",
       "1438     72\n",
       "1432     83\n",
       "1431    107\n",
       "1430     67\n",
       "1429    202\n",
       "1428     84\n",
       "1427     97\n",
       "1426    143\n",
       "1422     51\n",
       "1418     93\n",
       "1419    131\n",
       "1420     51\n",
       "1421     77\n",
       "1423     93\n",
       "1424     74\n",
       "1425     85\n",
       "1417     98\n",
       "1416     65\n",
       "1415     50\n",
       "1414    101\n",
       "1413     66\n",
       "       ... \n",
       "36      112\n",
       "28      123\n",
       "27       77\n",
       "26       85\n",
       "25       64\n",
       "24       98\n",
       "23      104\n",
       "22       72\n",
       "21       51\n",
       "20       69\n",
       "18       86\n",
       "19      117\n",
       "17       58\n",
       "16       82\n",
       "15       78\n",
       "14       47\n",
       "13       40\n",
       "12       44\n",
       "11      143\n",
       "10       55\n",
       "9       172\n",
       "8        44\n",
       "7        83\n",
       "2        72\n",
       "4        61\n",
       "3        43\n",
       "1        70\n",
       "5       128\n",
       "6        62\n",
       "0        35\n",
       "Name: A_atkPass_Ok, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.df['A_atkPass_Ok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "def plotErrorDate(X_test, X_err, dateCol = 10):\n",
    "    X_test_date =np.sort(X_test[:,c.dateColumn])\n",
    "    X_date=[]\n",
    "    y_date=[]\n",
    "    for v in X_test_date:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_date) ==0  or X_date[-1] != date:\n",
    "            X_date.append(date)\n",
    "            y_date.append(1)\n",
    "        else:\n",
    "            y_date[-1] = y_date[-1] +1\n",
    "    plt.plot_date(X_date,y_date,xdate=True)\n",
    "    X_err_d = np.sort(X_err[:,c.dateColumn])\n",
    "    X_err_date=[]\n",
    "    y_err_date = []\n",
    "    for v in X_err_d:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_err_date) ==0  or X_err_date[-1] != date:\n",
    "            X_err_date.append(date)\n",
    "            y_err_date.append(1)\n",
    "        else:\n",
    "            y_err_date[-1] = y_err_date[-1] +1\n",
    "    plt.plot_date(X_err_date,y_err_date,xdate=True,color='red')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "g_hiddenNodes = int(45)\n",
    "g_alpha = 0 \n",
    "clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_hiddenNodes], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "              learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=1000,early_stopping = False,verbose = 3)\n",
    "mlp = Pipeline([ ('scl', StandardScaler()),('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"start learning\")\n",
    "sys.stdout.flush()\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=mlp, \n",
    "                       X=X, \n",
    "                      y=y, \n",
    "                      train_sizes=np.linspace(0.1, 1.0, 4), \n",
    "                      cv=4,\n",
    "                     n_jobs=1,verbose=3)\n",
    "print(\"finishing\")   \n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "print(test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learningRes = np.vstack([train_sizes,train_mean,train_std,test_mean,test_std]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learningDf = pd.DataFrame(learningRes,columns=['size','train_mean','train_std','test_mean','test_std'])\n",
    "print(learningDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotCurve(train_mean,train_std,test_mean,test_std,train_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate(mlp, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    firstNScores = []\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "\n",
    "        mlp.fit(X[train], y[train])\n",
    "        score = mlp.score(X[test], y[test])\n",
    "        firstNScores.append(firstNScore(2, mlp.predict_proba(X[test]), y[test]))\n",
    "        train_scores.append(mlp.score(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, firstNScores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def lamda_test(mlp, X, y, lamdas):\n",
    "    \n",
    "    train_scores=[]\n",
    "    test_scores=[]\n",
    "    for lamda in lamdas:\n",
    "        clf.set_params(alpha= lamda)\n",
    "        train_s, test_s, firstNScores = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_s, test_s, firstNScores =futureTest(mlp,X,y,numOfWeek=20) \n",
    "        train_scores.append(train_s)\n",
    "        test_scores.append(test_s)\n",
    "        print(\"lamda: {}, train: {}, test: {}\".format(lamda, \n",
    "                    np.mean(train_s), np.mean(test_s)) )\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plotCurve(train_mean,train_std,test_mean,test_std,lamdas)\n",
    "    return np.array(train_scores),np.array(test_scores)\n",
    "\n",
    "l_range = []\n",
    "for i in range(0,50):\n",
    "    l_range.append(2*i)\n",
    "train_scores,test_scores = lamda_test(mlp,X,y,np.array(l_range))\n",
    "#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "alphaRes = np.vstack([l_range,train_mean,train_std,test_mean,test_std]).T\n",
    "alphaDf = pd.DataFrame(alphaRes,columns=['alpha','train_mean','train_std','test_mean','test_std'])\n",
    "print(alphaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testNodeSize(start ,end):\n",
    "    node_range = range(start,end,5)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    for node in node_range:   \n",
    "        print(\"start node:{}\".format(node))\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[node], activation='logistic', algorithm='l-bfgs', alpha=0, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores,first2 = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_scores,test_scores , first2= futureTest(mlp, X,y,numOfWeek = 10)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        print(\"Node {}: train_mean {}  v.s. test_mean {}\".format(node,np.mean(train_scores),np.mean(test_scores)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(node_range))\n",
    "    return node_range, train_means,train_std,test_means,test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node_range, train_means,train_std,test_means,test_std=testNodeSize(1,X.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodeRes = np.vstack([node_range,train_means,train_std,test_means,test_std]).T\n",
    "nodeDf = pd.DataFrame(nodeRes,columns=['nodeNum','train_mean','train_std','test_mean','test_std'])\n",
    "print(nodeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.set_params(alpha=g_alpha)\n",
    "print(clf)\n",
    "train_score, test_score, first2 = futureTest(mlp,X,y,numOfWeek = 30, verbose=True)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testRecentNum(start, end):\n",
    "    recent_range = range(start,end)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    first2_mean=[]\n",
    "    for recent in recent_range:\n",
    "        print(\"start recent:{}\".format(recent))\n",
    "        X,y = c.getH7(recent)\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_alpha], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores, first2 = crossValidate(mlp,X,y,fold=10)\n",
    "        #train_scores,test_scores, first2 = futureTest(mlp, X,y,numOfWeek = 15)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        first2_mean.append(np.mean(first2))\n",
    "        print(\"recent {}: train_mean {}  v.s. test_mean {} , first2_mean {}\".format(\n",
    "                recent,np.mean(train_scores),np.mean(test_scores),np.mean(first2)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(recent_range))\n",
    "    return train_means,train_std,test_means,test_std,first2_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_means,train_std,test_means,test_std,first2_mean=testRecentNum(1 ,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "proba = mlp.predict_proba(X_test)\n",
    "precisionMatrix(proba,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#future\n",
    "mlp.fit(X,y)\n",
    "inData = c.readPredict(\"future.csv\")\n",
    "print(inData)\n",
    "X_in, y_in = c.getH6(5,target=inData)\n",
    "res = mlp.predict(X_in)\n",
    "proba= mlp.predict_proba(X_in)\n",
    "print(mlp.score(X_in,y_in))\n",
    "print (np.hstack([proba,y_in]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "return\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient=True, encode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1358, 21)\n",
      "                home         away        Referee   time HRestTime ARestTime  \\\n",
      "0             Fulham     Man City       M Halsey  15612         7         3   \n",
      "1            Norwich    Liverpool        M Jones  15612         2         2   \n",
      "2            Everton  Southampton      L Probert  15612         3         3   \n",
      "3              Stoke      Swansea         J Moss  15612         7         3   \n",
      "4            Arsenal      Chelsea     M Atkinson  15612         2         3   \n",
      "5         Man United    Tottenham          C Foy  15612         2         2   \n",
      "6        Aston Villa    West Brom       A Taylor  15613         4         3   \n",
      "7                QPR     West Ham  M Clattenburg  15614         4         5   \n",
      "8              Wigan      Everton       K Friend  15619         7         7   \n",
      "9          West Brom          QPR        M Jones  15619         6         4   \n",
      "10          Man City   Sunderland      L Probert  15619         2         7   \n",
      "11           Chelsea      Norwich       A Taylor  15619         3         7   \n",
      "12          West Ham      Arsenal         P Dowd  15619         4         2   \n",
      "13           Swansea      Reading         M Dean  15619         7         7   \n",
      "14       Southampton       Fulham  M Clattenburg  15620         8         8   \n",
      "15         Tottenham  Aston Villa    N Swarbrick  15620         2         7   \n",
      "16         Liverpool        Stoke        L Mason  15620         2         8   \n",
      "17         Newcastle   Man United         H Webb  15620         2         4   \n",
      "18           Norwich      Arsenal      L Probert  15633        14        13   \n",
      "19          West Ham  Southampton    N Swarbrick  15633        13        13   \n",
      "20         West Brom     Man City  M Clattenburg  15633        14        14   \n",
      "21           Swansea        Wigan        M Jones  15633        14        14   \n",
      "22        Man United        Stoke       A Taylor  15633        13        13   \n",
      "23            Fulham  Aston Villa          C Foy  15633        13        13   \n",
      "24         Liverpool      Reading         R East  15633        13        14   \n",
      "25         Tottenham      Chelsea         M Dean  15633        13        14   \n",
      "26               QPR      Everton         J Moss  15634        15        15   \n",
      "27        Sunderland    Newcastle     M Atkinson  15634        15        14   \n",
      "28          Man City      Swansea     M Atkinson  15640         2         7   \n",
      "29           Arsenal          QPR       A Taylor  15640         2         6   \n",
      "...              ...          ...            ...    ...       ...       ...   \n",
      "1328        West Ham    Tottenham     A Marriner  16862         4         3   \n",
      "1329         Arsenal      Swansea       R Madley  16862         3         3   \n",
      "1330       Liverpool     Man City     M Atkinson  16862         2         2   \n",
      "1331      Man United      Watford        M Jones  16862         3         4   \n",
      "1332           Stoke    Newcastle    N Swarbrick  16862         4        17   \n",
      "1333         Watford    Leicester         J Moss  16865         2         3   \n",
      "1334        Man City  Aston Villa        L Mason  16865         2         3   \n",
      "1335       Tottenham      Arsenal       M Oliver  16865         2         2   \n",
      "1336       Newcastle  Bournemouth      P Tierney  16865         2         3   \n",
      "1337     Southampton   Sunderland    N Swarbrick  16865         3         3   \n",
      "1338         Swansea      Norwich       C Pawson  16865         2         3   \n",
      "1339         Chelsea        Stoke  M Clattenburg  16865         3         2   \n",
      "1340         Everton     West Ham       A Taylor  16865         3         2   \n",
      "1341  Crystal Palace    Liverpool     A Marriner  16866         4         3   \n",
      "1342       West Brom   Man United         M Dean  16866         4         3   \n",
      "1343     Bournemouth      Swansea         R East  16872         7         7   \n",
      "1344         Norwich     Man City         J Moss  16872         7         7   \n",
      "1345           Stoke  Southampton        L Mason  16872         7         7   \n",
      "1346     Aston Villa    Tottenham       A Taylor  16873         8         2   \n",
      "1347       Leicester    Newcastle       C Pawson  16874         8         9   \n",
      "1348         Swansea  Aston Villa         M Dean  16879         7         5   \n",
      "1349  Crystal Palace    Leicester        M Jones  16879         7         4   \n",
      "1350       West Brom      Norwich       A Taylor  16879        12         7   \n",
      "1351         Chelsea     West Ham       R Madley  16879         6         5   \n",
      "1352         Watford        Stoke       C Pawson  16879         6         7   \n",
      "1353         Everton      Arsenal  M Clattenburg  16879         6         2   \n",
      "1354       Newcastle   Sunderland     M Atkinson  16880         5        15   \n",
      "1355     Southampton    Liverpool         R East  16880         8         2   \n",
      "1356        Man City   Man United       M Oliver  16880         4         2   \n",
      "1357       Tottenham  Bournemouth    N Swarbrick  16880         2         8   \n",
      "\n",
      "     HS_Acc AS_Acc HST_Acc AST_Acc ... HWin HDraw HLose H goal Diff AWin  \\\n",
      "0        28     79      24      53 ...    3     0     2           5    2   \n",
      "1        37      0      20       0 ...    0     3     2          -6    0   \n",
      "2        35      0      18       0 ...    3     1     1           4    1   \n",
      "3         0     64       0      41 ...    0     4     1          -1    2   \n",
      "4        47     30      25      18 ...    2     3     0           7    4   \n",
      "5        26     67      13      40 ...    4     0     1           6    2   \n",
      "6        18     58       8      35 ...    1     1     3          -4    3   \n",
      "7         0      0       0       0 ...    0     2     3          -8    2   \n",
      "8        55     36      29      20 ...    1     1     3          -4    3   \n",
      "9        57      0      32       0 ...    2     2     1           0    0   \n",
      "10       80     13      50       8 ...    2     3     0           3    1   \n",
      "11       30     43      18      25 ...    4     1     0           6    0   \n",
      "12       17     61      10      30 ...    2     2     1           1    2   \n",
      "13       60     16      33       8 ...    1     1     3          -4    0   \n",
      "14        0     28       0      24 ...    1     0     4          -7    2   \n",
      "15       65      8      40       2 ...    3     2     0           4    1   \n",
      "16        0      0       0       0 ...    1     2     2           0    1   \n",
      "17       62     26      29      13 ...    1     3     1          -1    4   \n",
      "18       37     65      25      38 ...    0     2     3          -7    3   \n",
      "19       17      0      10       0 ...    2     2     1           2    1   \n",
      "20       60     95      35      61 ...    3     1     1           1    3   \n",
      "21       75     43      42      21 ...    0     2     3          -7    0   \n",
      "22       26      0      13       0 ...    4     0     1           8    1   \n",
      "23       28      8      18       2 ...    2     1     2           0    1   \n",
      "24       16      7       5       5 ...    1     2     2           0    0   \n",
      "25       88     30      50      18 ...    4     1     0           6    4   \n",
      "26        0     33       0      20 ...    0     1     4          -5    2   \n",
      "27       10     64       6      27 ...    1     3     1          -2    1   \n",
      "28       76     79      49      43 ...    3     2     0           5    1   \n",
      "29       54      0      31       0 ...    2     1     2           5    0   \n",
      "...     ...    ...     ...     ... ...  ...   ...   ...         ...  ...   \n",
      "1328     18    107       2      43 ...    2     2     1           2    5   \n",
      "1329     43     39      17      10 ...    2     1     2           1    1   \n",
      "1330      0     39       0       9 ...    2     1     2           4    2   \n",
      "1331     40     36      13       7 ...    2     1     2           2    2   \n",
      "1332      0     51       0      19 ...    2     0     3          -6    2   \n",
      "1333     49     69      10      24 ...    1     2     2          -1    3   \n",
      "1334     26     17       7       4 ...    1     1     3          -5    1   \n",
      "1335     92     34      34      16 ...    4     0     1           5    2   \n",
      "1336     33     45      12      14 ...    1     0     4          -8    2   \n",
      "1337      0     58       0      17 ...    2     1     2          -1    1   \n",
      "1338     43     38       7       9 ...    1     2     2          -1    0   \n",
      "1339     16      0       4       0 ...    3     2     0           6    3   \n",
      "1340     24     18      13       2 ...    3     0     2           6    3   \n",
      "1341      0      0       0       0 ...    0     2     3          -3    3   \n",
      "1342     40     32      13      12 ...    2     2     1           1    3   \n",
      "1343     37     53      10      10 ...    2     1     2           0    2   \n",
      "1344     38     32       9      13 ...    0     1     4          -5    2   \n",
      "1345      0      0       0       0 ...    3     1     1           1    2   \n",
      "1346     11    101       2      39 ...    1     0     4         -11    3   \n",
      "1347     70     36      25      13 ...    3     1     1           3    1   \n",
      "1348     46     11      11       2 ...    2     0     3          -1    0   \n",
      "1349     14     66       4      19 ...    0     2     3          -3    3   \n",
      "1350     32     26       8       5 ...    3     1     1           2    0   \n",
      "1351     36     18      11       2 ...    3     2     0           6    3   \n",
      "1352     37      0       7       0 ...    1     1     3          -2    3   \n",
      "1353     39     13      21       5 ...    3     0     2           6    2   \n",
      "1354     29     69      10      20 ...    1     0     4          -7    1   \n",
      "1355     11      0       4       0 ...    2     1     2          -1    3   \n",
      "1356     41     17      14       7 ...    1     1     3          -2    2   \n",
      "1357     94     20      38       6 ...    3     1     1           3    3   \n",
      "\n",
      "     ADraw ALose A goal diff moraldiff + h-a  y  \n",
      "0        3     0           3         -5.4577  A  \n",
      "1        2     3          -6         4.26691  A  \n",
      "2        0     4          -6         20.4973  H  \n",
      "3        1     2           3        -0.98787  H  \n",
      "4        1     0           7         -7.3312  A  \n",
      "5        2     1           2         7.24231  A  \n",
      "6        1     1           3        -16.1433  D  \n",
      "7        2     1           1        -16.4869  A  \n",
      "8        1     1           5        -18.8485  D  \n",
      "9        2     3          -4         17.7318  H  \n",
      "10       4     0           1         5.22302  H  \n",
      "11       3     2          -4         26.8163  H  \n",
      "12       2     1           6         1.33957  A  \n",
      "13       2     3          -5         3.22343  D  \n",
      "14       0     3          -1        -9.28572  D  \n",
      "15       2     2          -3         17.1502  H  \n",
      "16       3     1           1        -5.98228  D  \n",
      "17       0     1           6         -7.8114  A  \n",
      "18       1     1           8        -21.3046  H  \n",
      "19       1     3          -5         12.8002  H  \n",
      "20       2     0           6        -6.20904  A  \n",
      "21       2     3          -6        0.178857  H  \n",
      "22       3     1           1         9.43603  H  \n",
      "23       2     2          -3         5.66887  H  \n",
      "24       2     3          -5         7.88105  H  \n",
      "25       1     0           7        -2.72658  A  \n",
      "26       2     1           3        -23.5168  D  \n",
      "27       3     1          -2       -0.225469  D  \n",
      "28       1     3          -6         24.2804  H  \n",
      "29       2     3          -3         12.0548  H  \n",
      "...    ...   ...         ...             ... ..  \n",
      "1328     0     0           8        -19.9385  H  \n",
      "1329     2     2          -1         13.1779  A  \n",
      "1330     1     2           2        -2.28404  H  \n",
      "1331     2     1           1        -4.32401  H  \n",
      "1332     0     3          -6         3.14025  H  \n",
      "1333     1     1           4        -19.0615  A  \n",
      "1334     0     4          -9         13.3512  H  \n",
      "1335     1     2           1         13.5843  D  \n",
      "1336     1     2          -1        -12.8138  A  \n",
      "1337     2     2          -1         1.78991  D  \n",
      "1338     1     4          -7         19.0105  H  \n",
      "1339     0     2          -2         9.61243  D  \n",
      "1340     1     1           3        -10.8407  A  \n",
      "1341     1     1           8        -24.5246  A  \n",
      "1342     1     1           4        -4.45653  H  \n",
      "1343     1     2           0         1.36626  H  \n",
      "1344     0     3          -1        -21.0893  D  \n",
      "1345     1     2          -1         8.06201  A  \n",
      "1346     1     1           2        -30.6047  A  \n",
      "1347     0     4          -9         31.6266  H  \n",
      "1348     0     5         -15         16.7183  H  \n",
      "1349     1     1           2        -33.3654  A  \n",
      "1350     2     3          -3         20.5266  A  \n",
      "1351     1     1           2         1.42972  D  \n",
      "1352     1     1           3        -10.9102  A  \n",
      "1353     1     2           1        -2.52855  A  \n",
      "1354     3     1           0        -15.3967  D  \n",
      "1355     1     1           8           -9.09  H  \n",
      "1356     1     2           0        -4.08739  A  \n",
      "1357     1     1           3         3.93801  H  \n",
      "\n",
      "[1358 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "df = pd.DataFrame(np.hstack([X,y.reshape(y.shape[0],1)]))\n",
    "\n",
    "df.columns = ['home','away','Referee','time','HRestTime','ARestTime','HS_Acc','AS_Acc','HST_Acc','AST_Acc'\n",
    "              ,'HAccP - AAccP','H/A','HWin','HDraw','HLose','H goal Diff',\n",
    "'AWin','ADraw','ALose','A goal diff','moraldiff + h-a',\n",
    "              'y']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('dataSet/V8.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "def splitData(X,y):\n",
    "    X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_test, X_val, y_test,y_val = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "def createModel(hidSize, inputDim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidSize[0], input_dim=inputDim, init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(hidSize[1], init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(3, init='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd = SGD(lr=1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "    return model\n",
    "earlyCallback = EarlyStopping(patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "x_range = range(len(history.history['val_loss']))\n",
    "plt.plot(x_range,history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate2(node_sizes, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    proba_test = []\n",
    "    proba_y=[]\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "        earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "        model = createModel(node_sizes,X.shape[1])\n",
    "        history = model.fit(X[train],y[train],verbose=0,nb_epoch=500, validation_split=0.1,show_accuracy=True, callbacks=[earlyCallback])\n",
    "      #  firstNScores.append(firstNScore(2, model.predict_proba(X[test]), y[test]))\n",
    "        score = model.evaluate(X[test],y[test])\n",
    "        proba_test.append(model.predict_proba(X[test]))\n",
    "        proba_y.append(y[test])\n",
    "        train_scores.append(model.evaluate(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, val_loss: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, proba_test,proba_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNodeNum(X,y, sizes):\n",
    "    train_loss=[] \n",
    "    score_loss=[]\n",
    "    for s in sizes:\n",
    "        train_scores,scores,  proba_test,proba_y= crossValidate2([s,s],X,y,fold=5)\n",
    "        print(\"size:{} , val_loss_mean:{}\".format(s,np.mean(scores)))\n",
    "        train_loss.append(train_scores)\n",
    "        score_loss.append(scores)\n",
    "    return train_loss,score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00129: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 1.005\n",
      "Epoch 00170: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.997\n",
      "Epoch 00238: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.993\n",
      "Epoch 00118: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.024\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.061\n",
      "size:5 , val_loss_mean:1.0156987103630872\n",
      "Epoch 00090: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.996\n",
      "Epoch 00161: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.995\n",
      "Epoch 00103: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.979\n",
      "Epoch 00071: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.006\n",
      "Epoch 00054: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.040\n",
      "size:10 , val_loss_mean:1.003282395733176\n",
      "Epoch 00071: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.987\n",
      "Epoch 00152: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00154: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.977\n",
      "Epoch 00058: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.006\n",
      "Epoch 00039: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.030\n",
      "size:15 , val_loss_mean:0.9924951881411601\n",
      "Epoch 00067: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.987\n",
      "Epoch 00101: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.957\n",
      "Epoch 00111: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.983\n",
      "Epoch 00070: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00038: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.025\n",
      "size:20 , val_loss_mean:0.9894759895838184\n",
      "Epoch 00077: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.980\n",
      "Epoch 00091: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.960\n",
      "Epoch 00089: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.982\n",
      "Epoch 00053: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00040: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.029\n",
      "size:25 , val_loss_mean:0.9895436956506909\n",
      "Epoch 00103: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00072: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.956\n",
      "Epoch 00050: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.983\n",
      "Epoch 00044: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00042: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.035\n",
      "size:30 , val_loss_mean:0.992499697529162\n",
      "Epoch 00055: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00097: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.960\n",
      "Epoch 00049: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.981\n",
      "Epoch 00054: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00037: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.031\n",
      "size:35 , val_loss_mean:0.9898164930960178\n",
      "Epoch 00077: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.987\n",
      "Epoch 00073: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.960\n",
      "Epoch 00086: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 1.001\n",
      "Epoch 00047: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00031: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.033\n",
      "size:40 , val_loss_mean:0.9950401757515319\n",
      "Epoch 00080: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.993\n",
      "Epoch 00108: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.968\n",
      "Epoch 00061: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.977\n",
      "Epoch 00042: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00039: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.045\n",
      "size:45 , val_loss_mean:0.995724935230925\n",
      "Epoch 00086: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.992\n",
      "Epoch 00097: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.958\n",
      "Epoch 00055: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.982\n",
      "Epoch 00045: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.998\n",
      "Epoch 00036: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.038\n",
      "size:50 , val_loss_mean:0.993680298704468\n",
      "Epoch 00073: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.982\n",
      "Epoch 00084: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.957\n",
      "Epoch 00065: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.987\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00032: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.036\n",
      "size:55 , val_loss_mean:0.9916822062749369\n",
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.991\n",
      "Epoch 00094: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.956\n",
      "Epoch 00065: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.987\n",
      "Epoch 00054: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.027\n",
      "size:60 , val_loss_mean:0.9913218268509769\n",
      "Epoch 00088: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.998\n",
      "Epoch 00071: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.953\n",
      "Epoch 00066: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.990\n",
      "Epoch 00068: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.992\n",
      "Epoch 00031: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:65 , val_loss_mean:0.9940406148477337\n",
      "Epoch 00055: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.985\n",
      "Epoch 00067: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.960\n",
      "Epoch 00050: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.985\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00031: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.033\n",
      "size:70 , val_loss_mean:0.991373030329925\n",
      "Epoch 00082: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 1.017\n",
      "Epoch 00080: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.956\n",
      "Epoch 00047: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.986\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.993\n",
      "Epoch 00027: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.031\n",
      "size:75 , val_loss_mean:0.9964411361889554\n",
      "Epoch 00095: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 1.009\n",
      "Epoch 00056: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.955\n",
      "Epoch 00061: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.991\n",
      "Epoch 00036: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.998\n",
      "Epoch 00033: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.033\n",
      "size:80 , val_loss_mean:0.9972124396784107\n",
      "Epoch 00067: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 1.001\n",
      "Epoch 00067: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00039: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.982\n",
      "Epoch 00032: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00033: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.027\n",
      "size:85 , val_loss_mean:0.9934865331161339\n",
      "Epoch 00045: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.991\n",
      "Epoch 00066: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.958\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.981\n",
      "Epoch 00063: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.026\n",
      "size:90 , val_loss_mean:0.9903179394928964\n",
      "Epoch 00072: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.998\n",
      "Epoch 00072: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.957\n",
      "Epoch 00043: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.988\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.000\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.038\n",
      "size:95 , val_loss_mean:0.9960906234593649\n"
     ]
    }
   ],
   "source": [
    "sizes= range(5,X.shape[1],5)\n",
    "train_loss,score_loss= testNodeNum(X_scaled,y,sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(5, 96, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD7CAYAAACMlyg3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRAyTEBBCiBj2/wE6faVRp/vliwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6327ddfcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 20\n"
     ]
    }
   ],
   "source": [
    "print(sizes)\n",
    "loss = (np.mean(score_loss,axis=1))\n",
    "loss_std = (np.std(score_loss,axis=1))\n",
    "plt.plot(sizes,loss)\n",
    "plt.fill_between(sizes,loss-loss_std,loss+loss_std,alpha=0.3)\n",
    "plt.show()\n",
    "print(\"max: {}\".format(sizes[np.argmin(loss,axis=0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def futureTest2(node_sizes, X,y,ori_dates, numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(ori_dates)\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    model = createModel(node_sizes,X.shape[1])\n",
    "    results = None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            print(\"week{}\".format(w))\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "            history = model.fit(X_train,y_train,verbose=0,nb_epoch=500, validation_split=0.1, callbacks=[earlyCallback])\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = model.predict_proba(X_test)\n",
    "            train_proba =model.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            tresult = np.hstack([np.array([w for i in range(proba.shape[0])]).reshape(proba.shape[0],1),\n",
    "                                 ori_dates[start:last].reshape(proba.shape[0],1),stack,proba,y_test])\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "                results =    tresult\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "                results =  np.vstack([results, tresult])\n",
    "            if verbose == True:\n",
    "                print(\"numOftest {} , loss {}\".format(X_test.shape[0],model.evaluate(X_test,y_test)))               \n",
    "                print (tresult)\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "    start = X.shape[0] +index+1\n",
    "    print(\"start compute precision_mat\")\n",
    "    print(X[0:start,:].shape)\n",
    "    print(y[0:start,:].shape)\n",
    "    _,_, proba_test,proba_y = crossValidate2([85,85],X[0:start,:],y[0:start,:],fold=10)\n",
    "    p_matrix = precisionMatrix(np.vstack(proba_test),np.vstack(proba_y))\n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    resultdf= pd.DataFrame(results, columns=['week','DayStamp','HomeTeam','AwayTeam','H_prob','D_prob','A_prob','H','D','A'])\n",
    "    return sum_proba, sum_y,resultdf,p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week0\n",
      "Epoch 00090: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1348/1348 [==============================] - 0s     \n",
      "week1\n",
      "Epoch 00021: early stopping\n",
      "5/5 [==============================] - 0s\n",
      "1343/1343 [==============================] - 0s     \n",
      "week2\n",
      "Epoch 00022: early stopping\n",
      "15/15 [==============================] - 0s\n",
      "1328/1328 [==============================] - 0s     \n",
      "week3\n",
      "Epoch 00028: early stopping\n",
      "13/13 [==============================] - 0s\n",
      "1315/1315 [==============================] - 0s     \n",
      "week4\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1305/1305 [==============================] - 0s     \n",
      "week5\n",
      "Epoch 00026: early stopping\n",
      "12/12 [==============================] - 0s\n",
      "1293/1293 [==============================] - 0s     \n",
      "week6\n",
      "Epoch 00024: early stopping\n",
      "8/8 [==============================] - 0s\n",
      "1285/1285 [==============================] - 0s     \n",
      "week7\n",
      "Epoch 00030: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1275/1275 [==============================] - 0s     \n",
      "week8\n",
      "Epoch 00022: early stopping\n",
      "17/17 [==============================] - 0s\n",
      "1258/1258 [==============================] - 0s     \n",
      "week9\n",
      "Epoch 00023: early stopping\n",
      "3/3 [==============================] - 0s\n",
      "1255/1255 [==============================] - 0s     \n",
      "week10\n",
      "Epoch 00022: early stopping\n",
      "11/11 [==============================] - 0s\n",
      "1244/1244 [==============================] - 0s     \n",
      "week11\n",
      "Epoch 00025: early stopping\n",
      "19/19 [==============================] - 0s\n",
      "1225/1225 [==============================] - 0s     \n",
      "week12\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1215/1215 [==============================] - 0s     \n",
      "week13\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1205/1205 [==============================] - 0s     \n",
      "week14\n",
      "Epoch 00024: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1195/1195 [==============================] - 0s     \n",
      "week15\n",
      "Epoch 00024: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1185/1185 [==============================] - 0s     \n",
      "week16\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1175/1175 [==============================] - 0s     \n",
      "week17\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1165/1165 [==============================] - 0s     \n",
      "week18\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1155/1155 [==============================] - 0s     \n",
      "week19\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1145/1145 [==============================] - 0s     \n",
      "week20\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1135/1135 [==============================] - 0s     \n",
      "week21\n",
      "Epoch 00025: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1125/1125 [==============================] - 0s     \n",
      "week22\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1115/1115 [==============================] - 0s     \n",
      "week23\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1105/1105 [==============================] - 0s     \n",
      "week24\n",
      "Epoch 00024: early stopping\n",
      "8/8 [==============================] - 0s\n",
      "1097/1097 [==============================] - 0s     \n",
      "start compute precision_mat\n",
      "(1096, 96)\n",
      "(1096, 3)\n",
      "Epoch 00033: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [440 237 308], val_loss: 1.007\n",
      "Epoch 00030: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [440 237 308], val_loss: 0.982\n",
      "Epoch 00030: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [440 237 308], val_loss: 0.934\n",
      "Epoch 00030: early stopping\n",
      "110/110 [==============================] - 0s\n",
      "110/110 [==============================] - 0s\n",
      "986/986 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [440 237 309], val_loss: 1.004\n",
      "Epoch 00037: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [440 238 309], val_loss: 0.939\n",
      "Epoch 00029: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 6, Class dist.: [440 238 309], val_loss: 0.982\n",
      "Epoch 00034: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 7, Class dist.: [440 238 309], val_loss: 0.968\n",
      "Epoch 00038: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 8, Class dist.: [440 238 309], val_loss: 0.984\n",
      "Epoch 00035: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 9, Class dist.: [440 238 309], val_loss: 0.981\n",
      "Epoch 00052: early stopping\n",
      "108/108 [==============================] - 0s\n",
      "108/108 [==============================] - 0s\n",
      "988/988 [==============================] - 0s     \n",
      "Fold: 10, Class dist.: [441 238 309], val_loss: 1.055\n",
      "summary\n",
      "score:\n",
      "0.475095785441\n",
      "2like\n",
      "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
      ">80       0.8     1.0         14        7   0.666667          0        2   \n",
      "60-80     0.6     0.8         22       18   0.550000          1        2   \n",
      "50-60     0.5     0.6         13       19   0.406250          4        2   \n",
      "40-50     0.4     0.5         21       24   0.466667          6        8   \n",
      "30-40     0.3     0.4         24       25   0.489796         13       39   \n",
      "20-30     0.2     0.3          6       19   0.240000         18       55   \n",
      "<20       0.0     0.2          9       40   0.183673         26       85   \n",
      "\n",
      "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
      ">80     0.000000         13        7   0.650000  \n",
      "60-80   0.333333         10       13   0.434783  \n",
      "50-60   0.666667          6        5   0.545455  \n",
      "40-50   0.428571          3       12   0.200000  \n",
      "30-40   0.250000         16       34   0.320000  \n",
      "20-30   0.246575         20       45   0.307692  \n",
      "<20     0.234234         16       61   0.207792  \n",
      "sum precision:[ 0.5         0.38709677  0.45945946]\n"
     ]
    }
   ],
   "source": [
    "sum_proba, sum_y,resultdf,p_matrix= futureTest2([85,85],X_scaled,y,X[:,c.dateColumn],numOfWeek=25,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[lower</th>\n",
       "      <th>upper)</th>\n",
       "      <th>h_Correct</th>\n",
       "      <th>h_Wrong</th>\n",
       "      <th>h_Precent</th>\n",
       "      <th>d_Correct</th>\n",
       "      <th>d_Wrong</th>\n",
       "      <th>d_Precent</th>\n",
       "      <th>a_Correct</th>\n",
       "      <th>a_Wrong</th>\n",
       "      <th>a_Precent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&gt;80</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>191</td>\n",
       "      <td>77</td>\n",
       "      <td>0.712687</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-60</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>71</td>\n",
       "      <td>75</td>\n",
       "      <td>0.486301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "      <td>49</td>\n",
       "      <td>0.601626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-50</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>75</td>\n",
       "      <td>97</td>\n",
       "      <td>0.436047</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>92</td>\n",
       "      <td>0.468208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30-40</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>67</td>\n",
       "      <td>116</td>\n",
       "      <td>0.366120</td>\n",
       "      <td>42</td>\n",
       "      <td>97</td>\n",
       "      <td>0.302158</td>\n",
       "      <td>60</td>\n",
       "      <td>131</td>\n",
       "      <td>0.314136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-30</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>45</td>\n",
       "      <td>136</td>\n",
       "      <td>0.248619</td>\n",
       "      <td>191</td>\n",
       "      <td>548</td>\n",
       "      <td>0.258457</td>\n",
       "      <td>63</td>\n",
       "      <td>170</td>\n",
       "      <td>0.270386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>19</td>\n",
       "      <td>96</td>\n",
       "      <td>0.165217</td>\n",
       "      <td>31</td>\n",
       "      <td>187</td>\n",
       "      <td>0.142202</td>\n",
       "      <td>51</td>\n",
       "      <td>303</td>\n",
       "      <td>0.144068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
       ">80       0.8     1.0         21       10   0.677419          0        0   \n",
       "60-80     0.6     0.8        191       77   0.712687          0        0   \n",
       "50-60     0.5     0.6         71       75   0.486301          0        0   \n",
       "40-50     0.4     0.5         75       97   0.436047          0        0   \n",
       "30-40     0.3     0.4         67      116   0.366120         42       97   \n",
       "20-30     0.2     0.3         45      136   0.248619        191      548   \n",
       "<20       0.0     0.2         19       96   0.165217         31      187   \n",
       "\n",
       "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
       ">80          NaN          0        0        NaN  \n",
       "60-80        NaN         14        8   0.636364  \n",
       "50-60        NaN         74       49   0.601626  \n",
       "40-50        NaN         81       92   0.468208  \n",
       "30-40   0.302158         60      131   0.314136  \n",
       "20-30   0.258457         63      170   0.270386  \n",
       "<20     0.142202         51      303   0.144068  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findRecordsBy(self,df):\n",
    "    #print((df['DayStamp']).values)\n",
    "    #date = convertToDate((df['DayStamp']).values)\n",
    "    #df[\"Date\"] = date\n",
    "    \n",
    "    home = df['HomeTeam'].values\n",
    "    away = df['AwayTeam'].values\n",
    "    origin = self.df[[\"Date\",\"HomeTeam\",\"AwayTeam\",\"JocH\",\"JocD\",\"JocA\"]]\n",
    "    origin[\"DayStamp\"]=(pd.to_numeric(origin['Date'])/1e9/24/60/60).values\n",
    "    origin[\"DayStamp\"] = origin[\"DayStamp\"].apply(lambda x: \"%.f\"%(float(x)))\n",
    "    df['DayStamp']=df['DayStamp'].apply(lambda x: \"%.f\"%(float(x)))\n",
    "    return origin.merge(df,left_on=['DayStamp',\"HomeTeam\",\"AwayTeam\"],right_on=[\"DayStamp\",\"HomeTeam\",\"AwayTeam\"],how='inner')\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "withodds = findRecordsBy(c,resultdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def formatMatrixs(oddDf , precisionDf):\n",
    "    proba_mat = oddDf[['H_prob','D_prob','A_prob']].values\n",
    "    print(precisionDf)\n",
    "    def locatePrecision(proba_mat, precisionDf):\n",
    "        precisionMat = precisionDf.values\n",
    "        pre_cols=[4,7,10]\n",
    "        def convert(proba, pre_col = 4):\n",
    "            if proba < 0.2:\n",
    "                return proba\n",
    "            for i in range(precisionMat.shape[0]):\n",
    "                if precisionMat[i,0] <= proba and proba < precisionMat[i,1] :\n",
    "                    if math.isnan(precisionMat[i,pre_col]):\n",
    "                        return proba\n",
    "                    else:\n",
    "                        return precisionMat[i,pre_col]\n",
    "        h_fproba = np.array([ convert(float(proba)) for proba in proba_mat[:,0] ] )\n",
    "        d_fproba = np.array([ convert(float(proba),pre_col=7) for proba in proba_mat[:,1] ] )\n",
    "        a_fproba = np.array([ convert(float(proba),pre_col=10) for proba in proba_mat[:,2] ] )\n",
    "        return h_fproba, d_fproba,a_fproba\n",
    "    h_fproba, d_fproba,a_fproba =locatePrecision(proba_mat,precisionDf)\n",
    "    fproba_mat = np.hstack([h_fproba,d_fproba,a_fproba]).reshape(3,h_fproba.shape[0]).T\n",
    "    print(fproba_mat)\n",
    "    odd_mat = oddDf[['JocH','JocD','JocA']].values\n",
    "    print(odd_mat)\n",
    "    print(fproba_mat*odd_mat)\n",
    "    win_mat = oddDf[['H','D','A']].values\n",
    "    print(win_mat)\n",
    "    return fproba_mat,odd_mat,win_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
      ">80       0.8     1.0         21       10   0.677419          0        0   \n",
      "60-80     0.6     0.8        191       77   0.712687          0        0   \n",
      "50-60     0.5     0.6         71       75   0.486301          0        0   \n",
      "40-50     0.4     0.5         75       97   0.436047          0        0   \n",
      "30-40     0.3     0.4         67      116   0.366120         42       97   \n",
      "20-30     0.2     0.3         45      136   0.248619        191      548   \n",
      "<20       0.0     0.2         19       96   0.165217         31      187   \n",
      "\n",
      "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
      ">80          NaN          0        0        NaN  \n",
      "60-80        NaN         14        8   0.636364  \n",
      "50-60        NaN         74       49   0.601626  \n",
      "40-50        NaN         81       92   0.468208  \n",
      "30-40   0.302158         60      131   0.314136  \n",
      "20-30   0.258457         63      170   0.270386  \n",
      "<20     0.142202         51      303   0.144068  \n",
      "[[  7.12686567e-01   2.58457375e-01   1.59696043e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   1.53413460e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  2.48618785e-01   2.58457375e-01   6.01626016e-01]\n",
      " [  7.12686567e-01   1.87955096e-01   2.70386266e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   1.79950908e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  6.77419355e-01   7.63774589e-02   7.79646188e-02]\n",
      " [  2.90365033e-02   1.24205329e-01   8.46758187e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   1.99923903e-01]\n",
      " [  2.48618785e-01   3.02158273e-01   4.68208092e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  2.48618785e-01   3.02158273e-01   4.68208092e-01]\n",
      " [  2.48618785e-01   2.58457375e-01   4.68208092e-01]\n",
      " [  4.91091646e-02   6.00834973e-02   8.90807331e-01]\n",
      " [  7.12686567e-01   1.19317286e-01   1.46177411e-01]\n",
      " [  3.66120219e-01   4.99388307e-01   1.98743269e-01]\n",
      " [  7.12686567e-01   2.58457375e-01   1.88055024e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   1.64401934e-01]\n",
      " [  7.12686567e-01   2.58457375e-01   1.65597171e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   1.74328223e-01]\n",
      " [  2.48618785e-01   2.58457375e-01   6.01626016e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   1.93653002e-01]\n",
      " [  7.36520141e-02   1.37572333e-01   6.36363636e-01]\n",
      " [  2.48618785e-01   2.58457375e-01   4.68208092e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  1.55833974e-01   3.02158273e-01   6.01626016e-01]\n",
      " [  7.12686567e-01   1.54565960e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  4.86301370e-01   1.17343344e-01   3.14136126e-01]\n",
      " [  6.77419355e-01   6.03738241e-02   8.98151919e-02]\n",
      " [  1.28459454e-01   9.26857889e-02   6.36363636e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  7.12686567e-01   1.93286970e-01   2.70386266e-01]\n",
      " [  6.77419355e-01   3.94442230e-02   4.10505459e-02]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  7.12686567e-01   1.05361626e-01   2.70386266e-01]\n",
      " [  1.09497704e-01   1.32875726e-01   6.36363636e-01]\n",
      " [  7.12686567e-01   2.58457375e-01   6.81206211e-02]\n",
      " [  7.81636313e-02   1.85698733e-01   6.36363636e-01]\n",
      " [  3.66120219e-01   4.34473127e-01   2.70386266e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   1.98706746e-01]\n",
      " [  1.48403659e-01   1.60003468e-01   6.36363636e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  2.48618785e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  1.48004413e-01   6.95824623e-02   6.36363636e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   1.87999517e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  7.12686567e-01   1.50989369e-01   1.78113878e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  7.12686567e-01   6.32372200e-02   2.70386266e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   1.87388554e-01   3.14136126e-01]\n",
      " [  1.05025969e-01   1.90229893e-01   6.36363636e-01]\n",
      " [  7.12686567e-01   2.58457375e-01   1.61620095e-01]\n",
      " [  2.48618785e-01   1.17641993e-01   6.36363636e-01]\n",
      " [  3.95732671e-02   2.58457375e-01   6.36363636e-01]\n",
      " [  1.87193323e-02   1.44928843e-01   8.36351812e-01]\n",
      " [  4.36046512e-01   1.88777119e-01   3.14136126e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   1.95038229e-01]\n",
      " [  7.12686567e-01   1.79442301e-01   1.51016459e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  4.86301370e-01   1.53133944e-01   3.14136126e-01]\n",
      " [  2.48618785e-01   9.99055356e-02   6.36363636e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  8.31376910e-02   3.98970805e-02   8.76965225e-01]\n",
      " [  1.94975823e-01   2.58457375e-01   6.01626016e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   4.85139847e-01   2.70386266e-01]\n",
      " [  2.48618785e-01   1.81139648e-01   6.01626016e-01]\n",
      " [  2.48618785e-01   4.48485464e-01   2.70386266e-01]\n",
      " [  1.48625433e-01   5.78145325e-01   2.70386266e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  4.86301370e-01   2.58457375e-01   1.79633588e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  7.12686567e-01   4.83665690e-02   1.84673920e-01]\n",
      " [  1.51698276e-01   1.36476427e-01   6.36363636e-01]\n",
      " [  4.86301370e-01   3.02158273e-01   8.44736248e-02]\n",
      " [  6.77419355e-01   2.02122331e-02   3.59380841e-02]\n",
      " [  2.48618785e-01   4.35194343e-01   2.70386266e-01]\n",
      " [  7.12686567e-01   3.02158273e-01   8.78835171e-02]\n",
      " [  3.66120219e-01   4.68674868e-01   1.75162971e-01]\n",
      " [  4.86301370e-01   1.50634289e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  9.89830270e-02   1.29655808e-01   6.36363636e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  2.48618785e-01   1.86029226e-01   6.01626016e-01]\n",
      " [  4.36046512e-01   5.08674383e-01   1.07315704e-02]\n",
      " [  3.66120219e-01   7.26499856e-02   6.01626016e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  1.09967612e-01   8.80089253e-02   8.02023470e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  4.86301370e-01   1.07310675e-01   2.70386266e-01]\n",
      " [  7.12686567e-01   1.96614861e-01   1.70820296e-01]\n",
      " [  6.77419355e-01   8.40889290e-02   4.82595824e-02]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  1.61323473e-01   3.02158273e-01   4.68208092e-01]\n",
      " [  2.61515751e-02   3.37759890e-02   9.40072417e-01]\n",
      " [  4.36046512e-01   1.86302021e-01   3.14136126e-01]\n",
      " [  7.12686567e-01   2.58457375e-01   5.46826534e-02]\n",
      " [  4.86301370e-01   2.58457375e-01   1.62137166e-01]\n",
      " [  4.36046512e-01   1.78172857e-01   3.14136126e-01]\n",
      " [  3.94305354e-03   3.22683970e-03   9.92830098e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  1.99189380e-01   7.53731251e-01   4.70793471e-02]\n",
      " [  7.12686567e-01   3.02158273e-01   2.64532752e-02]\n",
      " [  1.77283600e-01   2.97167748e-02   6.36363636e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  2.48618785e-01   2.58457375e-01   6.01626016e-01]\n",
      " [  4.86301370e-01   3.02158273e-01   1.45598739e-01]\n",
      " [  7.12686567e-01   1.59527794e-01   1.84605330e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  8.63690451e-02   1.87080219e-01   6.36363636e-01]\n",
      " [  2.48618785e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  1.07300527e-01   1.06245972e-01   6.36363636e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  6.77419355e-01   1.00699402e-02   2.57597286e-02]\n",
      " [  7.12686567e-01   2.58457375e-01   4.35819477e-02]\n",
      " [  4.86301370e-01   4.11744773e-01   4.44416739e-02]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  3.66120219e-01   7.39848092e-02   6.01626016e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  7.12686567e-01   1.61381096e-01   1.42771304e-01]\n",
      " [  6.42139371e-03   2.43041920e-03   9.91148174e-01]\n",
      " [  7.12686567e-01   3.02158273e-01   4.53946367e-02]\n",
      " [  7.06773810e-03   8.68091166e-01   1.24841087e-01]\n",
      " [  2.48618785e-01   4.23202157e-01   3.14136126e-01]\n",
      " [  4.86301370e-01   1.83360025e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   1.70812309e-01   4.68208092e-01]\n",
      " [  3.66120219e-01   1.84865877e-01   4.68208092e-01]\n",
      " [  4.86301370e-01   1.83603719e-01   2.70386266e-01]\n",
      " [  1.66971274e-02   4.24998701e-02   9.40802991e-01]\n",
      " [  7.46538341e-02   1.94546700e-01   6.36363636e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  1.63885310e-01   8.28447282e-01   7.66742090e-03]\n",
      " [  4.36046512e-01   1.76544040e-01   4.68208092e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   1.87777579e-01]\n",
      " [  6.77419355e-01   2.14215554e-02   1.25174418e-01]\n",
      " [  7.12686567e-01   3.02158273e-01   4.93935421e-02]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  2.78275739e-02   5.66427037e-02   9.15529728e-01]\n",
      " [  4.86301370e-01   1.82471812e-01   2.70386266e-01]\n",
      " [  1.07761711e-01   6.44901931e-01   2.70386266e-01]\n",
      " [  6.77419355e-01   5.50204292e-02   4.91463169e-02]\n",
      " [  6.77419355e-01   6.71374127e-02   1.06526949e-01]\n",
      " [  1.48243591e-01   1.99357212e-01   6.36363636e-01]\n",
      " [  2.48618785e-01   5.12725472e-01   1.91751078e-01]\n",
      " [  6.26435280e-02   2.58457375e-01   6.36363636e-01]\n",
      " [  3.66120219e-01   1.29638940e-01   4.68208092e-01]\n",
      " [  7.12686567e-01   7.50403106e-02   2.70386266e-01]\n",
      " [  1.40139638e-02   5.20397946e-02   9.33946252e-01]\n",
      " [  1.04263760e-01   6.18385077e-02   8.33897710e-01]\n",
      " [  2.17065718e-02   6.58867136e-02   9.12406683e-01]\n",
      " [  6.77419355e-01   6.00658730e-02   3.27061042e-02]\n",
      " [  7.12686567e-01   7.46980011e-02   3.14136126e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  6.77419355e-01   7.69528151e-02   1.00321800e-01]\n",
      " [  6.77419355e-01   4.79303338e-02   5.74989570e-03]\n",
      " [  1.71988979e-01   8.35599676e-02   6.36363636e-01]\n",
      " [  4.86301370e-01   1.71140999e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   4.68208092e-01]\n",
      " [  4.86301370e-01   4.34280485e-01   1.90290865e-02]\n",
      " [  2.48618785e-01   4.70273435e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   4.68208092e-01]\n",
      " [  7.12686567e-01   9.37698260e-02   1.41292810e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  6.77419355e-01   1.14171341e-01   2.37155836e-02]\n",
      " [  2.48618785e-01   4.31169629e-01   3.14136126e-01]\n",
      " [  6.77419355e-01   7.91212296e-05   3.09750263e-04]\n",
      " [  3.37850116e-03   8.44363123e-03   9.88177896e-01]\n",
      " [  1.83225516e-02   1.20271025e-02   9.69650328e-01]\n",
      " [  7.12686567e-01   1.00831315e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  4.86301370e-01   3.02158273e-01   8.45935270e-02]\n",
      " [  7.12686567e-01   3.02158273e-01   4.87898886e-02]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  1.68057889e-01   1.22111104e-01   6.36363636e-01]\n",
      " [  2.48618785e-01   1.69742599e-01   6.36363636e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  2.48618785e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  3.66120219e-01   1.46451339e-01   4.68208092e-01]\n",
      " [  7.12686567e-01   4.36129309e-02   2.70386266e-01]\n",
      " [  7.12686567e-01   7.18393251e-02   1.37221202e-01]\n",
      " [  2.48618785e-01   6.98840797e-01   6.91742525e-02]\n",
      " [  6.77419355e-01   1.29984692e-02   1.02694206e-01]\n",
      " [  1.05235423e-03   2.96104513e-03   9.95986581e-01]\n",
      " [  7.12686567e-01   7.01110289e-02   2.70386266e-01]\n",
      " [  6.77419355e-01   1.96886972e-01   1.71586778e-03]\n",
      " [  6.77419355e-01   8.30085017e-04   1.08822472e-02]\n",
      " [  4.36046512e-01   1.81320548e-01   3.14136126e-01]\n",
      " [  4.86301370e-01   5.82720265e-02   3.14136126e-01]\n",
      " [  7.12686567e-01   2.58457375e-01   1.05636127e-01]\n",
      " [  3.66120219e-01   2.58457375e-01   3.14136126e-01]\n",
      " [  1.34315509e-02   8.52783993e-02   9.01290059e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  6.77419355e-01   8.46061041e-04   7.82349799e-03]\n",
      " [  3.66120219e-01   2.58457375e-01   4.68208092e-01]\n",
      " [  4.36046512e-01   4.08516288e-01   9.88957435e-02]\n",
      " [  2.48618785e-01   5.38014770e-01   1.70448080e-01]\n",
      " [  2.48618785e-01   5.09664416e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   4.87262964e-01   1.10574022e-01]\n",
      " [  7.12686567e-01   1.25640765e-01   1.44730866e-01]\n",
      " [  2.48618785e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  1.48330435e-01   1.43588901e-01   6.36363636e-01]\n",
      " [  3.66120219e-01   2.15154104e-02   6.36363636e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  4.86301370e-01   4.19803113e-02   3.14136126e-01]\n",
      " [  4.36046512e-01   3.02158273e-01   1.74047709e-01]\n",
      " [  6.77419355e-01   2.83090454e-02   1.30754247e-01]\n",
      " [  4.36046512e-01   4.73445952e-02   6.01626016e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  7.12686567e-01   2.58457375e-01   5.80766387e-02]\n",
      " [  7.12686567e-01   2.58457375e-01   7.81826749e-02]\n",
      " [  3.66120219e-01   3.02158273e-01   2.70386266e-01]\n",
      " [  4.36046512e-01   2.58457375e-01   2.70386266e-01]\n",
      " [  7.12686567e-01   3.02158273e-01   2.54075900e-02]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  3.66120219e-01   3.02158273e-01   3.14136126e-01]\n",
      " [  8.71524855e-04   5.73945343e-01   4.68208092e-01]\n",
      " [  7.12686567e-01   1.37221664e-01   2.70386266e-01]\n",
      " [  1.08041726e-02   2.12937462e-04   9.88982916e-01]\n",
      " [  7.12686567e-01   1.95472926e-01   1.23717394e-02]\n",
      " [  7.12686567e-01   9.04223919e-02   3.14136126e-01]\n",
      " [  8.90452787e-02   4.29386161e-02   8.68016124e-01]\n",
      " [  6.77419355e-01   8.54526996e-04   1.78109165e-02]\n",
      " [  1.65466731e-03   1.73334345e-01   8.25010955e-01]\n",
      " [  6.77419355e-01   3.64925800e-05   4.71334177e-04]\n",
      " [  3.66120219e-01   2.58457375e-01   4.68208092e-01]\n",
      " [  2.64973193e-02   4.01827425e-01   6.01626016e-01]\n",
      " [  7.12686567e-01   1.39022633e-01   2.70386266e-01]]\n",
      "[[  1.45   4.05   5.6 ]\n",
      " [  1.69   3.5    4.2 ]\n",
      " [  2.45   3.1    2.6 ]\n",
      " [  2.06   3.2    3.15]\n",
      " [  1.54   3.6    5.35]\n",
      " [  2.8    3.15   2.27]\n",
      " [  2.4    3.4    2.48]\n",
      " [  1.55   3.7    5.  ]\n",
      " [  2.18   3.     3.1 ]\n",
      " [  2.02   3.05   3.45]\n",
      " [  1.41   4.1    6.2 ]\n",
      " [  6.6    4.2    1.38]\n",
      " [  2.5    2.95   2.67]\n",
      " [  5.5    4.1    1.45]\n",
      " [  1.78   3.2    4.15]\n",
      " [  4.     3.15   1.83]\n",
      " [  3.45   3.2    1.95]\n",
      " [  2.8    3.1    2.3 ]\n",
      " [  1.12   6.5   15.  ]\n",
      " [  2.55   3.2    2.45]\n",
      " [  2.22   3.25   2.8 ]\n",
      " [  1.59   3.75   4.6 ]\n",
      " [  1.5    3.7    5.6 ]\n",
      " [  1.8    3.2    4.05]\n",
      " [  1.36   4.1    7.4 ]\n",
      " [  1.85   3.25   3.75]\n",
      " [  1.53   3.55   5.6 ]\n",
      " [  2.42   3.2    2.57]\n",
      " [  1.28   4.6    8.6 ]\n",
      " [  3.7    3.3    1.85]\n",
      " [  2.37   3.1    2.7 ]\n",
      " [  5.4    3.8    1.5 ]\n",
      " [  4.5    3.5    1.65]\n",
      " [  1.51   3.65   5.6 ]\n",
      " [  2.7    3.     2.43]\n",
      " [  2.9    3.05   2.27]\n",
      " [  1.38   4.05   7.  ]\n",
      " [  2.22   3.1    2.95]\n",
      " [  2.65   3.05   2.45]\n",
      " [  1.54   3.55   5.5 ]\n",
      " [  1.4    4.15   6.25]\n",
      " [  2.6    2.92   2.58]\n",
      " [  1.72   3.4    4.2 ]\n",
      " [  1.9    3.45   3.35]\n",
      " [  4.75   3.4    1.64]\n",
      " [  1.58   3.8    4.6 ]\n",
      " [  5.05   3.45   1.6 ]\n",
      " [  2.25   3.2    2.8 ]\n",
      " [  1.41   4.05   6.3 ]\n",
      " [  2.1    3.05   3.25]\n",
      " [  1.33   4.4    7.4 ]\n",
      " [  1.92   3.2    3.55]\n",
      " [  3.     3.     2.23]\n",
      " [  3.7    3.3    1.85]\n",
      " [  1.92   3.1    3.7 ]\n",
      " [  2.03   3.05   3.4 ]\n",
      " [  1.37   4.2    6.8 ]\n",
      " [  2.45   3.2    2.55]\n",
      " [  1.59   3.45   5.15]\n",
      " [  1.32   4.4    7.75]\n",
      " [  1.45   4.05   5.6 ]\n",
      " [  2.25   3.05   2.92]\n",
      " [  2.07   3.1    3.25]\n",
      " [  4.     3.25   1.8 ]\n",
      " [  1.55   3.9    4.7 ]\n",
      " [  2.47   3.15   2.55]\n",
      " [  6.2    4.2    1.4 ]\n",
      " [  4.05   3.45   1.73]\n",
      " [  1.69   3.35   4.5 ]\n",
      " [  1.62   3.5    4.7 ]\n",
      " [  1.52   3.55   5.7 ]\n",
      " [  2.33   3.05   2.8 ]\n",
      " [  2.57   2.85   2.68]\n",
      " [  1.62   3.55   4.65]\n",
      " [  1.87   3.3    3.6 ]\n",
      " [  4.9    3.9    1.53]\n",
      " [  2.03   2.95   3.55]\n",
      " [  1.94   3.2    3.5 ]\n",
      " [  3.9    3.3    1.8 ]\n",
      " [  1.71   3.2    4.6 ]\n",
      " [  1.84   3.25   3.8 ]\n",
      " [  3.4    3.15   1.98]\n",
      " [  2.85   3.15   2.25]\n",
      " [  2.25   2.95   3.  ]\n",
      " [  4.1    3.4    1.74]\n",
      " [  2.1    3.15   3.1 ]\n",
      " [  1.25   4.85   9.25]\n",
      " [  2.38   3.2    2.62]\n",
      " [  1.56   3.45   5.5 ]\n",
      " [  1.63   3.65   4.4 ]\n",
      " [  1.78   3.35   3.95]\n",
      " [  1.29   4.75   7.75]\n",
      " [  3.3    3.2    2.  ]\n",
      " [  1.8    3.25   4.  ]\n",
      " [  1.37   4.5    6.2 ]\n",
      " [  1.87   3.2    3.75]\n",
      " [  2.72   3.25   2.28]\n",
      " [  1.63   3.35   4.95]\n",
      " [  1.61   3.7    4.5 ]\n",
      " [  1.31   4.3    8.4 ]\n",
      " [  4.75   3.35   1.65]\n",
      " [  1.88   3.25   3.65]\n",
      " [  3.     3.05   2.2 ]\n",
      " [  2.47   3.3    2.47]\n",
      " [  3.65   3.2    1.9 ]\n",
      " [  2.23   3.05   2.95]\n",
      " [  4.75   3.75   1.57]\n",
      " [  2.6    2.9    2.6 ]\n",
      " [  3.75   3.2    1.87]\n",
      " [  1.41   3.85   6.85]\n",
      " [  1.17   5.75  11.5 ]\n",
      " [  1.8    3.45   3.7 ]\n",
      " [  3.     3.1    2.18]\n",
      " [  5.2    3.6    1.55]\n",
      " [  3.9    3.7    1.7 ]\n",
      " [  2.5    3.05   2.58]\n",
      " [  1.37   4.3    6.6 ]\n",
      " [  1.77   3.3    4.05]\n",
      " [  2.     3.05   3.5 ]\n",
      " [  3.5    3.2    1.94]\n",
      " [  2.03   3.05   3.4 ]\n",
      " [  3.35   3.05   2.05]\n",
      " [  1.92   3.1    3.7 ]\n",
      " [  1.33   4.35   7.5 ]\n",
      " [  1.75   3.6    3.75]\n",
      " [  3.     3.35   2.08]\n",
      " [  2.25   3.15   2.83]\n",
      " [  1.12   6.6   14.  ]\n",
      " [  2.4    2.95   2.78]\n",
      " [  3.4    3.3    1.93]\n",
      " [  3.25   3.05   2.1 ]\n",
      " [  1.76   3.15   4.35]\n",
      " [  1.34   4.35   7.25]\n",
      " [  2.2    3.3    2.8 ]\n",
      " [  1.87   3.2    3.75]\n",
      " [  3.9    3.25   1.82]\n",
      " [  2.3    3.     2.88]\n",
      " [  2.45   3.1    2.6 ]\n",
      " [  1.88   3.25   3.65]\n",
      " [  1.35   4.1    7.7 ]\n",
      " [  1.92   3.45   3.3 ]\n",
      " [  1.21   5.1   10.75]\n",
      " [  2.37   3.05   2.73]\n",
      " [  2.93   3.25   2.15]\n",
      " [  1.29   4.4    8.9 ]\n",
      " [  1.29   4.7    7.9 ]\n",
      " [  5.8    3.9    1.46]\n",
      " [  2.45   3.05   2.65]\n",
      " [  2.6    3.05   2.5 ]\n",
      " [  2.72   3.3    2.25]\n",
      " [  1.21   5.35  10.  ]\n",
      " [  2.47   3.05   2.62]\n",
      " [  3.15   3.15   2.08]\n",
      " [  1.88   3.35   3.5 ]\n",
      " [  5.4    3.95   1.48]\n",
      " [  4.3    3.4    1.7 ]\n",
      " [  1.95   3.25   3.4 ]\n",
      " [  2.55   3.25   2.42]\n",
      " [  5.45   3.9    1.48]\n",
      " [  1.18   5.6   11.25]\n",
      " [  1.32   4.25   8.2 ]\n",
      " [  1.29   4.7    7.9 ]\n",
      " [  1.42   3.95   6.35]\n",
      " [  1.9    3.25   3.6 ]\n",
      " [  2.37   3.25   2.6 ]\n",
      " [  5.25   4.05   1.48]\n",
      " [  1.35   4.25   7.25]\n",
      " [  2.9    3.     2.3 ]\n",
      " [  1.47   4.     5.5 ]\n",
      " [  2.9    3.25   2.17]\n",
      " [  1.64   3.55   4.5 ]\n",
      " [  2.55   3.35   2.37]\n",
      " [  2.58   2.95   2.58]\n",
      " [  1.46   3.85   5.9 ]\n",
      " [  1.43   4.05   6.  ]\n",
      " [  6.2    4.1    1.41]\n",
      " [  2.57   3.4    2.32]\n",
      " [  4.25   3.2    1.77]\n",
      " [  1.84   3.3    3.75]\n",
      " [  1.56   3.75   4.85]\n",
      " [  1.46   3.75   6.2 ]\n",
      " [  1.48   3.8    5.7 ]\n",
      " [  1.31   4.5    7.75]\n",
      " [  7.25   4.35   1.34]\n",
      " [  1.54   3.9    4.8 ]\n",
      " [  1.56   3.7    5.  ]\n",
      " [  1.23   4.75  11.  ]\n",
      " [  2.4    3.35   2.52]\n",
      " [  4.85   3.6    1.59]\n",
      " [  3.65   3.25   1.88]\n",
      " [  1.7    3.55   4.1 ]\n",
      " [  2.07   3.3    3.05]\n",
      " [  2.33   3.2    2.7 ]\n",
      " [  1.32   4.35   8.  ]\n",
      " [  1.45   3.95   5.9 ]\n",
      " [  1.38   4.15   6.75]\n",
      " [  3.65   3.25   1.88]\n",
      " [  2.3    3.15   2.77]\n",
      " [  4.75   3.75   1.57]\n",
      " [  2.82   3.2    2.25]\n",
      " [  1.19   5.65  10.5 ]\n",
      " [  1.9    3.25   3.6 ]\n",
      " [  2.14   3.2    3.  ]\n",
      " [  1.86   3.3    3.65]\n",
      " [  3.     3.25   2.12]\n",
      " [  2.45   3.15   2.58]\n",
      " [  2.42   3.3    2.52]\n",
      " [  2.75   3.     2.4 ]\n",
      " [  1.85   3.25   3.8 ]\n",
      " [  1.97   3.1    3.55]\n",
      " [  3.7    3.5    1.8 ]\n",
      " [  2.02   3.4    3.1 ]\n",
      " [  1.34   4.5    7.  ]\n",
      " [  1.86   3.2    3.8 ]\n",
      " [  2.35   3.25   2.62]\n",
      " [  5.5    4.05   1.46]\n",
      " [  2.22   3.25   2.82]\n",
      " [  1.65   3.6    4.35]\n",
      " [  1.25   5.     8.75]\n",
      " [  2.95   3.2    2.17]\n",
      " [  1.83   3.4    3.65]\n",
      " [  1.22   5.2   10.  ]\n",
      " [  1.88   3.1    3.85]\n",
      " [  2.52   3.2    2.47]\n",
      " [  2.48   3.25   2.48]\n",
      " [  1.78   3.4    3.9 ]\n",
      " [  1.16   6.1   11.5 ]\n",
      " [  1.66   3.4    4.6 ]\n",
      " [  1.66   3.55   4.4 ]\n",
      " [  1.95   3.25   3.4 ]\n",
      " [  2.27   3.05   2.9 ]\n",
      " [  2.95   3.25   2.15]\n",
      " [  2.12   3.35   2.92]\n",
      " [  2.98   3.1    2.2 ]\n",
      " [  2.22   3.15   2.9 ]\n",
      " [  4.15   3.7    1.66]\n",
      " [  2.05   3.3    3.1 ]\n",
      " [  3.65   3.5    1.81]\n",
      " [  2.42   3.15   2.62]\n",
      " [  1.13   6.1   15.  ]\n",
      " [  5.1    3.85   1.52]\n",
      " [  1.38   4.1    6.9 ]\n",
      " [  1.85   3.25   3.75]\n",
      " [  1.53   3.7    5.3 ]\n",
      " [  3.     3.05   2.22]\n",
      " [  1.77   3.45   3.9 ]\n",
      " [  2.28   3.3    2.7 ]\n",
      " [  2.08   3.05   3.3 ]\n",
      " [  1.58   3.6    4.9 ]\n",
      " [  2.07   3.2    3.15]\n",
      " [  1.16   6.    12.  ]\n",
      " [  2.3    3.1    2.82]\n",
      " [  2.32   3.15   2.75]\n",
      " [  2.05   3.25   3.15]\n",
      " [  4.1    3.35   1.75]\n",
      " [  1.72   3.4    4.2 ]\n",
      " [  3.     3.05   2.22]\n",
      " [  1.18   5.6   11.5 ]\n",
      " [  3.85   3.35   1.8 ]\n",
      " [  5.25   4.05   1.48]\n",
      " [  1.8    3.25   4.  ]]\n",
      "[[  1.03339552e+00   1.04675237e+00   8.94297838e-01]\n",
      " [  8.21849315e-01   9.04600812e-01   6.44336531e-01]\n",
      " [  8.96994536e-01   9.36690647e-01   8.16753927e-01]\n",
      " [  1.00178082e+00   8.27063599e-01   8.51716738e-01]\n",
      " [  6.71511628e-01   1.08776978e+00   1.44656652e+00]\n",
      " [  6.96132597e-01   8.14140731e-01   1.36569106e+00]\n",
      " [  1.71044776e+00   6.39047328e-01   6.70557940e-01]\n",
      " [  7.53767123e-01   9.56292287e-01   8.99754539e-01]\n",
      " [  7.98142077e-01   9.06474820e-01   9.73821990e-01]\n",
      " [  8.80813953e-01   7.88294993e-01   9.32832618e-01]\n",
      " [  9.55161290e-01   3.13147581e-01   4.83380637e-01]\n",
      " [  1.91640922e-01   5.21662380e-01   1.16852630e+00]\n",
      " [  1.21575342e+00   7.62449256e-01   5.33796820e-01]\n",
      " [  1.36740331e+00   1.23884892e+00   6.78901734e-01]\n",
      " [  8.65616438e-01   8.27063599e-01   1.12210300e+00]\n",
      " [  9.94475138e-01   9.51798561e-01   8.56820809e-01]\n",
      " [  8.57734807e-01   8.27063599e-01   9.13005780e-01]\n",
      " [  1.37505661e-01   1.86258842e-01   2.04885686e+00]\n",
      " [  7.98208955e-01   7.75562357e-01   2.19266117e+00]\n",
      " [  9.33606557e-01   1.59804258e+00   4.86921009e-01]\n",
      " [  1.58216418e+00   8.39986468e-01   5.26554066e-01]\n",
      " [  7.73219178e-01   9.69215156e-01   1.24377682e+00]\n",
      " [  5.49180328e-01   1.11798561e+00   1.51416309e+00]\n",
      " [  8.75342466e-01   8.27063599e-01   6.65827831e-01]\n",
      " [  9.69253731e-01   1.05967524e+00   1.22541906e+00]\n",
      " [  8.99657534e-01   8.39986468e-01   6.53730836e-01]\n",
      " [  3.80386740e-01   9.17523681e-01   3.36910569e+00]\n",
      " [  1.05523256e+00   9.66906475e-01   6.94892704e-01]\n",
      " [  6.22465753e-01   1.18890392e+00   1.66541582e+00]\n",
      " [  2.72512452e-01   4.53988700e-01   1.17727273e+00]\n",
      " [  5.89226519e-01   8.01217862e-01   1.26416185e+00]\n",
      " [  1.97704918e+00   9.82138024e-01   4.71204188e-01]\n",
      " [  7.01252885e-01   1.05755396e+00   9.92682927e-01]\n",
      " [  1.07615672e+00   5.64165755e-01   1.51416309e+00]\n",
      " [  9.88524590e-01   9.06474820e-01   7.63350785e-01]\n",
      " [  1.41027397e+00   3.57897199e-01   7.13089005e-01]\n",
      " [  9.34838710e-01   2.44513988e-01   6.28706343e-01]\n",
      " [  2.85179987e-01   2.87325945e-01   1.87727273e+00]\n",
      " [  9.70218579e-01   7.88294993e-01   7.69633508e-01]\n",
      " [  1.09753731e+00   6.86168744e-01   1.48712446e+00]\n",
      " [  9.48387097e-01   1.63693526e-01   2.56565912e-01]\n",
      " [  9.51912568e-01   7.54695535e-01   8.10471204e-01]\n",
      " [  8.36438356e-01   8.78755074e-01   1.13562232e+00]\n",
      " [  1.35410448e+00   3.63497608e-01   9.05793991e-01]\n",
      " [  5.20114092e-01   4.51777467e-01   1.04363636e+00]\n",
      " [  1.12604478e+00   9.82138024e-01   3.13354857e-01]\n",
      " [  3.94726338e-01   6.40660628e-01   1.01818182e+00]\n",
      " [  8.23770492e-01   1.39031401e+00   7.57081545e-01]\n",
      " [  6.85684932e-01   1.04675237e+00   1.25185250e+00]\n",
      " [  3.11647685e-01   4.88010579e-01   2.06818182e+00]\n",
      " [  6.46780822e-01   1.13721245e+00   2.00085837e+00]\n",
      " [  7.02950820e-01   9.66906475e-01   9.59871245e-01]\n",
      " [  7.45856354e-01   9.06474820e-01   7.00523560e-01]\n",
      " [  5.47616327e-01   2.29622126e-01   1.17727273e+00]\n",
      " [  8.37209302e-01   9.36690647e-01   6.95598212e-01]\n",
      " [  7.43224044e-01   9.21582734e-01   1.06806283e+00]\n",
      " [  9.76380597e-01   6.34155348e-01   1.21117437e+00]\n",
      " [  8.96994536e-01   9.66906475e-01   6.89484979e-01]\n",
      " [  6.93313953e-01   8.91677943e-01   1.39248927e+00]\n",
      " [  5.75581395e-01   1.32949640e+00   2.09549356e+00]\n",
      " [  1.03339552e+00   2.56110741e-01   1.51416309e+00]\n",
      " [  9.81104651e-01   7.88294993e-01   7.89527897e-01]\n",
      " [  9.02616279e-01   5.80904518e-01   1.02094241e+00]\n",
      " [  4.20103878e-01   6.18247151e-01   1.14545455e+00]\n",
      " [  1.10466418e+00   1.00798376e+00   7.59614448e-01]\n",
      " [  6.14088398e-01   3.70572278e-01   1.62272727e+00]\n",
      " [  2.45354256e-01   1.08552097e+00   8.90909091e-01]\n",
      " [  7.58132958e-02   5.00004508e-01   1.44688863e+00]\n",
      " [  7.36918605e-01   6.32403348e-01   1.41361257e+00]\n",
      " [  7.87808219e-01   9.04600812e-01   9.16679677e-01]\n",
      " [  1.08328358e+00   6.37020170e-01   8.60793816e-01]\n",
      " [  8.53060109e-01   7.88294993e-01   8.79581152e-01]\n",
      " [  1.12063953e+00   7.36603518e-01   8.41884817e-01]\n",
      " [  7.06395349e-01   9.17523681e-01   1.25729614e+00]\n",
      " [  9.09383562e-01   5.05342014e-01   1.13089005e+00]\n",
      " [  1.21823204e+00   3.89631589e-01   9.73636364e-01]\n",
      " [  8.85174419e-01   7.62449256e-01   1.11518325e+00]\n",
      " [  1.61287121e-01   1.27670658e-01   3.06937829e+00]\n",
      " [  7.60405710e-01   8.52909337e-01   1.08292683e+00]\n",
      " [  8.31575342e-01   8.27063599e-01   1.24377682e+00]\n",
      " [  6.73661202e-01   1.57670450e+00   1.02746781e+00]\n",
      " [  8.45303867e-01   5.70589891e-01   1.19121951e+00]\n",
      " [  7.08563536e-01   1.41272921e+00   6.08369099e-01]\n",
      " [  3.34407225e-01   1.70552871e+00   8.11158798e-01]\n",
      " [  1.99383562e+00   8.78755074e-01   4.70472103e-01]\n",
      " [  9.15697674e-01   8.14140731e-01   8.38197425e-01]\n",
      " [  5.45058140e-01   1.25351827e+00   2.50107296e+00]\n",
      " [  8.71366120e-01   8.27063599e-01   8.23036649e-01]\n",
      " [  5.71147541e-01   1.04244604e+00   1.48712446e+00]\n",
      " [  7.92671233e-01   9.43369418e-01   7.90387785e-01]\n",
      " [  7.76162791e-01   1.01223022e+00   1.06802575e+00]\n",
      " [  9.19365672e-01   2.29741203e-01   1.43122288e+00]\n",
      " [  5.00604312e-01   4.36724567e-01   1.27272727e+00]\n",
      " [  8.75342466e-01   9.82014388e-01   3.37894499e-01]\n",
      " [  9.28064516e-01   9.09550488e-02   2.22816122e-01]\n",
      " [  4.64917127e-01   1.39262190e+00   1.01394850e+00]\n",
      " [  1.93850746e+00   9.82014388e-01   2.00374419e-01]\n",
      " [  5.96775956e-01   1.57006081e+00   8.67056707e-01]\n",
      " [  7.82945205e-01   5.57346869e-01   1.21673820e+00]\n",
      " [  5.71220930e-01   1.11136671e+00   2.27124464e+00]\n",
      " [  4.70169378e-01   4.34346958e-01   1.05000000e+00]\n",
      " [  8.19767442e-01   8.39986468e-01   9.86909871e-01]\n",
      " [  7.45856354e-01   5.67389138e-01   1.32357724e+00]\n",
      " [  1.07703488e+00   1.67862546e+00   2.65069789e-02]\n",
      " [  1.33633880e+00   2.32479954e-01   1.14308943e+00]\n",
      " [  8.16448087e-01   9.21582734e-01   9.26701571e-01]\n",
      " [  5.22346156e-01   3.30033470e-01   1.25917685e+00]\n",
      " [  9.51912568e-01   8.76258993e-01   8.16753927e-01]\n",
      " [  1.63517442e+00   8.27063599e-01   5.87434555e-01]\n",
      " [  6.85684932e-01   4.13146099e-01   1.85214592e+00]\n",
      " [  8.33843284e-01   1.13053545e+00   1.96443340e+00]\n",
      " [  1.21935484e+00   2.90106805e-01   1.78560455e-01]\n",
      " [  1.09836066e+00   8.01217862e-01   6.84816754e-01]\n",
      " [  8.38882059e-01   1.08776978e+00   7.25722543e-01]\n",
      " [  1.01991143e-01   1.24971159e-01   1.59812311e+00]\n",
      " [  1.09011628e+00   5.68221164e-01   8.10471204e-01]\n",
      " [  9.76380597e-01   1.11136671e+00   3.60905512e-01]\n",
      " [  8.60753425e-01   8.52909337e-01   6.56655521e-01]\n",
      " [  8.72093023e-01   5.43427213e-01   1.09947644e+00]\n",
      " [  1.38006874e-02   1.03258871e-02   1.92609039e+00]\n",
      " [  8.85174419e-01   9.21582734e-01   9.19313305e-01]\n",
      " [  1.46075581e+00   7.88294993e-01   5.54291845e-01]\n",
      " [  7.02950820e-01   9.36690647e-01   1.00042918e+00]\n",
      " [  2.64921875e-01   3.27873094e+00   3.53095103e-01]\n",
      " [  1.24720149e+00   1.08776978e+00   9.91997821e-02]\n",
      " [  5.31850800e-01   9.95511957e-02   1.32363636e+00]\n",
      " [  9.81104651e-01   9.51798561e-01   7.65193133e-01]\n",
      " [  4.88372093e-01   1.70581867e+00   3.78540773e+00]\n",
      " [  8.78688525e-01   7.62449256e-01   8.73298429e-01]\n",
      " [  1.24480874e+00   8.52909337e-01   6.06282723e-01]\n",
      " [  8.08011050e-01   7.88294993e-01   1.26341463e+00]\n",
      " [  8.55890411e-01   9.51798561e-01   6.33354516e-01]\n",
      " [  9.55000000e-01   6.93945902e-01   1.33838864e+00]\n",
      " [  9.59302326e-01   8.52909337e-01   8.79581152e-01]\n",
      " [  8.15406977e-01   8.27063599e-01   1.17801047e+00]\n",
      " [  3.36839276e-01   6.08010713e-01   1.15818182e+00]\n",
      " [  5.71823204e-01   9.06474820e-01   9.04712042e-01]\n",
      " [  2.62886292e-01   3.29362514e-01   1.65454545e+00]\n",
      " [  8.19767442e-01   8.39986468e-01   1.14659686e+00]\n",
      " [  9.14516129e-01   4.12867550e-02   1.98349910e-01]\n",
      " [  1.36835821e+00   8.91677943e-01   1.43820427e-01]\n",
      " [  5.88424658e-01   2.09989834e+00   4.77747994e-01]\n",
      " [  8.67704918e-01   9.21582734e-01   8.57591623e-01]\n",
      " [  1.07273224e+00   2.40450630e-01   1.29349593e+00]\n",
      " [  5.62500000e-01   1.13721245e+00   2.40643777e+00]\n",
      " [  9.19365672e-01   7.58491150e-01   1.12789330e+00]\n",
      " [  3.72440835e-02   9.47863490e-03   1.44707633e+00]\n",
      " [  1.74608209e+00   9.21582734e-01   1.20295787e-01]\n",
      " [  1.83761191e-02   2.64767806e+00   3.12102716e-01]\n",
      " [  6.76243094e-01   1.39656712e+00   7.06806283e-01]\n",
      " [  5.88424658e-01   9.80976135e-01   2.70386266e+00]\n",
      " [  1.07703488e+00   5.20977542e-01   1.22670520e+00]\n",
      " [  1.15327869e+00   5.82327513e-01   9.73872832e-01]\n",
      " [  9.14246575e-01   6.15072458e-01   9.46351931e-01]\n",
      " [  9.01644878e-02   1.67874487e-01   1.39238843e+00]\n",
      " [  3.21011487e-01   6.61458778e-01   1.08181818e+00]\n",
      " [  8.50290698e-01   8.39986468e-01   9.19313305e-01]\n",
      " [  4.17907541e-01   2.69245367e+00   1.85551586e-02]\n",
      " [  2.37645349e+00   6.88521758e-01   6.92947977e-01]\n",
      " [  5.14534884e-01   1.69208633e+00   2.11249776e+00]\n",
      " [  8.94193548e-01   9.10416106e-02   1.02643023e+00]\n",
      " [  9.19365672e-01   1.42014388e+00   3.90208983e-01]\n",
      " [  5.19890710e-01   1.19352518e+00   1.99476440e+00]\n",
      " [  8.28488372e-01   8.39986468e-01   9.73390558e-01]\n",
      " [  8.67704918e-01   8.39986468e-01   8.16753927e-01]\n",
      " [  1.46094763e-01   2.29402950e-01   1.35498400e+00]\n",
      " [  6.56506849e-01   7.75505200e-01   1.96030043e+00]\n",
      " [  3.12508962e-01   1.93470579e+00   6.21888412e-01]\n",
      " [  9.95806452e-01   2.20081717e-01   2.70304743e-01]\n",
      " [  1.96451613e+00   2.18196591e-01   2.31163478e-01]\n",
      " [  2.43119490e-01   7.07718101e-01   2.86363636e+00]\n",
      " [  6.33977901e-01   1.71763033e+00   4.54450054e-01]\n",
      " [  1.61620302e-01   7.62449256e-01   1.64181818e+00]\n",
      " [  5.34535519e-01   4.99109919e-01   2.76242775e+00]\n",
      " [  1.01914179e+00   3.03913258e-01   1.62231760e+00]\n",
      " [  8.68865753e-02   2.13363158e-01   1.31686422e+00]\n",
      " [  2.67957864e-01   2.10250926e-01   1.93464269e+00]\n",
      " [  9.22529302e-02   2.10837483e-01   1.61495983e+00]\n",
      " [  1.24645161e+00   1.98217381e-01   1.22647891e-01]\n",
      " [  1.11179104e+00   2.80117504e-01   1.52356021e+00]\n",
      " [  6.36627907e-01   9.69215156e-01   1.94764398e+00]\n",
      " [  1.00258065e+00   2.92420697e-01   5.71834257e-01]\n",
      " [  8.87419355e-01   2.15686502e-01   4.45616917e-02]\n",
      " [  1.24692010e+00   3.63485859e-01   8.52727273e-01]\n",
      " [  7.48904110e-01   6.67449895e-01   1.29785408e+00]\n",
      " [  5.71147541e-01   9.56292287e-01   1.57068063e+00]\n",
      " [  4.50327869e-01   1.22767253e+00   5.15028902e+00]\n",
      " [  1.16712329e+00   1.45483962e+00   4.79532979e-02]\n",
      " [  1.20580110e+00   1.69298437e+00   4.29914163e-01]\n",
      " [  1.33633880e+00   8.39986468e-01   8.80231214e-01]\n",
      " [  1.21156716e+00   3.32882882e-01   5.79300523e-01]\n",
      " [  9.02616279e-01   9.97122302e-01   8.24678112e-01]\n",
      " [  1.01598837e+00   9.66906475e-01   7.30042918e-01]\n",
      " [  4.83278689e-01   1.12428958e+00   2.51308901e+00]\n",
      " [  6.32267442e-01   1.19352518e+00   1.59527897e+00]\n",
      " [  9.34838710e-01   4.73811065e-01   1.60080189e-01]\n",
      " [  9.07458564e-01   1.40130129e+00   5.90575916e-01]\n",
      " [  1.55806452e+00   2.49231873e-04   8.58008229e-04]\n",
      " [  1.60478805e-02   3.16636171e-02   1.55143930e+00]\n",
      " [  5.16695956e-02   3.84867281e-02   2.18171324e+00]\n",
      " [  8.48097015e-01   5.69696929e-01   2.83905579e+00]\n",
      " [  6.95628415e-01   9.82014388e-01   9.73390558e-01]\n",
      " [  1.04068493e+00   9.66906475e-01   2.53780581e-01]\n",
      " [  1.32559701e+00   9.97122302e-01   1.78083093e-01]\n",
      " [  1.30813953e+00   8.39986468e-01   5.73218884e-01]\n",
      " [  8.96994536e-01   9.51798561e-01   6.97596567e-01]\n",
      " [  4.06700091e-01   4.02966645e-01   1.60363636e+00]\n",
      " [  6.83701657e-01   5.09227797e-01   1.52727273e+00]\n",
      " [  6.77322404e-01   9.82014388e-01   1.02746781e+00]\n",
      " [  4.89779006e-01   9.36690647e-01   1.11518325e+00]\n",
      " [  1.35464481e+00   5.12579687e-01   8.42774566e-01]\n",
      " [  1.43962687e+00   1.48283965e-01   8.38197425e-01]\n",
      " [  9.55000000e-01   3.23276963e-01   9.60548416e-01]\n",
      " [  4.62430939e-01   2.23629055e+00   2.62862159e-01]\n",
      " [  1.59193548e+00   4.22450248e-02   2.69058820e-01]\n",
      " [  5.78794826e-03   1.19922328e-02   1.45414041e+00]\n",
      " [  1.58216418e+00   2.27860844e-01   7.62489270e-01]\n",
      " [  1.11774194e+00   7.08793098e-01   7.46402484e-03]\n",
      " [  8.46774194e-01   4.15042508e-03   9.52196633e-02]\n",
      " [  1.28633721e+00   5.80225754e-01   6.81675393e-01]\n",
      " [  8.89931507e-01   1.98124890e-01   1.14659686e+00]\n",
      " [  8.69477612e-01   1.34397835e+00   1.05636127e+00]\n",
      " [  6.88306011e-01   8.01217862e-01   1.20942408e+00]\n",
      " [  3.38475084e-02   2.72890878e-01   2.22618645e+00]\n",
      " [  9.07978142e-01   9.82014388e-01   7.79057592e-01]\n",
      " [  6.51693989e-01   1.02733813e+00   1.22513089e+00]\n",
      " [  7.85806452e-01   5.16097235e-03   8.99702269e-02]\n",
      " [  6.07759563e-01   8.78755074e-01   2.15375723e+00]\n",
      " [  7.23837209e-01   1.45023282e+00   4.35141271e-01]\n",
      " [  4.84806630e-01   1.74854800e+00   5.79523471e-01]\n",
      " [  5.64364641e-01   1.55447647e+00   7.84120172e-01]\n",
      " [  1.28633721e+00   1.58360463e+00   2.37734147e-01]\n",
      " [  1.51089552e+00   4.20896562e-01   4.22614129e-01]\n",
      " [  7.40883978e-01   9.36690647e-01   6.91099476e-01]\n",
      " [  3.29293566e-01   4.52305037e-01   1.84545455e+00]\n",
      " [  1.51939891e+00   7.96070185e-02   1.05636364e+00]\n",
      " [  7.50546448e-01   9.97122302e-01   9.73821990e-01]\n",
      " [  1.77500000e+00   1.46931089e-01   5.68586387e-01]\n",
      " [  1.05523256e+00   9.51798561e-01   4.56004996e-01]\n",
      " [  7.65483871e-01   1.72685177e-01   1.96131371e+00]\n",
      " [  2.22383721e+00   1.82276691e-01   9.14471545e-01]\n",
      " [  5.05245902e-01   1.23884892e+00   2.16753927e+00]\n",
      " [  1.31847015e+00   8.39986468e-01   2.17787395e-01]\n",
      " [  1.09041045e+00   9.56292287e-01   4.14368177e-01]\n",
      " [  1.09836066e+00   9.21582734e-01   6.00257511e-01]\n",
      " [  7.71802326e-01   8.91677943e-01   1.05450644e+00]\n",
      " [  1.62492537e+00   9.97122302e-01   6.86004929e-02]\n",
      " [  7.61530055e-01   9.21582734e-01   1.03664921e+00]\n",
      " [  5.78469945e-01   1.08776978e+00   1.53926702e+00]\n",
      " [  1.80405645e-03   1.83662510e+00   1.47485549e+00]\n",
      " [  8.26716418e-01   8.23329985e-01   3.24463519e+00]\n",
      " [  2.48495970e-02   6.60106134e-04   2.78893182e+00]\n",
      " [  1.65343284e+00   6.15739717e-01   3.40222833e-02]\n",
      " [  1.46100746e+00   2.93872774e-01   9.89528796e-01]\n",
      " [  3.65085643e-01   1.43844364e-01   1.51902822e+00]\n",
      " [  1.16516129e+00   2.90539179e-03   7.48058494e-02]\n",
      " [  4.96400194e-03   5.28669753e-01   1.83152432e+00]\n",
      " [  7.99354839e-01   2.04358448e-04   5.42034303e-03]\n",
      " [  1.40956284e+00   8.65832206e-01   8.42774566e-01]\n",
      " [  1.39110927e-01   1.62740107e+00   8.90406504e-01]\n",
      " [  1.28283582e+00   4.51823559e-01   1.08154506e+00]]\n",
      "[['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'False' 'True']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'True' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['True' 'False' 'False']\n",
      " ['False' 'False' 'True']\n",
      " ['True' 'False' 'False']]\n"
     ]
    }
   ],
   "source": [
    "fproba_mat,odd_mat,win_mat= formatMatrixs(withodds,p_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strategy1(fproba_mat,odd_mat,win_mat):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    print(exp)\n",
    "    maxi = np.argmax(exp,axis=1)\n",
    "    print(maxi)\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    for i in range(maxi.shape[0]):\n",
    "        if exp[i,maxi[i]] > 1:\n",
    "            expectation = expectation+ exp[i,maxi[i]]\n",
    "            spent = spent+1\n",
    "            if maxi[i] == y_true[i]:\n",
    "                income = income + odd_mat[i,maxi[i]]\n",
    "            \n",
    "            \n",
    "        \n",
    "    print(\"Spent:{}, Income:{}, expectation:{}\".format(spent,income,expectation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.03339552e+00   1.04675237e+00   8.94297838e-01]\n",
      " [  8.21849315e-01   9.04600812e-01   6.44336531e-01]\n",
      " [  8.96994536e-01   9.36690647e-01   8.16753927e-01]\n",
      " [  1.00178082e+00   8.27063599e-01   8.51716738e-01]\n",
      " [  6.71511628e-01   1.08776978e+00   1.44656652e+00]\n",
      " [  6.96132597e-01   8.14140731e-01   1.36569106e+00]\n",
      " [  1.71044776e+00   6.39047328e-01   6.70557940e-01]\n",
      " [  7.53767123e-01   9.56292287e-01   8.99754539e-01]\n",
      " [  7.98142077e-01   9.06474820e-01   9.73821990e-01]\n",
      " [  8.80813953e-01   7.88294993e-01   9.32832618e-01]\n",
      " [  9.55161290e-01   3.13147581e-01   4.83380637e-01]\n",
      " [  1.91640922e-01   5.21662380e-01   1.16852630e+00]\n",
      " [  1.21575342e+00   7.62449256e-01   5.33796820e-01]\n",
      " [  1.36740331e+00   1.23884892e+00   6.78901734e-01]\n",
      " [  8.65616438e-01   8.27063599e-01   1.12210300e+00]\n",
      " [  9.94475138e-01   9.51798561e-01   8.56820809e-01]\n",
      " [  8.57734807e-01   8.27063599e-01   9.13005780e-01]\n",
      " [  1.37505661e-01   1.86258842e-01   2.04885686e+00]\n",
      " [  7.98208955e-01   7.75562357e-01   2.19266117e+00]\n",
      " [  9.33606557e-01   1.59804258e+00   4.86921009e-01]\n",
      " [  1.58216418e+00   8.39986468e-01   5.26554066e-01]\n",
      " [  7.73219178e-01   9.69215156e-01   1.24377682e+00]\n",
      " [  5.49180328e-01   1.11798561e+00   1.51416309e+00]\n",
      " [  8.75342466e-01   8.27063599e-01   6.65827831e-01]\n",
      " [  9.69253731e-01   1.05967524e+00   1.22541906e+00]\n",
      " [  8.99657534e-01   8.39986468e-01   6.53730836e-01]\n",
      " [  3.80386740e-01   9.17523681e-01   3.36910569e+00]\n",
      " [  1.05523256e+00   9.66906475e-01   6.94892704e-01]\n",
      " [  6.22465753e-01   1.18890392e+00   1.66541582e+00]\n",
      " [  2.72512452e-01   4.53988700e-01   1.17727273e+00]\n",
      " [  5.89226519e-01   8.01217862e-01   1.26416185e+00]\n",
      " [  1.97704918e+00   9.82138024e-01   4.71204188e-01]\n",
      " [  7.01252885e-01   1.05755396e+00   9.92682927e-01]\n",
      " [  1.07615672e+00   5.64165755e-01   1.51416309e+00]\n",
      " [  9.88524590e-01   9.06474820e-01   7.63350785e-01]\n",
      " [  1.41027397e+00   3.57897199e-01   7.13089005e-01]\n",
      " [  9.34838710e-01   2.44513988e-01   6.28706343e-01]\n",
      " [  2.85179987e-01   2.87325945e-01   1.87727273e+00]\n",
      " [  9.70218579e-01   7.88294993e-01   7.69633508e-01]\n",
      " [  1.09753731e+00   6.86168744e-01   1.48712446e+00]\n",
      " [  9.48387097e-01   1.63693526e-01   2.56565912e-01]\n",
      " [  9.51912568e-01   7.54695535e-01   8.10471204e-01]\n",
      " [  8.36438356e-01   8.78755074e-01   1.13562232e+00]\n",
      " [  1.35410448e+00   3.63497608e-01   9.05793991e-01]\n",
      " [  5.20114092e-01   4.51777467e-01   1.04363636e+00]\n",
      " [  1.12604478e+00   9.82138024e-01   3.13354857e-01]\n",
      " [  3.94726338e-01   6.40660628e-01   1.01818182e+00]\n",
      " [  8.23770492e-01   1.39031401e+00   7.57081545e-01]\n",
      " [  6.85684932e-01   1.04675237e+00   1.25185250e+00]\n",
      " [  3.11647685e-01   4.88010579e-01   2.06818182e+00]\n",
      " [  6.46780822e-01   1.13721245e+00   2.00085837e+00]\n",
      " [  7.02950820e-01   9.66906475e-01   9.59871245e-01]\n",
      " [  7.45856354e-01   9.06474820e-01   7.00523560e-01]\n",
      " [  5.47616327e-01   2.29622126e-01   1.17727273e+00]\n",
      " [  8.37209302e-01   9.36690647e-01   6.95598212e-01]\n",
      " [  7.43224044e-01   9.21582734e-01   1.06806283e+00]\n",
      " [  9.76380597e-01   6.34155348e-01   1.21117437e+00]\n",
      " [  8.96994536e-01   9.66906475e-01   6.89484979e-01]\n",
      " [  6.93313953e-01   8.91677943e-01   1.39248927e+00]\n",
      " [  5.75581395e-01   1.32949640e+00   2.09549356e+00]\n",
      " [  1.03339552e+00   2.56110741e-01   1.51416309e+00]\n",
      " [  9.81104651e-01   7.88294993e-01   7.89527897e-01]\n",
      " [  9.02616279e-01   5.80904518e-01   1.02094241e+00]\n",
      " [  4.20103878e-01   6.18247151e-01   1.14545455e+00]\n",
      " [  1.10466418e+00   1.00798376e+00   7.59614448e-01]\n",
      " [  6.14088398e-01   3.70572278e-01   1.62272727e+00]\n",
      " [  2.45354256e-01   1.08552097e+00   8.90909091e-01]\n",
      " [  7.58132958e-02   5.00004508e-01   1.44688863e+00]\n",
      " [  7.36918605e-01   6.32403348e-01   1.41361257e+00]\n",
      " [  7.87808219e-01   9.04600812e-01   9.16679677e-01]\n",
      " [  1.08328358e+00   6.37020170e-01   8.60793816e-01]\n",
      " [  8.53060109e-01   7.88294993e-01   8.79581152e-01]\n",
      " [  1.12063953e+00   7.36603518e-01   8.41884817e-01]\n",
      " [  7.06395349e-01   9.17523681e-01   1.25729614e+00]\n",
      " [  9.09383562e-01   5.05342014e-01   1.13089005e+00]\n",
      " [  1.21823204e+00   3.89631589e-01   9.73636364e-01]\n",
      " [  8.85174419e-01   7.62449256e-01   1.11518325e+00]\n",
      " [  1.61287121e-01   1.27670658e-01   3.06937829e+00]\n",
      " [  7.60405710e-01   8.52909337e-01   1.08292683e+00]\n",
      " [  8.31575342e-01   8.27063599e-01   1.24377682e+00]\n",
      " [  6.73661202e-01   1.57670450e+00   1.02746781e+00]\n",
      " [  8.45303867e-01   5.70589891e-01   1.19121951e+00]\n",
      " [  7.08563536e-01   1.41272921e+00   6.08369099e-01]\n",
      " [  3.34407225e-01   1.70552871e+00   8.11158798e-01]\n",
      " [  1.99383562e+00   8.78755074e-01   4.70472103e-01]\n",
      " [  9.15697674e-01   8.14140731e-01   8.38197425e-01]\n",
      " [  5.45058140e-01   1.25351827e+00   2.50107296e+00]\n",
      " [  8.71366120e-01   8.27063599e-01   8.23036649e-01]\n",
      " [  5.71147541e-01   1.04244604e+00   1.48712446e+00]\n",
      " [  7.92671233e-01   9.43369418e-01   7.90387785e-01]\n",
      " [  7.76162791e-01   1.01223022e+00   1.06802575e+00]\n",
      " [  9.19365672e-01   2.29741203e-01   1.43122288e+00]\n",
      " [  5.00604312e-01   4.36724567e-01   1.27272727e+00]\n",
      " [  8.75342466e-01   9.82014388e-01   3.37894499e-01]\n",
      " [  9.28064516e-01   9.09550488e-02   2.22816122e-01]\n",
      " [  4.64917127e-01   1.39262190e+00   1.01394850e+00]\n",
      " [  1.93850746e+00   9.82014388e-01   2.00374419e-01]\n",
      " [  5.96775956e-01   1.57006081e+00   8.67056707e-01]\n",
      " [  7.82945205e-01   5.57346869e-01   1.21673820e+00]\n",
      " [  5.71220930e-01   1.11136671e+00   2.27124464e+00]\n",
      " [  4.70169378e-01   4.34346958e-01   1.05000000e+00]\n",
      " [  8.19767442e-01   8.39986468e-01   9.86909871e-01]\n",
      " [  7.45856354e-01   5.67389138e-01   1.32357724e+00]\n",
      " [  1.07703488e+00   1.67862546e+00   2.65069789e-02]\n",
      " [  1.33633880e+00   2.32479954e-01   1.14308943e+00]\n",
      " [  8.16448087e-01   9.21582734e-01   9.26701571e-01]\n",
      " [  5.22346156e-01   3.30033470e-01   1.25917685e+00]\n",
      " [  9.51912568e-01   8.76258993e-01   8.16753927e-01]\n",
      " [  1.63517442e+00   8.27063599e-01   5.87434555e-01]\n",
      " [  6.85684932e-01   4.13146099e-01   1.85214592e+00]\n",
      " [  8.33843284e-01   1.13053545e+00   1.96443340e+00]\n",
      " [  1.21935484e+00   2.90106805e-01   1.78560455e-01]\n",
      " [  1.09836066e+00   8.01217862e-01   6.84816754e-01]\n",
      " [  8.38882059e-01   1.08776978e+00   7.25722543e-01]\n",
      " [  1.01991143e-01   1.24971159e-01   1.59812311e+00]\n",
      " [  1.09011628e+00   5.68221164e-01   8.10471204e-01]\n",
      " [  9.76380597e-01   1.11136671e+00   3.60905512e-01]\n",
      " [  8.60753425e-01   8.52909337e-01   6.56655521e-01]\n",
      " [  8.72093023e-01   5.43427213e-01   1.09947644e+00]\n",
      " [  1.38006874e-02   1.03258871e-02   1.92609039e+00]\n",
      " [  8.85174419e-01   9.21582734e-01   9.19313305e-01]\n",
      " [  1.46075581e+00   7.88294993e-01   5.54291845e-01]\n",
      " [  7.02950820e-01   9.36690647e-01   1.00042918e+00]\n",
      " [  2.64921875e-01   3.27873094e+00   3.53095103e-01]\n",
      " [  1.24720149e+00   1.08776978e+00   9.91997821e-02]\n",
      " [  5.31850800e-01   9.95511957e-02   1.32363636e+00]\n",
      " [  9.81104651e-01   9.51798561e-01   7.65193133e-01]\n",
      " [  4.88372093e-01   1.70581867e+00   3.78540773e+00]\n",
      " [  8.78688525e-01   7.62449256e-01   8.73298429e-01]\n",
      " [  1.24480874e+00   8.52909337e-01   6.06282723e-01]\n",
      " [  8.08011050e-01   7.88294993e-01   1.26341463e+00]\n",
      " [  8.55890411e-01   9.51798561e-01   6.33354516e-01]\n",
      " [  9.55000000e-01   6.93945902e-01   1.33838864e+00]\n",
      " [  9.59302326e-01   8.52909337e-01   8.79581152e-01]\n",
      " [  8.15406977e-01   8.27063599e-01   1.17801047e+00]\n",
      " [  3.36839276e-01   6.08010713e-01   1.15818182e+00]\n",
      " [  5.71823204e-01   9.06474820e-01   9.04712042e-01]\n",
      " [  2.62886292e-01   3.29362514e-01   1.65454545e+00]\n",
      " [  8.19767442e-01   8.39986468e-01   1.14659686e+00]\n",
      " [  9.14516129e-01   4.12867550e-02   1.98349910e-01]\n",
      " [  1.36835821e+00   8.91677943e-01   1.43820427e-01]\n",
      " [  5.88424658e-01   2.09989834e+00   4.77747994e-01]\n",
      " [  8.67704918e-01   9.21582734e-01   8.57591623e-01]\n",
      " [  1.07273224e+00   2.40450630e-01   1.29349593e+00]\n",
      " [  5.62500000e-01   1.13721245e+00   2.40643777e+00]\n",
      " [  9.19365672e-01   7.58491150e-01   1.12789330e+00]\n",
      " [  3.72440835e-02   9.47863490e-03   1.44707633e+00]\n",
      " [  1.74608209e+00   9.21582734e-01   1.20295787e-01]\n",
      " [  1.83761191e-02   2.64767806e+00   3.12102716e-01]\n",
      " [  6.76243094e-01   1.39656712e+00   7.06806283e-01]\n",
      " [  5.88424658e-01   9.80976135e-01   2.70386266e+00]\n",
      " [  1.07703488e+00   5.20977542e-01   1.22670520e+00]\n",
      " [  1.15327869e+00   5.82327513e-01   9.73872832e-01]\n",
      " [  9.14246575e-01   6.15072458e-01   9.46351931e-01]\n",
      " [  9.01644878e-02   1.67874487e-01   1.39238843e+00]\n",
      " [  3.21011487e-01   6.61458778e-01   1.08181818e+00]\n",
      " [  8.50290698e-01   8.39986468e-01   9.19313305e-01]\n",
      " [  4.17907541e-01   2.69245367e+00   1.85551586e-02]\n",
      " [  2.37645349e+00   6.88521758e-01   6.92947977e-01]\n",
      " [  5.14534884e-01   1.69208633e+00   2.11249776e+00]\n",
      " [  8.94193548e-01   9.10416106e-02   1.02643023e+00]\n",
      " [  9.19365672e-01   1.42014388e+00   3.90208983e-01]\n",
      " [  5.19890710e-01   1.19352518e+00   1.99476440e+00]\n",
      " [  8.28488372e-01   8.39986468e-01   9.73390558e-01]\n",
      " [  8.67704918e-01   8.39986468e-01   8.16753927e-01]\n",
      " [  1.46094763e-01   2.29402950e-01   1.35498400e+00]\n",
      " [  6.56506849e-01   7.75505200e-01   1.96030043e+00]\n",
      " [  3.12508962e-01   1.93470579e+00   6.21888412e-01]\n",
      " [  9.95806452e-01   2.20081717e-01   2.70304743e-01]\n",
      " [  1.96451613e+00   2.18196591e-01   2.31163478e-01]\n",
      " [  2.43119490e-01   7.07718101e-01   2.86363636e+00]\n",
      " [  6.33977901e-01   1.71763033e+00   4.54450054e-01]\n",
      " [  1.61620302e-01   7.62449256e-01   1.64181818e+00]\n",
      " [  5.34535519e-01   4.99109919e-01   2.76242775e+00]\n",
      " [  1.01914179e+00   3.03913258e-01   1.62231760e+00]\n",
      " [  8.68865753e-02   2.13363158e-01   1.31686422e+00]\n",
      " [  2.67957864e-01   2.10250926e-01   1.93464269e+00]\n",
      " [  9.22529302e-02   2.10837483e-01   1.61495983e+00]\n",
      " [  1.24645161e+00   1.98217381e-01   1.22647891e-01]\n",
      " [  1.11179104e+00   2.80117504e-01   1.52356021e+00]\n",
      " [  6.36627907e-01   9.69215156e-01   1.94764398e+00]\n",
      " [  1.00258065e+00   2.92420697e-01   5.71834257e-01]\n",
      " [  8.87419355e-01   2.15686502e-01   4.45616917e-02]\n",
      " [  1.24692010e+00   3.63485859e-01   8.52727273e-01]\n",
      " [  7.48904110e-01   6.67449895e-01   1.29785408e+00]\n",
      " [  5.71147541e-01   9.56292287e-01   1.57068063e+00]\n",
      " [  4.50327869e-01   1.22767253e+00   5.15028902e+00]\n",
      " [  1.16712329e+00   1.45483962e+00   4.79532979e-02]\n",
      " [  1.20580110e+00   1.69298437e+00   4.29914163e-01]\n",
      " [  1.33633880e+00   8.39986468e-01   8.80231214e-01]\n",
      " [  1.21156716e+00   3.32882882e-01   5.79300523e-01]\n",
      " [  9.02616279e-01   9.97122302e-01   8.24678112e-01]\n",
      " [  1.01598837e+00   9.66906475e-01   7.30042918e-01]\n",
      " [  4.83278689e-01   1.12428958e+00   2.51308901e+00]\n",
      " [  6.32267442e-01   1.19352518e+00   1.59527897e+00]\n",
      " [  9.34838710e-01   4.73811065e-01   1.60080189e-01]\n",
      " [  9.07458564e-01   1.40130129e+00   5.90575916e-01]\n",
      " [  1.55806452e+00   2.49231873e-04   8.58008229e-04]\n",
      " [  1.60478805e-02   3.16636171e-02   1.55143930e+00]\n",
      " [  5.16695956e-02   3.84867281e-02   2.18171324e+00]\n",
      " [  8.48097015e-01   5.69696929e-01   2.83905579e+00]\n",
      " [  6.95628415e-01   9.82014388e-01   9.73390558e-01]\n",
      " [  1.04068493e+00   9.66906475e-01   2.53780581e-01]\n",
      " [  1.32559701e+00   9.97122302e-01   1.78083093e-01]\n",
      " [  1.30813953e+00   8.39986468e-01   5.73218884e-01]\n",
      " [  8.96994536e-01   9.51798561e-01   6.97596567e-01]\n",
      " [  4.06700091e-01   4.02966645e-01   1.60363636e+00]\n",
      " [  6.83701657e-01   5.09227797e-01   1.52727273e+00]\n",
      " [  6.77322404e-01   9.82014388e-01   1.02746781e+00]\n",
      " [  4.89779006e-01   9.36690647e-01   1.11518325e+00]\n",
      " [  1.35464481e+00   5.12579687e-01   8.42774566e-01]\n",
      " [  1.43962687e+00   1.48283965e-01   8.38197425e-01]\n",
      " [  9.55000000e-01   3.23276963e-01   9.60548416e-01]\n",
      " [  4.62430939e-01   2.23629055e+00   2.62862159e-01]\n",
      " [  1.59193548e+00   4.22450248e-02   2.69058820e-01]\n",
      " [  5.78794826e-03   1.19922328e-02   1.45414041e+00]\n",
      " [  1.58216418e+00   2.27860844e-01   7.62489270e-01]\n",
      " [  1.11774194e+00   7.08793098e-01   7.46402484e-03]\n",
      " [  8.46774194e-01   4.15042508e-03   9.52196633e-02]\n",
      " [  1.28633721e+00   5.80225754e-01   6.81675393e-01]\n",
      " [  8.89931507e-01   1.98124890e-01   1.14659686e+00]\n",
      " [  8.69477612e-01   1.34397835e+00   1.05636127e+00]\n",
      " [  6.88306011e-01   8.01217862e-01   1.20942408e+00]\n",
      " [  3.38475084e-02   2.72890878e-01   2.22618645e+00]\n",
      " [  9.07978142e-01   9.82014388e-01   7.79057592e-01]\n",
      " [  6.51693989e-01   1.02733813e+00   1.22513089e+00]\n",
      " [  7.85806452e-01   5.16097235e-03   8.99702269e-02]\n",
      " [  6.07759563e-01   8.78755074e-01   2.15375723e+00]\n",
      " [  7.23837209e-01   1.45023282e+00   4.35141271e-01]\n",
      " [  4.84806630e-01   1.74854800e+00   5.79523471e-01]\n",
      " [  5.64364641e-01   1.55447647e+00   7.84120172e-01]\n",
      " [  1.28633721e+00   1.58360463e+00   2.37734147e-01]\n",
      " [  1.51089552e+00   4.20896562e-01   4.22614129e-01]\n",
      " [  7.40883978e-01   9.36690647e-01   6.91099476e-01]\n",
      " [  3.29293566e-01   4.52305037e-01   1.84545455e+00]\n",
      " [  1.51939891e+00   7.96070185e-02   1.05636364e+00]\n",
      " [  7.50546448e-01   9.97122302e-01   9.73821990e-01]\n",
      " [  1.77500000e+00   1.46931089e-01   5.68586387e-01]\n",
      " [  1.05523256e+00   9.51798561e-01   4.56004996e-01]\n",
      " [  7.65483871e-01   1.72685177e-01   1.96131371e+00]\n",
      " [  2.22383721e+00   1.82276691e-01   9.14471545e-01]\n",
      " [  5.05245902e-01   1.23884892e+00   2.16753927e+00]\n",
      " [  1.31847015e+00   8.39986468e-01   2.17787395e-01]\n",
      " [  1.09041045e+00   9.56292287e-01   4.14368177e-01]\n",
      " [  1.09836066e+00   9.21582734e-01   6.00257511e-01]\n",
      " [  7.71802326e-01   8.91677943e-01   1.05450644e+00]\n",
      " [  1.62492537e+00   9.97122302e-01   6.86004929e-02]\n",
      " [  7.61530055e-01   9.21582734e-01   1.03664921e+00]\n",
      " [  5.78469945e-01   1.08776978e+00   1.53926702e+00]\n",
      " [  1.80405645e-03   1.83662510e+00   1.47485549e+00]\n",
      " [  8.26716418e-01   8.23329985e-01   3.24463519e+00]\n",
      " [  2.48495970e-02   6.60106134e-04   2.78893182e+00]\n",
      " [  1.65343284e+00   6.15739717e-01   3.40222833e-02]\n",
      " [  1.46100746e+00   2.93872774e-01   9.89528796e-01]\n",
      " [  3.65085643e-01   1.43844364e-01   1.51902822e+00]\n",
      " [  1.16516129e+00   2.90539179e-03   7.48058494e-02]\n",
      " [  4.96400194e-03   5.28669753e-01   1.83152432e+00]\n",
      " [  7.99354839e-01   2.04358448e-04   5.42034303e-03]\n",
      " [  1.40956284e+00   8.65832206e-01   8.42774566e-01]\n",
      " [  1.39110927e-01   1.62740107e+00   8.90406504e-01]\n",
      " [  1.28283582e+00   4.51823559e-01   1.08154506e+00]]\n",
      "[1 1 1 0 2 2 0 1 2 2 0 2 0 0 2 0 2 2 2 1 0 2 2 0 2 0 2 0 2 2 2 0 1 2 0 0 0\n",
      " 2 0 2 0 0 2 0 2 0 2 1 2 2 2 1 1 2 1 2 2 1 2 2 2 0 2 2 0 2 1 2 2 2 0 2 0 2\n",
      " 2 0 2 2 2 2 1 2 1 1 0 0 2 0 2 1 2 2 2 1 0 1 0 1 2 2 2 2 2 1 0 2 2 0 0 2 2\n",
      " 0 0 1 2 0 1 0 2 2 1 0 2 1 0 2 0 2 0 0 2 1 2 0 2 2 1 2 2 0 0 1 1 2 2 2 2 0\n",
      " 1 1 2 2 0 2 2 2 2 1 0 2 2 1 2 2 0 2 2 1 0 0 2 1 2 2 2 2 2 2 0 2 2 0 0 0 2\n",
      " 2 2 1 1 0 0 1 0 2 2 0 1 0 2 2 2 1 0 0 0 1 2 2 2 2 0 0 2 1 0 2 0 0 0 0 2 1\n",
      " 2 2 1 2 0 2 1 1 1 1 0 1 2 0 1 0 0 2 0 2 0 0 0 2 0 2 2 1 2 2 0 0 2 0 2 0 0\n",
      " 1 0]\n",
      "Spent:205, Income:258.19, expectation:322.188139700667\n"
     ]
    }
   ],
   "source": [
    "strategy1(fproba_mat,odd_mat,win_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy3(fproba_mat,odd_mat,win_mat,info, z = 0.5  ):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    total = fproba_mat.shape[0]\n",
    "    withdraw = 0\n",
    "    buy = 0\n",
    "    homeTeam = info[\"HomeTeam\"].values\n",
    "    awayTeam = info[\"AwayTeam\"].values\n",
    "    receipt = []\n",
    "    def chooseFrom(match, chosable):\n",
    "        max_prob = -1\n",
    "        c = 0\n",
    "        for choice in chosable:\n",
    "            if fproba_mat[match,choice] > max_prob:\n",
    "                c = choice\n",
    "                max_prob = fproba_mat[match,choice]\n",
    "        return c\n",
    "    for match in range(exp.shape[0]):\n",
    "        chosable = []\n",
    "        for choose in range(3):\n",
    "            if exp[match,choose]> 1 and fproba_mat[match,choose] >= z:\n",
    "                chosable.append(choose)\n",
    "                \n",
    "        if len(chosable) == 0:\n",
    "            withdraw= withdraw+1\n",
    "            continue\n",
    "        choice = chooseFrom(match,chosable)\n",
    "        expectation = expectation+ exp[match,choice]\n",
    "        spent = spent+1\n",
    "        buy =buy +1 \n",
    "        receipt.append(np.hstack([homeTeam[match],awayTeam[match], odd_mat[match,choice],choice,y_true[match],fproba_mat[match,:]]))\n",
    "        if choice == y_true[match]:\n",
    "            income = income + odd_mat[match,choice]\n",
    "    print(buy)\n",
    "    print(\"Spent:{}, Income:{}, expectation:{}, withdraw:{}(total:{})\".\n",
    "          format(spent,income,expectation,withdraw,total))\n",
    "    receiptDf = pd.DataFrame(receipt,columns=[\"home\",\"away\",\"odd of choice\",\"choice\",\"result\",\"Hp\",\"Dp\",\"Ap\"])\n",
    "    return spent,income,expectation,withdraw,total,receiptDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "Spent:90, Income:91.14999999999998, expectation:139.70355912289673, withdraw:171(total:261)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy3(fproba_mat,odd_mat,win_mat,withodds,z=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "receipt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_range = range(30,80,2)\n",
    "spents = []\n",
    "incomes = []\n",
    "exps = []\n",
    "withs = []\n",
    "for z in z_range:\n",
    "    fz = z/100\n",
    "    print(z)\n",
    "    spent,income,expectation,withdraw,total,_ = strategy3(fproba_mat,odd_mat,win_mat,withodds,z=fz) \n",
    "    spents.append(spent)\n",
    "    incomes.append(income)\n",
    "    exps.append(expectation)\n",
    "    withs.append(withdraw)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(z_range, spents)\n",
    "plt.plot(z_range, incomes,color='green')\n",
    "plt.plot(z_range, np.array(incomes)/np.array(spents)*100,color='red')\n",
    "plt.plot(z_range,exps,color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy4(fproba_mat,odd_mat,win_mat,info, z = 0.2 ,e =1.8 ):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    total = fproba_mat.shape[0]\n",
    "    withdraw = 0\n",
    "    buy = 0\n",
    "    homeTeam = info[\"HomeTeam\"].values\n",
    "    awayTeam = info[\"AwayTeam\"].values\n",
    "    receipt = []\n",
    "    def chooseFrom(match, chosable):\n",
    "        max_prob = -1\n",
    "        c = 0\n",
    "        for choice in chosable:\n",
    "            if fproba_mat[match,choice] > max_prob:\n",
    "                c = choice\n",
    "                max_prob = fproba_mat[match,choice]\n",
    "        return c\n",
    "    for match in range(exp.shape[0]):\n",
    "        chosable = []\n",
    "        for choose in range(3):\n",
    "            if exp[match,choose]> e and fproba_mat[match,choose] >= z:\n",
    "                chosable.append(choose)\n",
    "                \n",
    "        if len(chosable) == 0:\n",
    "            withdraw= withdraw+1\n",
    "            continue\n",
    "        choice = chooseFrom(match,chosable)\n",
    "        expectation = expectation+ exp[match,choice]\n",
    "        spent = spent+1\n",
    "        buy =buy +1 \n",
    "        receipt.append(np.hstack([homeTeam[match],awayTeam[match], odd_mat[match,choice],choice,y_true[match],fproba_mat[match,:]]))\n",
    "        if choice == y_true[match]:\n",
    "            income = income + odd_mat[match,choice]\n",
    "    print(buy)\n",
    "    print(\"Spent:{}, Income:{}, expectation:{}, withdraw:{}(total:{})\".\n",
    "          format(spent,income,expectation,withdraw,total))\n",
    "    receiptDf = pd.DataFrame(receipt,columns=[\"home\",\"away\",\"odd of choice\",\"choice\",\"result\",\"Hp\",\"Dp\",\"Ap\"])\n",
    "    return spent,income,expectation,withdraw,total,receiptDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "Spent:85, Income:110.48000000000002, expectation:171.54410507473867, withdraw:176(total:261)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,win_mat,withodds,z=0.2,e=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
