{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    from IPython.core.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "import sys\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotCurve(train_mean, train_std,test_mean,test_std,sizes):\n",
    "    plt.plot(sizes, train_mean, \n",
    "            color='blue', marker='o', \n",
    "            markersize=5, \n",
    "            label='training accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                  train_mean + train_std,\n",
    "                   train_mean - train_std, alpha=0.15, color='blue')\n",
    "\n",
    "    plt.plot(sizes, test_mean, \n",
    "              color='green', linestyle='--', \n",
    "              marker='s', markersize=5, \n",
    "             label='validation accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                      test_mean + test_std,\n",
    "                     test_mean - test_std, \n",
    "                    alpha=0.15, color='green')\n",
    "    plt.xlabel('x_range')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "def likehoodScore(proba,y):\n",
    "    return np.sum(proba * y)/proba.shape[0]\n",
    "\n",
    "def firstNScore(n, pred, y):\n",
    "    backup = np.array(pred, copy =True)\n",
    "    for r in range(pred.shape[0]):\n",
    "        row = backup[r]\n",
    "        s = np.sort(row)\n",
    "        for c in range(pred.shape[1]):\n",
    "            temp = backup[r][c]\n",
    "            backup[r][c] = False\n",
    "            for j in range(1,n+1):\n",
    "                if temp == s[-j]:\n",
    "                    backup[r][c] = True\n",
    "                    break\n",
    "    res = np.sum(np.logical_and(backup,y))/pred.shape[0]\n",
    "    return res               \n",
    "\n",
    "def oneHotDecode(self, X_sample):\n",
    "    result=None\n",
    "    fiPos = 0\n",
    "    colIndex = 0\n",
    "    while colIndex < X_sample.shape[1]:\n",
    "        if fiPos < len(self.ohe.n_values_) and colIndex == self.ohe.feature_indices_[fiPos]:                \n",
    "            start = self.ohe.feature_indices_[fiPos]\n",
    "            end_ = start+ self.ohe.n_values_[fiPos]\n",
    "            #print(\"start{} end{}\".format(start,end_))\n",
    "            classes = np.argmax(X_sample[:,start:end_],axis=1).reshape(X_sample.shape[0],1)\n",
    "            if result is None:\n",
    "                result = classes\n",
    "            else:\n",
    "                result=np.hstack([result,classes])\n",
    "            colIndex = end_\n",
    "            fiPos = fiPos +1\n",
    "        else:\n",
    "            if result is None:\n",
    "                result = X_sample[:,colIndex:colIndex+1]\n",
    "            else:\n",
    "                result=np.hstack([result, X_sample[:,colIndex:colIndex+1]])\n",
    "            colIndex = colIndex +1\n",
    "        \n",
    "    return result \n",
    "def convertToDate(dayStamps):\n",
    "    res = [] \n",
    "    for v in dayStamps:\n",
    "        res.append(datetime.datetime.fromtimestamp(float(v)*24*60*60))\n",
    "    return res\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def precisionMatrix(proba, y):\n",
    "    def _precisionClassify(df,proba, wins, c =0 ):\n",
    "        for indx, v in enumerate(proba):\n",
    "            row = 0\n",
    "            col = 0\n",
    "            if wins[indx] == c:\n",
    "                col = 0\n",
    "            else:\n",
    "                col =1\n",
    "            if v <0.2:\n",
    "                row =6 \n",
    "            elif v < 0.3 and  v >=0.2:\n",
    "                row =5 \n",
    "            elif v < 0.4 and v >= 0.3:\n",
    "                row = 4 \n",
    "            elif v < 0.5 and v >= 0.4:\n",
    "                row = 3 \n",
    "            elif v < 0.6 and v >= 0.5:\n",
    "                row = 2 \n",
    "            elif v < 0.8 and v >= 0.6:\n",
    "                row = 1\n",
    "            df.iloc[row,col] = df.iloc[row,col]+1 \n",
    "        df[df.columns[2]] = df[df.columns[0]] /(df[df.columns[1]] + df[df.columns[0]])\n",
    "        return df\n",
    "    rowHeader = ['>80','60-80','50-60','40-50','30-40','20-30','<20']\n",
    "    df = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['h_Correct', 'h_Wrong','h_Precent'])\n",
    "    hproba = proba[:,0]\n",
    "    wins = np.argmax(y,axis=1)\n",
    "    df = _precisionClassify(df,hproba,wins)\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['d_Correct', 'd_Wrong','d_Precent'])\n",
    "    dproba = proba[:,1]\n",
    "    df = df.join(_precisionClassify(temp,dproba,wins,c=1))\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['a_Correct', 'a_Wrong','a_Precent'])\n",
    "    aproba = proba[:,2]\n",
    "    df = df.join(_precisionClassify(temp,aproba,wins,c=2))\n",
    "    \n",
    "    bound = pd.DataFrame(np.array([[0.8,1.0],[0.6,0.8],[0.5,0.6],[0.4,0.5],[0.3,0.4],[0.2,0.3],[0,0.2]] )\n",
    "                                ,index=rowHeader, columns=['[lower', 'upper)'])\n",
    "            \n",
    "    return bound.join(df)\n",
    "       \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "def futureTest(mlp, X,y,numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(X[:,c.dateColumn])\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            mlp.fit(X_train,y_train)\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = mlp.predict_proba(X_test)\n",
    "            train_proba =mlp.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "            if verbose == True:\n",
    "                print(\"week{}\".format(w))\n",
    "                print(\"numOftest {} , score {}\".format(X_test.shape[0],mlp.score(X_test,y_test)))\n",
    "                print(np.hstack([stack[errorIndx],proba[errorIndx],y_test[errorIndx]]))\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "        \n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    return firstNScore(1,sum_train_proba,sum_train_y), score, like2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "class FootballDataHelper:\n",
    "    def __init__ (self, recentNum=5):\n",
    "        self.win_mapping = {'H':0, 'D':1,'A':2}\n",
    "        self.recentNum = recentNum\n",
    "        self.df = None\n",
    "        self.teamsData={}\n",
    "        self.session = 0\n",
    "        #self.hiddensCount = 2\n",
    "       \n",
    "    def addColumns(self,df, addition):    \n",
    "        dates = df[\"Date\"].drop_duplicates().values\n",
    "        col_adds = []\n",
    "        for colAdd in addition.columns:\n",
    "             if colAdd not in df.columns:\n",
    "                    df[colAdd]=np.zeros(shape=(df.shape[0],))\n",
    "                    col_adds.append(colAdd)\n",
    "        for date in dates:\n",
    "            dateAddition= addition[addition['Date'] == date].sort(columns='HomeTeam')\n",
    "            dateDf  = df [df['Date']==date].sort(columns='HomeTeam')\n",
    "            for col in col_adds:\n",
    "                dateDf[col] = dateAddition[col].values\n",
    "            df.update(dateDf)\n",
    "        return df\n",
    "            \n",
    "    def saveDf(self,filename):\n",
    "        self.df.to_csv(filename,index=False)\n",
    "    def loadDf(self,filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])    \n",
    "        self.df = df\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "    def readFootBallData(self,year): \n",
    "        filename = \"dataSet/E{}.csv\".format(year)\n",
    "        df = pd.read_csv(filename)\n",
    "        #df = df.drop(df.columns[range(23,df.shape[1])], axis=1)\n",
    "        #df = df.drop(\"Div\",axis=1)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "        df['session'] = pd.Series(np.ones(shape=(df.shape[0],))*self.session, index=df.index)\n",
    "        self.session = self.session +1\n",
    "        \n",
    "        matchDetail = pd.read_csv(\"dataSet/match{}.csv\".format(year))\n",
    "        matchDetail['Date'] =pd.to_datetime(matchDetail['Date'])\n",
    "        df = self.addColumns(df,matchDetail)\n",
    "        \n",
    "        df[\"Future\"] = np.zeros(shape=(df.shape[0],))\n",
    "        \n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = pd.concat([self.df,df])\n",
    "            \n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "    def readFuture(self):\n",
    "        filename = \"dataSet/future.csv\"\n",
    "        df = pd.read_csv(filename)\n",
    "        #df = df.drop(df.columns[range(23,df.shape[1])], axis=1)\n",
    "        #df = df.drop(\"Div\",axis=1)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df[\"Future\"] = np.ones(shape=(df.shape[0],))\n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = pd.concat([self.df,df])\n",
    "            \n",
    "        self.df['HTR']=self.df['HTR'].fillna('D')\n",
    "        self.df = self.df.fillna(0)\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "        \n",
    "    def getTeam(self,dataFrame, teamName):       \n",
    "        return dataFrame[(dataFrame[\"HomeTeam\"] == teamName) | (dataFrame[\"AwayTeam\"] == teamName)]\n",
    " \n",
    "        \n",
    "    def previousRecords(self,team, date , recentNum):\n",
    "        prev = team[( team[\"Date\"] < date)]\n",
    "        \n",
    "        if prev.shape[0] < recentNum :\n",
    "            #print(\"less than min Num\")\n",
    "            return None\n",
    "        else:\n",
    "            return prev.iloc[-recentNum:]\n",
    "    def readPredict (self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "  \n",
    "    def inverseTeamMapping (self, col):\n",
    "        inverseMap ={}\n",
    "        for name in self.teamsMap.keys():        \n",
    "            inverseMap[self.teamsMap[name]] = name\n",
    "        res =[]\n",
    "        for idex, v in enumerate(col):\n",
    "            res.append(inverseMap[v])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def readTeamMatch(self, teamName):\n",
    "        df = pd.read_csv('teams/'+teamName+'.csv')\n",
    "        df['1'] = pd.to_datetime(df['1'],yearfirst=True)\n",
    "        #df['1']= (pd.to_numeric(df['1'])/1e9/24/60/60)\n",
    "        self.teamsData[teamName]=df.sort(['1'],ascending=[False])\n",
    "        self.teamsById[self.teamsMap[teamName]]=self.teamsData[teamName]\n",
    "    \n",
    "    def commonMapping(self, X):\n",
    "        X['HomeTeam'] = X['HomeTeam'].map(self.teamsMap)\n",
    "        X['AwayTeam'] = X['AwayTeam'].map(self.teamsMap)\n",
    "        X['Referee']=X['Referee'].map(self.refereesMap).fillna(0)\n",
    "        X['HTR'] = X['HTR'].map(self.win_mapping)\n",
    "        X['FTR'] = X['FTR'].map(self.win_mapping)\n",
    "        return X\n",
    "    def initData(self, X, target,encode):\n",
    "        X  = X.sort_values(by=\"Date\")\n",
    "        isInput = False\n",
    "        if target is None:\n",
    "            target =X      \n",
    "        else:\n",
    "            if self.ohe is None:\n",
    "                raise Exception(\"Not yet get train data\")\n",
    "            isInput = True\n",
    "            if encode == True:\n",
    "                target = self.commonMapping(target)\n",
    "        y=None\n",
    "        if encode == True:    \n",
    "            X =self.commonMapping(X)\n",
    "            y = []\n",
    "            for v in target['FTR']:\n",
    "                y.append(range(3)==v)\n",
    "        else:\n",
    "            y = target['FTR'].values\n",
    "        target_date = (pd.to_numeric(target['Date'])/1e9/24/60/60).values\n",
    "        return isInput, X,y, target, target_date\n",
    "   \n",
    "    def aggregate(self,recents,nonExpand,isInput,encode):\n",
    "        res =None\n",
    "        if encode == True:\n",
    "            if isInput==False:\n",
    "                self.ohe = OneHotEncoder(categorical_features='all')\n",
    "                self.ohe.fit(recents)\n",
    "            res = self.ohe.transform(recents).toarray()\n",
    "        else:\n",
    "            res = np.array(recents)\n",
    "        self.dateColumn = res.shape[1]\n",
    "        res = np.hstack([res,nonExpand])\n",
    "        return res\n",
    "  \n",
    "    def getH7(self,removeInsufficient=False, target=None,encode = True,future =0):\n",
    "        #Simple recent win,draw, lose \n",
    "        df = self.df\n",
    "        if removeInsufficient == True:\n",
    "            df= df[df['Sufficient'] == 1]\n",
    "        df=df[df['Future']==future]\n",
    "        \n",
    "        isInput, X, y,target, target_date = self.initData(df,target,encode)\n",
    "        resy=[]\n",
    "        resx=[]\n",
    "        print(\"start format\")\n",
    "        recents = X[['HomeTeam','AwayTeam','Referee']].values\n",
    "        haccp = X['HAccP'].values.reshape(X.shape[0],1)\n",
    "        aaccp = X['AAccP'].values.reshape(X.shape[0],1)\n",
    "        homeRecent = np.hstack([X[['HWin','HDraw','HLose']].values,\n",
    "                                (X['HScore'].values - X['HConcede'].values).reshape(X.shape[0],1)])\n",
    "        awayRecent = np.hstack([X[['AWin','ADraw','ALose']].values,\n",
    "                                (X['AScore'].values - X['AConcede'].values).reshape(X.shape[0],1)])\n",
    "        homeMoral = X['HMoral'].values.reshape(X.shape[0],1)\n",
    "        awayMoral = X['AMoral'].values.reshape(X.shape[0],1)\n",
    "        target_date = target_date.reshape(X.shape[0],1)\n",
    "        nonExpand =np.hstack([target_date,X[['HRestDay','ARestDay','HS_Acc','AS_Acc','HST_Acc','AST_Acc',\n",
    "                                            'H_Poss_Acc','A_Poss_Acc','H_atkPass_tot_Acc','A_atkPass_tot_Acc',\n",
    "                                             'H_tkPass_OK_Acc','A_atkPass_Ok_Acc','H_ins_Acc','A_ins_Acc'\n",
    "                                            ]].values,haccp-aaccp,(haccp+1)/(aaccp+1),\n",
    "                                homeRecent,awayRecent, homeMoral - awayMoral + haccp - aaccp])\n",
    "        res = self.aggregate(recents,nonExpand,future,encode)\n",
    "        print(\"finish\")\n",
    "        sys.stdout.flush()\n",
    "        return res, np.array(y)\n",
    "    def _getRank(self,x, X,teamName,recentNum):\n",
    "        team = self.getTeam(X,teamName)\n",
    "        prev = team[team['Date'] < x['Date']].values      \n",
    "        for i in range(recentNum):\n",
    "            pass\n",
    "    def initRanking(self, n = 20):\n",
    "        defaultPt = 1\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HPoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"APoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        hpoints= df[\"HPoints\"].values\n",
    "        apoints=df[\"APoints\"].values\n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hpoints[i] = 3\n",
    "                apoints[i] = 0\n",
    "            elif ftr[i] == 'D':\n",
    "                hpoints[i] = 1\n",
    "                apoints[i] = 1\n",
    "            else :\n",
    "                hpoints[i] = 0\n",
    "                apoints[i] = 3\n",
    "        df[\"HPoints\"]=hpoints\n",
    "        df[\"APoints\"]=apoints\n",
    "        for teamName in self.teamsMap.keys():\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hpoints = team['HPoints'].values\n",
    "            apoints = team['APoints'].values\n",
    "            psum = 0\n",
    "            haccp = team['HAccP'].values\n",
    "            aaccp = team['AAccP'].values\n",
    "        \n",
    "            for  i in range(0,n):\n",
    "                if i < hpoints.shape[0]:\n",
    "                    psum = psum + (hpoints[i] if hometeam[i] == teamName else apoints[i] ) \n",
    "                else:\n",
    "                    psum = psum + defaultPt        \n",
    "                    \n",
    "        \n",
    "            for j in range(team.shape[0]):\n",
    "\n",
    "                if j+n < hpoints.shape[0]:                     \n",
    "                    psum = psum + (hpoints[j+n] if hometeam[j+n]==teamName else apoints[j+n])\n",
    "                else:\n",
    "                    psum = psum + defaultPt \n",
    "                \n",
    "                psum = psum - (hpoints[j] if hometeam[j]==teamName else apoints[j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    haccp[j]=psum\n",
    "                else:\n",
    "                    aaccp[j]=psum\n",
    "            team['HAccp']=haccp\n",
    "            team['AAccP']=aaccp\n",
    "            #print(team[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "            df.update(team)\n",
    "            \n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initRecentData(self, n =5):\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HDraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ADraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HLose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ALose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "                \n",
    "        df[\"HScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HRestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ARestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        #general\n",
    "        generalList = ['HS','AS','HST','AST','H_Poss','A_Poss','H_tkPass_OK','A_atkPass_Ok',\n",
    "                       'H_atkPass_tot','A_atkPass_tot','H_ins','A_ins']\n",
    "        generalOutput = []\n",
    "        for attr in generalList:\n",
    "            temp = attr+'_Acc'\n",
    "            df[temp]=pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "            generalOutput.append(temp)\n",
    "        #\n",
    "        df[\"Sufficient\"] = pd.Series(np.ones(shape=(df.shape[0],)))\n",
    "        \n",
    "      \n",
    "            \n",
    "        \n",
    "        hscore = df['FTHG'].values\n",
    "        ascore = df['FTAG'].values\n",
    "        hconcede = df ['FTAG'].values\n",
    "        aconcede = df['FTHG'].values\n",
    "               \n",
    "        hwin = df['HWin'].values\n",
    "        awin = df['AWin'].values\n",
    "        hlose = df['HLose'].values\n",
    "        alose = df['ALose'].values\n",
    "        hdraw = df['HDraw'].values\n",
    "        adraw = df['ADraw'].values\n",
    "        hmoral = df['HMoral'].values\n",
    "        amoral = df['AMoral'].values\n",
    "        \n",
    "        rankRatio = (df['HAccP'].values+1) / (df['AAccP'].values +1)\n",
    "        \n",
    "        \n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hwin[i] = 1\n",
    "                hmoral[i] = 3 * 1/rankRatio[i]\n",
    "                alose[i]= 1\n",
    "                amoral[i] = -3 * 1/rankRatio[i]\n",
    "            elif ftr[i] == 'D':\n",
    "                hdraw[i] = 1\n",
    "                hmoral[i] = 1 * 1/rankRatio[i]\n",
    "                adraw[i] = 1\n",
    "                amoral[i] = 1 * rankRatio[i]\n",
    "            else :\n",
    "                hlose[i] = 1\n",
    "                hmoral[i] = -3*rankRatio[i]\n",
    "                awin [i] = 1\n",
    "                amoral[i] = 3*rankRatio[i]\n",
    "        \n",
    "        \n",
    "        df[\"HWin\"]=hwin\n",
    "        df[\"AWin\"]=awin\n",
    "        df[\"HDraw\"]=hdraw\n",
    "        df[\"ADraw\"]=adraw\n",
    "        df[\"HLose\"]=hlose\n",
    "        df[\"ALose\"]=alose\n",
    "        df[\"HScore\"]=hscore\n",
    "        df[\"AScore\"]=ascore\n",
    "        df[\"HConcede\"]=hconcede\n",
    "        df[\"AConcede\"]=aconcede\n",
    "        df[\"HMoral\"] = hmoral\n",
    "        df[\"AMoral\"] = amoral\n",
    "        \n",
    "        \n",
    "        \n",
    "        for teamName in self.teamsMap.keys():\n",
    "            print(teamName)\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hwin = team[\"HWin\"].values\n",
    "            awin = team[\"AWin\"].values\n",
    "            hlose= team[\"HLose\"].values\n",
    "            alose = team[\"ALose\"].values\n",
    "            hdraw = team[\"HDraw\"].values\n",
    "            adraw = team[\"ADraw\"].values\n",
    "            hscore = team[\"HScore\"].values\n",
    "            ascore = team[\"AScore\"].values\n",
    "            hconcede = team[\"HConcede\"].values\n",
    "            aconcede = team[\"AConcede\"].values\n",
    "            hmoral = team[\"HMoral\"].values\n",
    "            amoral = team[\"AMoral\"].values\n",
    "            hrestday = team[\"HRestDay\"].values\n",
    "            arestday = team[\"ARestDay\"].values\n",
    "            \n",
    "            #general\n",
    "            hs= team[\"HS\"].values\n",
    "            as_ = team[\"AS\"].values\n",
    "            hst = team[\"HST\"].values\n",
    "            ast = team[\"AST\"].values\n",
    "            original_list =[]\n",
    "            output_list=[]\n",
    "            for indx, o_attr in enumerate(generalList):\n",
    "                original_list.append(team[o_attr].values)\n",
    "                output_list.append(team[generalOutput[indx]].values)\n",
    "            \n",
    "            \n",
    "            matchDate =team['Date'].values\n",
    "            sufficient = team['Sufficient'].values\n",
    "            teamMatchesDate = self.teamsData[teamName].sort('1',ascending=False)['1'].values\n",
    "            \n",
    "            restday = 0\n",
    "            winsum =0 \n",
    "            losesum=0\n",
    "            drawsum=0\n",
    "            scoresum =0\n",
    "            concedesum=0\n",
    "            moralsum = 0\n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            teamAttrSum_list=[0 for i in range(int(len(original_list)/2))]\n",
    "            for  i in range(0,n):\n",
    "                if i < team.shape[0]:\n",
    "                    scoresum = scoresum + (hscore[i] if hometeam[i] == teamName else ascore[i])\n",
    "                    winsum = winsum + (hwin[i] if hometeam[i] == teamName else awin[i])\n",
    "                    losesum= losesum + (hlose[i] if hometeam[i] == teamName else alose[i])\n",
    "                    drawsum= drawsum + (hdraw[i] if hometeam[i] == teamName else adraw[i])\n",
    "                    concedesum = concedesum+ (hconcede[i] if hometeam[i] == teamName else aconcede[i])\n",
    "                    moralsum= moralsum+ (hmoral[i] if hometeam[i] == teamName else amoral[i])\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        teamAttrSum_list[attrIndx] +=(original_list[2*attrIndx][i] if hometeam[i] == teamName else original_list[2*attrIndx+1][i])\n",
    "                else:\n",
    "                    # + 0\n",
    "                    pass\n",
    "            dateIndx = 0\n",
    "            for j in range(team.shape[0]):\n",
    "                while True:\n",
    "                    if dateIndx >= teamMatchesDate.shape[0]:\n",
    "                        sufficient[j] = False\n",
    "                        break\n",
    "                    if teamMatchesDate[dateIndx] < matchDate[j] :\n",
    "                        restday = (matchDate[j] - teamMatchesDate[dateIndx])/np.timedelta64(1,'D')\n",
    "                        break\n",
    "                    else:\n",
    "                        dateIndx = dateIndx + 1\n",
    "                \n",
    "                if j+n < team.shape[0]:                     \n",
    "                    scoresum = scoresum + (hscore[j+n] if hometeam[j+n] == teamName else ascore[j+n])\n",
    "                    winsum = winsum + (hwin[j+n] if hometeam[j+n] == teamName else awin[j+n])\n",
    "                    losesum= losesum + (hlose[j+n] if hometeam[j+n] == teamName else alose[j+n])\n",
    "                    drawsum= drawsum + (hdraw[j+n] if hometeam[j+n] == teamName else adraw[j+n])\n",
    "                    concedesum = concedesum+ (hconcede[j+n] if hometeam[j+n] == teamName else aconcede[j+n])\n",
    "                    moralsum= moralsum+ (hmoral[j+n] if hometeam[j+n] == teamName else amoral[j+n])\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        teamAttrSum_list[attrIndx] += (original_list[2*attrIndx][j+n] if hometeam[j+n] == teamName else original_list[2*attrIndx+1][j+n])\n",
    "                else:\n",
    "                    sufficient[j] = False\n",
    "                    \n",
    "                \n",
    "                scoresum = scoresum - (hscore[j] if hometeam[j] == teamName else ascore[j])\n",
    "                winsum = winsum - (hwin[j] if hometeam[j] == teamName else awin[j])\n",
    "                losesum= losesum - (hlose[j] if hometeam[j] == teamName else alose[j])\n",
    "                drawsum= drawsum - (hdraw[j] if hometeam[j] == teamName else adraw[j])\n",
    "                concedesum = concedesum - (hconcede[j] if hometeam[j] == teamName else aconcede[j])\n",
    "                moralsum= moralsum - (hmoral[j] if hometeam[j] == teamName else amoral[j])\n",
    "                for attrIndx in range(len(teamAttrSum_list)):\n",
    "                    teamAttrSum_list[attrIndx] -=  (original_list[2*attrIndx][j] if hometeam[j] == teamName else original_list[2*attrIndx+1][j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    hscore[j] = scoresum\n",
    "                    hwin[j] = winsum\n",
    "                    hlose[j] = losesum\n",
    "                    hdraw[j] = drawsum\n",
    "                    hconcede[j] = concedesum\n",
    "                    hmoral[j] = moralsum\n",
    "                    hrestday[j] = restday\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        output_list[2*attrIndx][j] = teamAttrSum_list[attrIndx]\n",
    "                else:\n",
    "                    ascore[j] = scoresum\n",
    "                    awin[j] = winsum\n",
    "                    alose[j] = losesum\n",
    "                    adraw[j] = drawsum\n",
    "                    aconcede[j] = concedesum\n",
    "                    amoral[j] = moralsum\n",
    "                    arestday[j] = restday\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        output_list[2*attrIndx+1][j] = teamAttrSum_list[attrIndx]\n",
    "            team[\"HWin\"]=hwin\n",
    "            team[\"AWin\"]=awin\n",
    "            team[\"HDraw\"]=hdraw\n",
    "            team[\"ADraw\"]=adraw\n",
    "            team[\"HLose\"]=hlose\n",
    "            team[\"ALose\"]=alose\n",
    "            team[\"HScore\"]=hscore\n",
    "            team[\"AScore\"]=ascore\n",
    "            team[\"HConcede\"]=hconcede\n",
    "            team[\"AConcede\"]=aconcede\n",
    "            team[\"HMoral\"] = hmoral\n",
    "            team[\"AMoral\"] = amoral\n",
    "            team['Sufficient'] = sufficient\n",
    "            for indx in range(len(output_list)):\n",
    "                team[generalOutput[indx]]= output_list[indx]\n",
    "        \n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            df.update(team)\n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initTeamData(self):\n",
    "        self.teamsData={}\n",
    "        self.teamsById={}\n",
    "        for name in self.teamsMap.keys():\n",
    "            self.readTeamMatch(name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "c = FootballDataHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:23: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:24: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#c.readFootBallData(\"E0_1112.csv\")\n",
    "c.readFootBallData(2012)\n",
    "c.readFootBallData(2013)\n",
    "c.readFootBallData(2014)\n",
    "c.readFootBallData(2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>A_Poss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>37.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Man City</td>\n",
       "      <td>45.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>46.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>44.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Everton</td>\n",
       "      <td>46.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>40.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Watford</td>\n",
       "      <td>47.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>49.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>50.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>2016-03-13</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>62.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Norwich</td>\n",
       "      <td>66.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>53.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>61.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>51.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Everton</td>\n",
       "      <td>58.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Man City</td>\n",
       "      <td>28.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>50.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>44.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Watford</td>\n",
       "      <td>49.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>65.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>44.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Man United</td>\n",
       "      <td>38.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>37.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2012-09-15</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>48.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>31.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>55.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>38.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Man City</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>51.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>66.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>37.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Norwich</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Man United</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>47.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>60.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012-08-22</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>28.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012-08-20</td>\n",
       "      <td>Everton</td>\n",
       "      <td>69.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-08-19</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012-08-19</td>\n",
       "      <td>Man City</td>\n",
       "      <td>35.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>48.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Reading</td>\n",
       "      <td>47.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>QPR</td>\n",
       "      <td>50.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>59.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>65.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>29.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        HomeTeam  A_Poss\n",
       "1442 2016-03-20       Tottenham    37.2\n",
       "1441 2016-03-20     Southampton    50.8\n",
       "1440 2016-03-20       Newcastle    40.1\n",
       "1439 2016-03-20        Man City    45.5\n",
       "1436 2016-03-19         Swansea    46.2\n",
       "1434 2016-03-19  Crystal Palace    44.1\n",
       "1435 2016-03-19         Everton    46.1\n",
       "1433 2016-03-19         Chelsea    40.6\n",
       "1437 2016-03-19         Watford    47.9\n",
       "1438 2016-03-19       West Brom    49.9\n",
       "1432 2016-03-14       Leicester    50.1\n",
       "1431 2016-03-13     Aston Villa    62.2\n",
       "1430 2016-03-12           Stoke    39.4\n",
       "1429 2016-03-12         Norwich    66.1\n",
       "1428 2016-03-12     Bournemouth    56.0\n",
       "1427 2016-03-06       West Brom    53.4\n",
       "1426 2016-03-06  Crystal Palace    61.5\n",
       "1422 2016-03-05     Southampton    34.9\n",
       "1418 2016-03-05         Chelsea    51.3\n",
       "1419 2016-03-05         Everton    58.5\n",
       "1420 2016-03-05        Man City    28.8\n",
       "1421 2016-03-05       Newcastle    50.7\n",
       "1423 2016-03-05         Swansea    44.2\n",
       "1424 2016-03-05       Tottenham    48.2\n",
       "1425 2016-03-05         Watford    49.8\n",
       "1417 2016-03-02        West Ham    65.1\n",
       "1416 2016-03-02           Stoke    44.7\n",
       "1415 2016-03-02      Man United    38.8\n",
       "1414 2016-03-02       Liverpool    50.5\n",
       "1413 2016-03-02         Arsenal    37.4\n",
       "...         ...             ...     ...\n",
       "36   2012-09-15      Sunderland    65.9\n",
       "28   2012-09-02     Southampton    54.9\n",
       "27   2012-09-02       Newcastle    48.3\n",
       "26   2012-09-02       Liverpool    47.0\n",
       "25   2012-09-01           Wigan    31.3\n",
       "24   2012-09-01        West Ham    54.9\n",
       "23   2012-09-01       West Brom    55.7\n",
       "22   2012-09-01       Tottenham    38.4\n",
       "21   2012-09-01         Swansea    36.2\n",
       "20   2012-09-01        Man City    40.1\n",
       "18   2012-08-26       Liverpool    51.3\n",
       "19   2012-08-26           Stoke    66.7\n",
       "17   2012-08-25       Tottenham    41.0\n",
       "16   2012-08-25         Swansea    37.8\n",
       "15   2012-08-25     Southampton    50.8\n",
       "14   2012-08-25         Norwich    44.9\n",
       "13   2012-08-25      Man United    40.1\n",
       "12   2012-08-25         Chelsea    47.5\n",
       "11   2012-08-25     Aston Villa    60.8\n",
       "10   2012-08-22         Chelsea    28.4\n",
       "9    2012-08-20         Everton    69.2\n",
       "8    2012-08-19           Wigan    48.0\n",
       "7    2012-08-19        Man City    35.9\n",
       "2    2012-08-18       Newcastle    48.1\n",
       "4    2012-08-18         Reading    47.4\n",
       "3    2012-08-18             QPR    50.1\n",
       "1    2012-08-18          Fulham    40.2\n",
       "5    2012-08-18       West Brom    59.5\n",
       "6    2012-08-18        West Ham    65.8\n",
       "0    2012-08-18         Arsenal    29.9\n",
       "\n",
       "[1443 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.df[['Date','HomeTeam','A_Poss']].sort(columns='Date',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "c.readFuture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:140: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c.initTeamData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:224: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df = c.initRanking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:284: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:400: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:479: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:480: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:481: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:484: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:485: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:486: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:487: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:488: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:489: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:492: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burnley\n",
      "West Ham\n",
      "West Brom\n",
      "Aston Villa\n",
      "Arsenal\n",
      "Crystal Palace\n",
      "Norwich\n",
      "Man United\n",
      "Hull\n",
      "Chelsea\n",
      "QPR\n",
      "Watford\n",
      "Cardiff\n",
      "Fulham\n",
      "Everton\n",
      "Sunderland\n",
      "Swansea\n",
      "Reading\n",
      "Man City\n",
      "Leicester\n",
      "Southampton\n",
      "Bournemouth\n",
      "Stoke\n",
      "Tottenham\n",
      "Newcastle\n",
      "Wigan\n",
      "Liverpool\n"
     ]
    }
   ],
   "source": [
    "df=c.initRecentData(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.saveDf('dataSet/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.loadDf('dataSet/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y=c.getH7(removeInsufficient=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "def plotErrorDate(X_test, X_err, dateCol = 10):\n",
    "    X_test_date =np.sort(X_test[:,c.dateColumn])\n",
    "    X_date=[]\n",
    "    y_date=[]\n",
    "    for v in X_test_date:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_date) ==0  or X_date[-1] != date:\n",
    "            X_date.append(date)\n",
    "            y_date.append(1)\n",
    "        else:\n",
    "            y_date[-1] = y_date[-1] +1\n",
    "    plt.plot_date(X_date,y_date,xdate=True)\n",
    "    X_err_d = np.sort(X_err[:,c.dateColumn])\n",
    "    X_err_date=[]\n",
    "    y_err_date = []\n",
    "    for v in X_err_d:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_err_date) ==0  or X_err_date[-1] != date:\n",
    "            X_err_date.append(date)\n",
    "            y_err_date.append(1)\n",
    "        else:\n",
    "            y_err_date[-1] = y_err_date[-1] +1\n",
    "    plt.plot_date(X_err_date,y_err_date,xdate=True,color='red')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "g_hiddenNodes = int(45)\n",
    "g_alpha = 0 \n",
    "clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_hiddenNodes], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "              learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=1000,early_stopping = False,verbose = 3)\n",
    "mlp = Pipeline([ ('scl', StandardScaler()),('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"start learning\")\n",
    "sys.stdout.flush()\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=mlp, \n",
    "                       X=X, \n",
    "                      y=y, \n",
    "                      train_sizes=np.linspace(0.1, 1.0, 4), \n",
    "                      cv=4,\n",
    "                     n_jobs=1,verbose=3)\n",
    "print(\"finishing\")   \n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "print(test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learningRes = np.vstack([train_sizes,train_mean,train_std,test_mean,test_std]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learningDf = pd.DataFrame(learningRes,columns=['size','train_mean','train_std','test_mean','test_std'])\n",
    "print(learningDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotCurve(train_mean,train_std,test_mean,test_std,train_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate(mlp, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    firstNScores = []\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "\n",
    "        mlp.fit(X[train], y[train])\n",
    "        score = mlp.score(X[test], y[test])\n",
    "        firstNScores.append(firstNScore(2, mlp.predict_proba(X[test]), y[test]))\n",
    "        train_scores.append(mlp.score(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, firstNScores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def lamda_test(mlp, X, y, lamdas):\n",
    "    \n",
    "    train_scores=[]\n",
    "    test_scores=[]\n",
    "    for lamda in lamdas:\n",
    "        clf.set_params(alpha= lamda)\n",
    "        train_s, test_s, firstNScores = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_s, test_s, firstNScores =futureTest(mlp,X,y,numOfWeek=20) \n",
    "        train_scores.append(train_s)\n",
    "        test_scores.append(test_s)\n",
    "        print(\"lamda: {}, train: {}, test: {}\".format(lamda, \n",
    "                    np.mean(train_s), np.mean(test_s)) )\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plotCurve(train_mean,train_std,test_mean,test_std,lamdas)\n",
    "    return np.array(train_scores),np.array(test_scores)\n",
    "\n",
    "l_range = []\n",
    "for i in range(0,50):\n",
    "    l_range.append(2*i)\n",
    "train_scores,test_scores = lamda_test(mlp,X,y,np.array(l_range))\n",
    "#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "alphaRes = np.vstack([l_range,train_mean,train_std,test_mean,test_std]).T\n",
    "alphaDf = pd.DataFrame(alphaRes,columns=['alpha','train_mean','train_std','test_mean','test_std'])\n",
    "print(alphaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testNodeSize(start ,end):\n",
    "    node_range = range(start,end,5)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    for node in node_range:   \n",
    "        print(\"start node:{}\".format(node))\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[node], activation='logistic', algorithm='l-bfgs', alpha=0, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores,first2 = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_scores,test_scores , first2= futureTest(mlp, X,y,numOfWeek = 10)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        print(\"Node {}: train_mean {}  v.s. test_mean {}\".format(node,np.mean(train_scores),np.mean(test_scores)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(node_range))\n",
    "    return node_range, train_means,train_std,test_means,test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node_range, train_means,train_std,test_means,test_std=testNodeSize(1,X.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodeRes = np.vstack([node_range,train_means,train_std,test_means,test_std]).T\n",
    "nodeDf = pd.DataFrame(nodeRes,columns=['nodeNum','train_mean','train_std','test_mean','test_std'])\n",
    "print(nodeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.set_params(alpha=g_alpha)\n",
    "print(clf)\n",
    "train_score, test_score, first2 = futureTest(mlp,X,y,numOfWeek = 30, verbose=True)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testRecentNum(start, end):\n",
    "    recent_range = range(start,end)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    first2_mean=[]\n",
    "    for recent in recent_range:\n",
    "        print(\"start recent:{}\".format(recent))\n",
    "        X,y = c.getH7(recent)\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_alpha], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores, first2 = crossValidate(mlp,X,y,fold=10)\n",
    "        #train_scores,test_scores, first2 = futureTest(mlp, X,y,numOfWeek = 15)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        first2_mean.append(np.mean(first2))\n",
    "        print(\"recent {}: train_mean {}  v.s. test_mean {} , first2_mean {}\".format(\n",
    "                recent,np.mean(train_scores),np.mean(test_scores),np.mean(first2)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(recent_range))\n",
    "    return train_means,train_std,test_means,test_std,first2_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_means,train_std,test_means,test_std,first2_mean=testRecentNum(1 ,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "proba = mlp.predict_proba(X_test)\n",
    "precisionMatrix(proba,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#future\n",
    "mlp.fit(X,y)\n",
    "inData = c.readPredict(\"future.csv\")\n",
    "print(inData)\n",
    "X_in, y_in = c.getH6(5,target=inData)\n",
    "res = mlp.predict(X_in)\n",
    "proba= mlp.predict_proba(X_in)\n",
    "print(mlp.score(X_in,y_in))\n",
    "print (np.hstack([proba,y_in]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "return\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient=True, encode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1358, 29)\n",
      "                home         away        Referee   time HRestTime ARestTime  \\\n",
      "0             Fulham     Man City       M Halsey  15612         7         3   \n",
      "1            Norwich    Liverpool        M Jones  15612         2         2   \n",
      "2            Everton  Southampton      L Probert  15612         3         3   \n",
      "3              Stoke      Swansea         J Moss  15612         7         3   \n",
      "4            Arsenal      Chelsea     M Atkinson  15612         2         3   \n",
      "5         Man United    Tottenham          C Foy  15612         2         2   \n",
      "6        Aston Villa    West Brom       A Taylor  15613         4         3   \n",
      "7                QPR     West Ham  M Clattenburg  15614         4         5   \n",
      "8              Wigan      Everton       K Friend  15619         7         7   \n",
      "9          West Brom          QPR        M Jones  15619         6         4   \n",
      "10          Man City   Sunderland      L Probert  15619         2         7   \n",
      "11           Chelsea      Norwich       A Taylor  15619         3         7   \n",
      "12          West Ham      Arsenal         P Dowd  15619         4         2   \n",
      "13           Swansea      Reading         M Dean  15619         7         7   \n",
      "14       Southampton       Fulham  M Clattenburg  15620         8         8   \n",
      "15         Tottenham  Aston Villa    N Swarbrick  15620         2         7   \n",
      "16         Liverpool        Stoke        L Mason  15620         2         8   \n",
      "17         Newcastle   Man United         H Webb  15620         2         4   \n",
      "18           Norwich      Arsenal      L Probert  15633        14        13   \n",
      "19          West Ham  Southampton    N Swarbrick  15633        13        13   \n",
      "20         West Brom     Man City  M Clattenburg  15633        14        14   \n",
      "21           Swansea        Wigan        M Jones  15633        14        14   \n",
      "22        Man United        Stoke       A Taylor  15633        13        13   \n",
      "23            Fulham  Aston Villa          C Foy  15633        13        13   \n",
      "24         Liverpool      Reading         R East  15633        13        14   \n",
      "25         Tottenham      Chelsea         M Dean  15633        13        14   \n",
      "26               QPR      Everton         J Moss  15634        15        15   \n",
      "27        Sunderland    Newcastle     M Atkinson  15634        15        14   \n",
      "28          Man City      Swansea     M Atkinson  15640         2         7   \n",
      "29           Arsenal          QPR       A Taylor  15640         2         6   \n",
      "...              ...          ...            ...    ...       ...       ...   \n",
      "1328           Stoke    Newcastle    N Swarbrick  16862         4        17   \n",
      "1329      Man United      Watford        M Jones  16862         3         4   \n",
      "1330        West Ham    Tottenham     A Marriner  16862         4         3   \n",
      "1331         Arsenal      Swansea       R Madley  16862         3         3   \n",
      "1332       Liverpool     Man City     M Atkinson  16862         2         2   \n",
      "1333         Chelsea        Stoke  M Clattenburg  16865         3         2   \n",
      "1334     Southampton   Sunderland    N Swarbrick  16865         3         3   \n",
      "1335       Newcastle  Bournemouth      P Tierney  16865         2         3   \n",
      "1336         Everton     West Ham       A Taylor  16865         3         2   \n",
      "1337         Swansea      Norwich       C Pawson  16865         2         3   \n",
      "1338       Tottenham      Arsenal       M Oliver  16865         2         2   \n",
      "1339         Watford    Leicester         J Moss  16865         2         3   \n",
      "1340        Man City  Aston Villa        L Mason  16865         2         3   \n",
      "1341       West Brom   Man United         M Dean  16866         4         3   \n",
      "1342  Crystal Palace    Liverpool     A Marriner  16866         4         3   \n",
      "1343     Bournemouth      Swansea         R East  16872         7         7   \n",
      "1344           Stoke  Southampton        L Mason  16872         7         7   \n",
      "1345         Norwich     Man City         J Moss  16872         7         7   \n",
      "1346     Aston Villa    Tottenham       A Taylor  16873         8         2   \n",
      "1347       Leicester    Newcastle       C Pawson  16874         8         9   \n",
      "1348       West Brom      Norwich       A Taylor  16879        12         7   \n",
      "1349         Watford        Stoke       C Pawson  16879         6         7   \n",
      "1350         Chelsea     West Ham       R Madley  16879         6         5   \n",
      "1351         Everton      Arsenal  M Clattenburg  16879         6         2   \n",
      "1352  Crystal Palace    Leicester        M Jones  16879         7         4   \n",
      "1353         Swansea  Aston Villa         M Dean  16879         7         5   \n",
      "1354       Tottenham  Bournemouth    N Swarbrick  16880         2         8   \n",
      "1355       Newcastle   Sunderland     M Atkinson  16880         5        15   \n",
      "1356        Man City   Man United       M Oliver  16880         4         2   \n",
      "1357     Southampton    Liverpool         R East  16880         8         2   \n",
      "\n",
      "     HS_Acc AS_Acc HST_Acc AST_Acc ... HWin HDraw HLose H goal Diff AWin  \\\n",
      "0        74     79      56      53 ...    3     0     2           5    2   \n",
      "1        55     79      27      40 ...    0     3     2          -6    0   \n",
      "2        94     59      52      31 ...    3     1     1           4    1   \n",
      "3        46     64      24      41 ...    0     4     1          -1    2   \n",
      "4        71     69      33      38 ...    2     3     0           7    4   \n",
      "5        75     85      41      50 ...    4     0     1           6    2   \n",
      "6        55     58      33      35 ...    1     1     3          -4    3   \n",
      "7        54     61      32      37 ...    0     2     3          -8    2   \n",
      "8        55     95      29      54 ...    1     1     3          -4    3   \n",
      "9        57     47      32      30 ...    2     2     1           0    0   \n",
      "10       80     28      50      17 ...    2     3     0           3    1   \n",
      "11       74     61      40      32 ...    4     1     0           6    0   \n",
      "12       70     71      43      34 ...    2     2     1           1    2   \n",
      "13       60     42      33      18 ...    1     1     3          -4    0   \n",
      "14       58     70      27      50 ...    1     0     4          -7    2   \n",
      "15       83     64      50      35 ...    3     2     0           4    1   \n",
      "16       75     50      39      26 ...    1     2     2           0    1   \n",
      "17       62     78      29      42 ...    1     3     1          -1    4   \n",
      "18       55     75      32      42 ...    0     2     3          -7    3   \n",
      "19       70     57      40      24 ...    2     2     1           2    1   \n",
      "20       60     95      35      61 ...    3     1     1           1    3   \n",
      "21       75     53      42      27 ...    0     2     3          -7    0   \n",
      "22       70     48      40      25 ...    4     0     1           8    1   \n",
      "23       70     65      44      37 ...    2     1     2           0    1   \n",
      "24       76     41      36      21 ...    1     2     2           0    0   \n",
      "25       88     71      50      42 ...    4     1     0           6    4   \n",
      "26       54     92      32      54 ...    0     1     4          -5    2   \n",
      "27       33     64      21      27 ...    1     3     1          -2    1   \n",
      "28       97     79      62      43 ...    3     2     0           5    1   \n",
      "29       77     59      42      33 ...    2     1     2           5    0   \n",
      "...     ...    ...     ...     ... ...  ...   ...   ...         ...  ...   \n",
      "1328     46     70      14      26 ...    2     0     3          -6    2   \n",
      "1329     59     52      24      12 ...    2     1     2           2    2   \n",
      "1330     73    107      20      43 ...    2     2     1           2    5   \n",
      "1331     77     67      28      18 ...    2     1     2           1    1   \n",
      "1332     73     80      27      17 ...    2     1     2           4    2   \n",
      "1333     69     52      20      14 ...    3     2     0           6    3   \n",
      "1334     48     69      13      19 ...    2     1     2          -1    1   \n",
      "1335     52     56      19      17 ...    1     0     4          -8    2   \n",
      "1336     96     77      28      20 ...    3     0     2           6    3   \n",
      "1337     71     57      15      12 ...    1     2     2          -1    0   \n",
      "1338     92     86      34      31 ...    4     0     1           5    2   \n",
      "1339     52     69      10      24 ...    1     2     2          -1    3   \n",
      "1340     67     41      15      12 ...    1     1     3          -5    1   \n",
      "1341     51     65      16      26 ...    2     2     1           1    3   \n",
      "1342     68     65      19      30 ...    0     2     3          -3    3   \n",
      "1343     64     63      18      14 ...    2     1     2           0    2   \n",
      "1344     58     50      19      15 ...    3     1     1           1    2   \n",
      "1345     55     73      13      21 ...    0     1     4          -5    2   \n",
      "1346     37    101      11      39 ...    1     0     4         -11    3   \n",
      "1347     70     55      25      20 ...    3     1     1           3    1   \n",
      "1348     43     48      11       9 ...    3     1     1           2    0   \n",
      "1349     51     60      11      18 ...    1     1     3          -2    3   \n",
      "1350     77     69      23      19 ...    3     2     0           6    3   \n",
      "1351     91     75      34      24 ...    3     0     2           6    2   \n",
      "1352     68     66      20      19 ...    0     2     3          -3    3   \n",
      "1353     56     38      15      10 ...    2     0     3          -1    0   \n",
      "1354     94     59      38      18 ...    3     1     1           3    3   \n",
      "1355     61     69      19      20 ...    1     0     4          -7    1   \n",
      "1356     82     57      22      22 ...    1     1     3          -2    2   \n",
      "1357     53     71      17      28 ...    2     1     2          -1    3   \n",
      "\n",
      "     ADraw ALose A goal diff moraldiff + h-a  y  \n",
      "0        3     0           3         -5.4577  A  \n",
      "1        2     3          -6         4.26691  A  \n",
      "2        0     4          -6         20.4973  H  \n",
      "3        1     2           3        -0.98787  H  \n",
      "4        1     0           7         -7.3312  A  \n",
      "5        2     1           2         7.24231  A  \n",
      "6        1     1           3        -16.1433  D  \n",
      "7        2     1           1        -16.4869  A  \n",
      "8        1     1           5        -18.8485  D  \n",
      "9        2     3          -4         17.7318  H  \n",
      "10       4     0           1         5.22302  H  \n",
      "11       3     2          -4         26.8163  H  \n",
      "12       2     1           6         1.33957  A  \n",
      "13       2     3          -5         3.22343  D  \n",
      "14       0     3          -1        -9.28572  D  \n",
      "15       2     2          -3         17.1502  H  \n",
      "16       3     1           1        -5.98228  D  \n",
      "17       0     1           6         -7.8114  A  \n",
      "18       1     1           8        -21.3046  H  \n",
      "19       1     3          -5         12.8002  H  \n",
      "20       2     0           6        -6.20904  A  \n",
      "21       2     3          -6        0.178857  H  \n",
      "22       3     1           1         9.43603  H  \n",
      "23       2     2          -3         5.66887  H  \n",
      "24       2     3          -5         7.88105  H  \n",
      "25       1     0           7        -2.72658  A  \n",
      "26       2     1           3        -23.5168  D  \n",
      "27       3     1          -2       -0.225469  D  \n",
      "28       1     3          -6         24.2804  H  \n",
      "29       2     3          -3         12.0548  H  \n",
      "...    ...   ...         ...             ... ..  \n",
      "1328     0     3          -6         3.14025  H  \n",
      "1329     2     1           1        -4.32401  H  \n",
      "1330     0     0           8        -19.9385  H  \n",
      "1331     2     2          -1         13.1779  A  \n",
      "1332     1     2           2        -2.28404  H  \n",
      "1333     0     2          -2         9.61243  D  \n",
      "1334     2     2          -1         1.78991  D  \n",
      "1335     1     2          -1        -12.8138  A  \n",
      "1336     1     1           3        -10.8407  A  \n",
      "1337     1     4          -7         19.0105  H  \n",
      "1338     1     2           1         13.5843  D  \n",
      "1339     1     1           4        -19.0615  A  \n",
      "1340     0     4          -9         13.3512  H  \n",
      "1341     1     1           4        -4.45653  H  \n",
      "1342     1     1           8        -24.5246  A  \n",
      "1343     1     2           0         1.36626  H  \n",
      "1344     1     2          -1         8.06201  A  \n",
      "1345     0     3          -1        -21.0893  D  \n",
      "1346     1     1           2        -30.6047  A  \n",
      "1347     0     4          -9         31.6266  H  \n",
      "1348     2     3          -3         20.5266  A  \n",
      "1349     1     1           3        -10.9102  A  \n",
      "1350     1     1           2         1.42972  D  \n",
      "1351     1     2           1        -2.52855  A  \n",
      "1352     1     1           2        -33.3654  A  \n",
      "1353     0     5         -15         16.7183  H  \n",
      "1354     1     1           3         3.93801  H  \n",
      "1355     3     1           0        -15.3967  D  \n",
      "1356     1     2           0        -4.08739  A  \n",
      "1357     1     1           8           -9.09  H  \n",
      "\n",
      "[1358 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "df = pd.DataFrame(np.hstack([X,y.reshape(y.shape[0],1)]))\n",
    "\n",
    "df.columns = ['home','away','Referee','time','HRestTime','ARestTime','HS_Acc','AS_Acc','HST_Acc','AST_Acc',\n",
    "                                            'H_Poss_Acc','A_Poss_Acc','H_atkPass_tot_Acc','A_atkPass_tot_Acc'\n",
    "                                             ,'H_atkPass_Ok_Acc','A_atkPass_OK_Acc','H_ins_Acc','A_ins_Acc'\n",
    "              ,'HAccP - AAccP','H/A','HWin','HDraw','HLose','H goal Diff',\n",
    "'AWin','ADraw','ALose','A goal diff','moraldiff + h-a',\n",
    "              'y']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('dataSet/V9.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "def splitData(X,y):\n",
    "    X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_test, X_val, y_test,y_val = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "def createModel(hidSize, inputDim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidSize[0], input_dim=inputDim, init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(hidSize[1], init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(3, init='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd = SGD(lr=1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "    return model\n",
    "earlyCallback = EarlyStopping(patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1358, 104)\n"
     ]
    }
   ],
   "source": [
    "print(X_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate2(node_sizes, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    proba_test = []\n",
    "    proba_y=[]\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "        earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "        model = createModel(node_sizes,X.shape[1])\n",
    "        history = model.fit(X[train],y[train],verbose=0,nb_epoch=500, validation_split=0.1,show_accuracy=True, callbacks=[earlyCallback])\n",
    "      #  firstNScores.append(firstNScore(2, model.predict_proba(X[test]), y[test]))\n",
    "        score = model.evaluate(X[test],y[test])\n",
    "        proba_test.append(model.predict_proba(X[test]))\n",
    "        proba_y.append(y[test])\n",
    "        train_scores.append(model.evaluate(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, val_loss: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, proba_test,proba_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNodeNum(X,y, sizes):\n",
    "    train_loss=[] \n",
    "    score_loss=[]\n",
    "    for s in sizes:\n",
    "        train_scores,scores,  proba_test,proba_y= crossValidate2([s,s],X,y,fold=5)\n",
    "        print(\"size:{} , val_loss_mean:{}\".format(s,np.mean(scores)))\n",
    "        train_loss.append(train_scores)\n",
    "        score_loss.append(scores)\n",
    "    return train_loss,score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.996\n",
      "Epoch 00119: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.966\n",
      "Epoch 00078: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.983\n",
      "Epoch 00057: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00045: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.034\n",
      "size:15 , val_loss_mean:0.994878006496512\n",
      "Epoch 00090: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.997\n",
      "Epoch 00064: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.961\n",
      "Epoch 00108: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00056: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.000\n",
      "Epoch 00044: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.032\n",
      "size:20 , val_loss_mean:0.997879133147474\n",
      "Epoch 00057: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.985\n",
      "Epoch 00109: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00106: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00081: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.008\n",
      "Epoch 00044: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.042\n",
      "size:25 , val_loss_mean:1.002979383102754\n",
      "Epoch 00064: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00066: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.957\n",
      "Epoch 00044: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.985\n",
      "Epoch 00053: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.993\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.035\n",
      "size:30 , val_loss_mean:0.9919062859579861\n",
      "Epoch 00047: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.985\n",
      "Epoch 00063: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:35 , val_loss_mean:0.9929705902102708\n",
      "Epoch 00048: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.984\n",
      "Epoch 00098: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.987\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.991\n",
      "Epoch 00042: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.040\n",
      "size:40 , val_loss_mean:0.9931346279183522\n",
      "Epoch 00043: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.984\n",
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00089: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00042: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.033\n",
      "size:45 , val_loss_mean:0.9946483096903833\n",
      "Epoch 00074: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.995\n",
      "Epoch 00091: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.985\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00027: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.038\n",
      "size:50 , val_loss_mean:0.9952394351575709\n",
      "Epoch 00050: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00080: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00035: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00039: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.040\n",
      "size:55 , val_loss_mean:0.9950326937267647\n",
      "Epoch 00070: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00084: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.965\n",
      "Epoch 00066: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.992\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00027: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:60 , val_loss_mean:0.9955750316447294\n",
      "Epoch 00056: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.988\n",
      "Epoch 00040: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00042: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:65 , val_loss_mean:0.9943353890837905\n",
      "Epoch 00072: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00080: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.000\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:70 , val_loss_mean:0.9954743319077854\n",
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.990\n",
      "Epoch 00061: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.966\n",
      "Epoch 00045: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00044: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.034\n",
      "size:75 , val_loss_mean:0.9945887298112925\n",
      "Epoch 00087: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 1.010\n",
      "Epoch 00074: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.972\n",
      "Epoch 00037: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:80 , val_loss_mean:1.000475552932563\n",
      "Epoch 00045: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.990\n",
      "Epoch 00059: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00064: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00035: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.054\n",
      "size:85 , val_loss_mean:0.999098368823273\n",
      "Epoch 00036: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00056: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00040: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.993\n",
      "Epoch 00033: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00036: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.051\n",
      "size:90 , val_loss_mean:0.9974389820081926\n",
      "Epoch 00058: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.999\n",
      "Epoch 00071: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.954\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.987\n",
      "Epoch 00047: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:95 , val_loss_mean:0.9948091308396705\n",
      "Epoch 00057: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.995\n",
      "Epoch 00067: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.965\n",
      "Epoch 00040: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.992\n",
      "Epoch 00032: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00026: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:100 , val_loss_mean:0.997403977341019\n"
     ]
    }
   ],
   "source": [
    "sizes= range(15,X.shape[1],5)\n",
    "train_loss,score_loss= testNodeNum(X_scaled,y,sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(15, 104, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8XNWV4PHfkVTa19Ju7Ta2hY0tL+AF2yAgGNuA2Zo0\nSaa7k850Mz1JJzM9051MuntwT8JkmaUbPpmehJ6ECZkQSEIChgBhNVuwsfEmvO/Wbi2lpUpbqerO\nH7dky7ZkyVKpqqQ638/nfVT16tV7RyXVPe/d7YkxBqWUUtEpJtwBKKWUCh9NAkopFcU0CSilVBTT\nJKCUUlFMk4BSSkUxTQJKKRXFxkwCIvIjEWkWkf1X2OZxETkmIntFZElgXYKI7BCRPSJSIyKPBDNw\npZRSkzeeK4EngTtGe1FENgJzjDFzgYeBHwAYY/qBW4wxS4ElwEYRWTH5kJVSSgXLmEnAGPM+4LrC\nJvcATwW23QFkiEh+4HlPYJsEIA7QkWlKKRVBgtEmUATUDnteH1iHiMSIyB6gCXjdGLMzCMdTSikV\nJFPaMGyM8Qeqg4qBlSKyYCqPp5RS6urEBWEf9UDJsOfFgXXnGWO6RORtYANwcKSdiIhWFSml1FUy\nxshk3j/eKwEJLCPZCvwxgIisAjqMMc0ikiMiGYH1ScDtwOErHcQYE1HLI488EvYYNKaZE1OkxqUx\nTd+YgmHMKwEReRqoBrJF5CzwCBBvy2zzhDHmZRHZJCLHAQ/whcBbC4GfiEgMNtk8a4x5OShRK6WU\nCooxk4Ax5rPj2ObLI6yrAZZNMC6llJq0s2draW3t4JprykhPTw93OBEpGG0CM1Z1dXW4Q7iMxjQ+\nkRgTRGZcMzWmo0eP8/rrZ4iLK2bHjh3Mnp3C4sUVFBQUIHL11eiR+DkFgwSrXmmyRMRESixKqemt\npuYQ77zTRGHhauLjE/H7/bhcTfT0nMLp7GXZsnLKy0uJj48Pd6iTIiKYSTYMaxJQSs0Yxhh2767h\nww87mTVrJQ7H5YW8x9NJR8cp4uKaqKoqZP78imlbVaRJQCmlAvx+P9u372H37n6Ki1cQG3vl2m6v\nd4DW1jP4fKepqEhm8eIKCgsLJ1RVFC6aBJRSCvD5fLz33i4OHIihuHg5MTHjHwdrjMHlasLjOUlW\nVg/LlpVTUVE2LaqKNAkopaKe1+vl7bc/4vjxZIqKqq4qAVyqp6eL9vZTOByNLF5cwPz5FWRkZAQx\n2uDSJKCUimr9/f288cYOamudzJq1MGhVOV7vAG1tZxkcPE15eRJVVbZX0WQSzFTQJKCUilq9vb28\n8sqHtLYWUVg4f0qOcaGq6BROp4dPfWopOTk5U3KsidAkoJSKSm63m9/+djtu92zy8maH5JhdXa14\nPLvZuHEBJSXFITnmWDQJKKWiTmdnJy+9tIP+/kpyc0tDeuyenm5aWz/i1ltLuPbaeSE99kg0CSil\nokp7eztbt+5EZDFOZ2FYYvB6+6mv38HKlelcf/3isLYTaBJQSkWNc+fOsXXrHhITl5GRkRvWWHw+\nH/X1H3PttX7WrVuOw+EISxyaBJRSUaGhoYGXXvqEtLQbSE3NCnc4gG00rq//hOLidj71qRUkJSWF\nPAZNAtNEV1cXiYmJ02LwiVKR5vTpM7zyylGczpUkJ0fe9A7NzSdISTnJnXeuDPn0E5oEIpzf7+fQ\noaO8885J5sxJ5/bbb4y4fsZKRbKhmUDz8laRmJgS7nBG1d7eiN+/nzvvXEpeXl7IjqtJIIJ5PB7e\neWc3p07FU1BQRVPTfm68MZmlS68Ld2hKTQs1NYd4991mCgpWER+fGO5wxtTd3U5X1y42bqykrCw0\nvZY0CUSoM2fO8uabh/D755GfXwHA4KCXhoZ3uffeSoqKisIcoVKRazwzgUaqvj4P587t4OabZ7Fw\n4fwpn4xOk0CEGRgYYMeOfezb10Nu7jKSk9Muer2npwuP50MefPBG0tLSRtmLUtHN3gugneLilWPO\nBBqJvN4BGho+YvnyZFauXDKlVcCaBCJIS0sLr7++l+7uIgoKKkf9w7e21pKZeZy7715HXNz0+wdX\naiqdOHGKV189TWHhmml1BXAp24V0D/PnD3DzzTdMWRdSTQIRwOfzsX//IX7/+0YyM5eSnj72vCJ1\ndftYvHiQNWuWhyBCpaaHpqYmfvObGnJz15CQkBzucCbNGENj4yHy8pq5446VJCcH/3fSJBBmXV1d\nvP32burr0ygsXExc3Piyvd/vp7b2fTZsKOaaa0Iz74lSkay9vZ1f/3onqamrSEmJ3KmbJ6Kl5TQJ\nCce4884byMzMDOq+NQmEiTGG48dP8vbbx4mPX0h29tVPJtXf30Nb2/s8+OD1OJ3OKYhSqenB7Xbz\nm9/8HpGlYR8JPFVcrmYGBvZy111VFBQUBG2/mgTCoK+vjw8+2MPhw37y85dO6rK1o+MccXH7uP/+\nm0hISAhilEpND/39/Wzd+j5u9zxyckrCHc6Ucrs76OjYyfr11zBnTkVQ9hmSJCAiPwLuApqNMYtH\n2eZxYCPgAT5vjNkrIsXAU0A+4Af+xRjz+BWOE/FJoLGxkddfr6G/v5z8/LlB6f7V2HiE2bPbuO22\n1dPq3qZKTdbg4CC/+93vaWgooKAg/DNyhkJ/fw9NTTu44YYM8vKySE1NJTk5meTk5Al9/0OVBNYC\nbuCpkZKAiGwEvmyMuVNEVgKPGWNWiUgBUBBICKnAx8A9xpjDoxwnYpPA4OAgu3d/ws6d7TidS4M6\nd4kxhrNnd3DTTeksXrwgaPtVKpL5/X7efXcnhw4lUlxcFe5wQmpw0Etray2Dg25EPICHmJh+MjOT\nyM5OIScnlYyMFFJS7JKUlDRqgghGEhizj6Ix5n0RKbvCJvdgz/gxxuwQkQwRyTfGNAFNgfVuETkE\nFAEjJoFI1d7ezltv7aGlJYeiopuC3m9ZRJg1axnvvfcuublZFBaGZ3pcpUJp1679HDwoFBePWLkw\no8XFOSgouLhDiN/vp6/PQ329hxMnPPh8XcTENGLMhQSRk5NKdnbKRQkiKPEEYR9FQO2w5/WBdc1D\nK0SkHFgC7AjC8ULC7/dz+PAx3n33DMnJiykuDl5jzqUcjniczut59dUdPPhgGqmpqVN2LDU6v99P\nT08Pbrcbt9tNe7uHlhY33d19ZGQkkZeXSlZWKmlp9m+UmJioVXgTcPDgET76qJvi4hv18wuIiYkh\nOTntsgGmcCFB1NV5OH784gQRDFM+WilQFfQr4KvGGPeVtt2yZcv5x9XV1VRXV09pbCMxxlBXV8eH\nHx6ltTWN/PybQjJvSWpqJr29lbz55i7uumsdsbGxU37MaNXX14fb7cbj8dDR4aa11U1bmweXqxdj\nkhBJxZgUHI5MEhOLiY9PpK2th7o6Nz6fG5FmjHHjcHjJzk4lNzeVnJxU0tNtckhJSdGJAkdx5sxZ\ntm2rZ9asNfo/Pk7DE0RNzTZqarYFdf/j6h0UqA56cZQ2gR8Abxtjng08PwzcbIxpFpE44CXgFWPM\nY2McI6xtAsYYGhoa2L79CM3NiWRmVpKWFvqum3V1e1myxM/q1ctCfuyZxOfznT+j7+62hXxLiz27\nHxiIBVKAVGJiUklMTCExMZWEhOSrKrwHB7309Xno7e1mYMCNiBvbfNZDVpa9fM/NTSUjI5XUVHsF\nMZU3H/H5fHi9XgYGBvB6vRc97u/30tMzQG+vl54eL729A/T1eent9ZKYGMuqVXMpKyud0uR17tw5\nnntuLzk5ayJ6RtDpZPPmEHURDVTnvGiMWTTCa5uALwUahlcB/2SMWRV47Smg1RjzV+M4RtiSQFNT\nE9u3H6a+PpaMjMqw9lX2+XzU1b3Ppk1lzJ5dHrY4pqve3l6OHTvFnj219PUlAvasPiEhlcREW+CP\nd1DfRPn9fvr7e+jrc9Pb2429ALZLbKyfmBghJiYm8NMuIpc/HvoZGxsz7LEAQkwM9Pf7LirM/X4B\nHIg4gHjAgTF2iYmJJzbWQVycg7i4ix/39XlwuY7gdHZz443zKCkpCXo1TUdHB889t4Pk5BURc1OY\nmSAkSUBEngaqgWxsPf8j2P8wY4x5IrDN94ENXOgiukdE1gDvAjWACSzfMMa8OspxQp4Ezp07x86d\nRzh92k9aWiVZWfkhPf5o7JfyfR58cAVZWfqFGQ+Xy8XBgyf55JMWoITs7IqInHrA5/MBBmPsMtJj\nv98/5jbGGGJj4y4qzCd7Ft/d3Y7LdZiCgj5WrZrPrFmzgpIMPB4Pzz//e3y+RWRlTV3bWjQK2ZVA\nKIQyCbS1tbFr12FOnBggOXk+TmdhxDVQuVxNxMd/wv3336R3JBuF3++nsbGRvXtPcvr0AAkJs8nJ\nKZmWM09Gkq6uVjo7DzNr1iCrVlVOaoTrwMAAL730Ph0dc8jNvVInQzURmgSuksvlYvfuIxw+7CE5\neT7Z2UURV/gP19h4mDlzXNx666qgxmmMobOzE4/Hc/750Gc//Odoj0f6mZ6ejtPpDMkNtwcGBjh9\n+iy7dp2ioyOF1NTZZGbmR/TfcjpyuZrp7j5MWZmwYkXlVd8xy+fz8frrH3L2bA6FhZVTFGV00yQw\nTp2dnezde4SDB7tISJhHdnbxtOi9YQeSbae6Oovrrpvcl8jj8dDa2sqZMy2cPNlKX18iImnY/5+h\n/6ELj40hUKjKRY8vbMewbQ3QSUxMB3l5yZSXZ1NQkE12dnZQr2LcbjdHjpxk794GvN4CsrJmR+Q9\nZ2cSYwwuVxMez2HmzInn+usryc7OHtf73n9/FzU1cZSULA1BpNFJk8AY3G43e/cepqamHYdjLrm5\nZdOi8B/O6+2nsfFdHnhg0VVdlg8MDNDa2kp9vS30XS4/xuSQmJhLenrOlHR7tf3sO+nubsPvbwPa\nyctLoqzMeT4pJCZe/XFbWlqoqTnJkSOdxMSUkZtbjsOhcy2FkjGGtrZ6enuPMn9+MsuWzb9ie5W9\nM5iH4uIV0+47N51oEhiFx+OhpuYoe/eeIzZ2Drm5FdO6T7Lb7aK//yMefHDtqKMEfT4f7e3tNDXZ\nQr+x0QNkExubS0ZGLklJoR+AZoyhp6eL7u42fL42jGkjOzueiopsCguzcTqdo86xbntJ1bFr10ma\nm2NITJxNdnaRFihh5vf7aWuro7//KAsWpLN0aSXp6RdfjR05cozXX2+guHiNts9MMU0CIzhw4Cgf\nfHAKqCAvb/aM+Sdsbj5Ffv5ZNm1aS2xsLMYYurq6OHeuhZMnW6it7WBwMJ2YmFxSU3NIScmMuALT\nGENvbzddXW34fO0Y00ZmZgzl5dkUFdkrhdjYWE6cOM2uXWfweJxkZFSM60Y9KrT8fj8tLWfweo+x\neHE2VVXzSU1Npba2jq1bD1NQsHZa3Bx+utMkMILnntvGwMASUlODe/OGSFBbu5tFi7wYE8upU630\n9iYCOSQn55Kenj0tE15vr5vu7ja8XnulIDLIUBdPHVAU+Xw+Hy0tp/D5TnDddU4OHnSRkbF6xOkP\nVPAFIwlMv1JjHCLtDDhYZs1azIEDx0hISCM9/Tqczul/ppWUlBqoqrLdB40x2stnGomNjaWg4Bp8\nvnIOHjxFauocTQDTzIxMAjNVbGwcRUXXhjuMKaUJYHqKjY2jsHBuuMNQEzAzT5mVUkqNiyYBpZSK\nYpoElFIqimkSUEqpKKZJQCmlopgmAaWUimKaBJRSKoppElBKqSimSUAppaKYJgGllIpimgSUUiqK\naRJQSqkopklAKaWimCYBpZSKYpoElFIqio2ZBETkRyLSLCL7r7DN4yJyTET2isjSq3mvUkqp8BnP\nlcCTwB2jvSgiG4E5xpi5wMPA/x7ve5VSSoXXmEnAGPM+4LrCJvcATwW23QFkiEj+ON+rlFIqjILR\nJlAE1A57Xh9Yp5RSKsJF1D2Gt2zZcv5xdXU11dXVYYtFKaUiTU3NNmpqtgV1n8FIAvVAybDnxYF1\nV214ElBKKXWxRYuqWbSo+vzzZ575h0nvc7zVQRJYRrIV+GMAEVkFdBhjmsf5XqWUUmE05pWAiDwN\nVAPZInIWeASIB4wx5gljzMsisklEjgMe4AtXeq8x5sng/xpKKaUmYswkYIz57Di2+fJE36uUUip8\ndMSwUkpFMU0CSikVxTQJKKVUFNMkoJRSUUyTgFJKRTFNAkopFcU0CSilVBTTJKCUUlFMk4BSSkUx\nTQJKKRXFNAkopVQU0ySglFJRTJOAUkpFMU0CSikVxTQJKKVUFNMkoJRSUUyTgFJKRTFNAlPsxAn4\nwQ/A7w93JEopdbkxby+pJm7XLvinf4LERLjuOli7NtwRKaXUxTQJTJFXX4Wf/xz+7u+gpwf+5V9g\n9WqIjQ13ZEopdYEmgSDz++H//T/44AP49rdh1iwwBrKy4K234Pbbwx2hUkpdoG0CQeT1wv/8n/DJ\nJ/Df/ptNAAAi8Md/DM88AwMD4Y1RKaWG0yQQJG43PPKITQTf/Cakp1/8emUlVFTAK6+EJz6llBrJ\nmElARH4kIs0isv8K2zwuIsdEZK+ILBm2foOIHBaRoyLytWAFHWmam+FrX4M5c+Bv/gYSEkbe7nOf\ng+ees20ESikVCcZzJfAkcMdoL4rIRmCOMWYu8DDwg8D6GOD7gfcuBD4jIpWTjjjCHD9uE8CGDfDF\nL1654beiAqqqYOvW0MWnlFJXMmYSMMa8D7iusMk9wFOBbXcAGSKSD6wAjhljzhhjvMAzgW1njJ07\nYcsWePhhuPvu8b3ns5+FF1+Erq4pDU2paeuFF+C//3d4+WU4dUrH2Ey1YPQOKgJqhz2vC6wbaf2K\nIBwvIrzyim3o/bu/s/X941VYCGvWwK9+BX/6p1MXn1LT0bPPwjvvwL33wuHD9qq5sxOuvRYWLLDL\nNddAfHy4I505pqKLqEz0jVu2bDn/uLq6murq6iCEE1x+P/z0p/Dhhxe6gF6tP/xD+Mu/hM2bIScn\n+DEqNR398pc2ATz6qO1SvX69Xe9ywaFDcPAg/J//A3V1MHu2TQwLF9qTsNTU8MYeKjU126ip2RbU\nfYoxZuyNRMqAF40xi0d47QfA28aYZwPPDwM3AxXAFmPMhsD6rwPGGPPdUY5hxhPLWJ57bhuDg8tI\nTk4fe+Or5PXaEcAtLfYK4NIeQFfj//5f8HjgS18KWnhKTVvPPQdvvGETgNN55W17euDoUZsUDh6E\nY8cgP//ClcKCBdFzcrV5s2CMmfCJN4z/SkAY/Qx/K/Al4FkRWQV0GGOaRaQVuCaQQBqBh4DPTCbY\ncOruhv/6XyEjw3YBHa0H0Hg98AD8xV/AffdN7GpCqZni17+G118fXwIASE6GJUvsAjA4CCdPwoED\n8P778MQTdqqWoYSwZAkUFEzt7zCdjZkERORpoBrIFpGzwCNAPPas/gljzMsisklEjgMe4AvYF30i\n8mXgNWwD9I+MMYem6PeYUk1N8F/+C1x/PXz+8xAThNEVaWm2OuhnP4O//uvJ70+p6eg3v4Hf/c6e\nYGVnT2wfcXEwb55d7rvPjtCvr7dXCZ98Yr9jqamwfLldFi7UNoXhxlUdFAqRWh107Jg9Q/mDP4C7\n7grKLs/r7bU9i7ZssXWcSkWTF16A3/7WJoCprL7x++2Vwscf2+XMGZsIli2zSaGwcOqOPdWCUR2k\nSeAKdu6Exx6DL38ZVq2a9O5G9OKLsGcP/Of/PDX7VyoSvfiiXR59FHJzQ3vs7m7Yuxd277ZLUpJN\nBsuW2dl+J1vVG0qhbBOIKl4vvPQSPP88/P3fw/z5U3esDRvsGdHBg7b+UqmZ7qWXbNfPcCQAsFWx\n69bZxe+3YxE+/tj2Tvre92yvo6Gqo2hor9MrgWF8Pti2zU4BXVQE/+bfhOZS8c03bcPYt79tJ5tT\naqZ6+WXbEPzoo7ZHT6Rxu2HfPpsUdu+2bQdD1UaLF0feVYJWB41gIknA74f33rODvzIz4V/9K1tn\nGCo+H3zlK3bw2PLloTuuUqH0yit2kOSjj06P3jrGwOnTNhkMtSWsX2/bBifaiB0sp07ZGoS33tLq\noEkxBrZvh6efthn+4Yft3D6hPhuPjbWTyz31FCxdGpzeR0pFkldfnV4JAGw5UFFhlwcesL0Et261\nAz1XrIB77rGvhYrfbxPSCy9AbS3ceWdw9huVVwLG2Mz+s5/Zx5/7nO3+Gc6qGGPgP/5HO1x+3brw\nxaFUsL32mr3KfvTR6d0TZ0h3t01qv/0tlJbabqlLlkxd+dHfD2+/bRNQfLxNPmvXgsOhDcMTsm+f\nLfw9Hlv4r1oVGWfeIvBHf2RvSr96te37rNR09/rrNgF861szIwGAbVh+8EF7wvbuu/DjH9v1994L\nN91kC+dgcLlsovnd72znlL/4C9t7KdjJJmqKmoMHbeHf2mpn8ly7NvLu91tVZesa33wT7hh18m6l\npoc337RVrd/61szsZeNwwG23wa232i6nv/mNnVfszjth48aJz2d06pQ969++HW6+Gb7zHdtRZarM\n+CRw7Jgt/Ovq4KGH4JZbIq/wHzJ0NfC970F1deT1RFAT199ve55kZUXGledUe+stWyB+61tTW4BF\nAhHblrd06YUG2z//c/sd3rx5fG0gfr8dL/T88xfq+3/4w8nNTzZeMzYJnDplz0KOH7eXbn/7t8G7\nTJtKlZX2DmWvvGIvL9XV8/uhr8/+vePipq6u1uu10xy7XNDRcfHPS9cNDkJKir3H9DXXXJjmYO7c\n0PY08XptIXPihF1On7bz7BQVXViKi+0cPhP93N5+23Zy+OY37b6iSUUF/Lt/B21tdjzEf/gPsGiR\n/S6PNOX88Pp+h8NuN1TfHyozrmH48cc/4plnlnLokIMHHrCDsabbGfWZM3aW0h/+0E6WpSxjbFtO\ne7v9krW3j7y4XLbwHxy0CcHhsA1qCQn259Ay9Hz4+pG2GRgYuYDv6bETCmZm2jP8oZ/DHw/9TE62\nhWpXl706PXLE/jx61B5j7twLiWHOnOD83fv77f/SUIF/8iScPWv758+ebY9TUWETZn09NDTYK+b6\nevs7z5p1cXIoKrLrkpJGP+Y778CTT9q5tkpLJ/87THe9vXZ21BdesMn+3nttz6KuLjtm4tVXbX3/\n5s02WVxt4tVxApf43e/g058e4M47/dx7b+IV/1kj3T/+o/2yfvaz4Y4kdM6dg8bGKxfuDoc9Sx1p\nycq68HhogjCfzxZoQ0t//+WPR1o3/PnQMTMzLy7o09ImX7VjjO16ePSoXY4ds1exBQUXJ4aysitX\nY/b22vcNFfgnTtjPsqjIFvZDS3m5PfMfi9ttk8GlyaGx0f7elyaHoiKb2H78Y3sFoAngYj6fvQfJ\n88/b/+OeHtuIfPfdk7ta0iRwib4++MUv3iMhoWpK7icQSk1N9lLyn//Znm3ORC4X7N9/Yenrg5KS\n0Qt5p3N8Bdh05/XaM/jhiaGlxZ69DyWGtLSLC/3WVpsohs7w58yxz4NdreD321guTQ719fa1LVts\nolEjM8ZekeXmBqe+X5PACKbypjKh9oMf2C/xF78Y7kiCw+22U/vu32+76ra32y5vixfbnlElJTpt\nxmg8Htu+NZQYurttQT9U6BcXh79bsTH69ws1HScww3360xduQxmOibYmq6/P3hZw3z5b8NfX2/rP\nqirbeDZ7duT21Io0KSn2c6uqCncko9MEMD1pEohgTqedq+SZZ2wyiHRer626GCr0T5ywBf3ixXZe\npPnzp0cPLaWiiSaBCPfAA3Y20/vuC013O2Nsrxqv1y4DA/bn0Lqh58OXlhZb6B86ZHuPLF5sb8Kz\nYMGVe5IopcJPk0CES021c4U8/TT8zd9MbB/G2EbY06dtY+Lp07arYG/v5QX+4KCtW3Y4Ll+G1sfH\nX7wuK8tesfzVX4VmcItSKng0CUwDd99trwZOnLCNgFfi9doeG6dOXSjwT5+2PTeGZkRcssS2M6Sk\njFzQR8OIVqWUpUlgGkhMtKOef/pT2wVvyNDZ/dAZ/qlTth93fr4t7MvL7eCU8vLJjQBVSs1cmgSm\nifXr7QRV3/++rYM/dcpW3QwV9osX27P70tILA6WUUmosmgSmCYfD3m9g/35YudIW/tnZenavlJoc\nTQLTSGXlyJNQKaXURI2rCVBENojIYRE5KiJfG+H1TBH5tYjsE5HtIrJg2GtfFZGawPKVYAavlFJq\ncsZMAiISA3wfuANYCHxGRC49H/0GsMcYUwX8CfB44L0LgS8C1wNLgLtEZHbwwldKKTUZ47kSWAEc\nM8acMcZ4gWeAey7ZZgHwFoAx5ghQLiK5wLXADmNMvzHGB7wL3B+06JVSSk3KeJJAEVA77HldYN1w\n+wgU7iKyAigFioFPgHUikiUiycAmoGSyQSullAqOYDUMfwd4TER2AzXAHsBnjDksIt8FXgfcQ+tH\n28mWYZ3gq6urqa6uDlJ4Sik1/dXUbKOmZltQ9znmVNIisgrYYozZEHj+dcAYY757hfecAhYZY9yX\nrH8UqDXG/GCE9+hU0kopdRWCMZX0eKqDdgLXiEiZiMQDDwFbh28gIhki4gg8/jPgnaEEEGgbQERK\ngfuApycTsFJKqeAZszrIGOMTkS8Dr2GTxo+MMYdE5GH7snkC2wD8ExHxAwewPYKGPCciTsAL/Ftj\nTFfQfwullFITMq42AWPMq8D8S9b9cNjj7Ze+Puy1myYToFJKqamj80UqpVQU0ySglFJRTJOAUkpF\nMU0CSikVxTQJKKVUFNMkoJRSUUyTgFJKRTFNAkopFcU0CSilVBTTJKCUUlFMk4BSSkUxTQJKKRXF\nNAkopVQU0ySglFJRTJOAUkpFMU0CSikVxTQJKKVUFNMkoJRSUUyTgFJKRTFNAkopFcU0CSilVBTT\nJBACfr8fY0y4w1BKqctoEphiXm8/Z89uo7n5ZLhDUUqpy4wrCYjIBhE5LCJHReRrI7yeKSK/FpF9\nIrJdRBYMe+3fi8gnIrJfRH4mIvHB/AUimdc7QEPDh6xcmYHXe0avBpRSEWfMJCAiMcD3gTuAhcBn\nRKTyks2+AewxxlQBfwI8HnjvLOAvgWXGmMVAHPBQ8MKPXIODXhoatnPzzfmsWLGcWbNi6e5uC3dY\nSil1kfECZfroAAAXYElEQVRcCawAjhljzhhjvMAzwD2XbLMAeAvAGHMEKBeR3MBrsUCKiMQByUBD\nUCKPYD7fIHV1O1izxsmiRdcCUFVVRlfXmTBHppRSFxtPEigCaoc9rwusG24fcD+AiKwASoFiY0wD\n8D+As0A90GGMeWOyQUcyn89Hff1OVq9OY8mShefXFxcXER/fgtc7EMbolFLqYnFB2s93gMdEZDdQ\nA+wBfCKSib1qKAM6gV+JyGeNMU+PtJMtW7acf1xdXU11dXWQwgsNv99Pff0uli1LYPnyxYjI+dcc\nDgfXXVfA/v21FBTMCWOUSqnpqqZmGzU124K6z/EkgXrsmf2Q4sC684wx3cCfDj0XkZPASWADcNIY\n0x5Y/2vgRmDMJDDdGGOoq/uYxYtjWLlyyUUJYMi8eWV8/PEeQJOAUurqLVpUzaJF1eefP/PMP0x6\nn+OpDtoJXCMiZYGePQ8BW4dvICIZIuIIPP4z4F1jjBtbDbRKRBLFloq3AYcmHXWEsQlgDwsX+lmz\nZjkxMSN/rFlZWRQWxtDV1RriCJVSamRjJgFjjA/4MvAacAB4xhhzSEQeFpE/D2x2LfCJiBzC9iL6\nauC9HwG/wlYP7QMEeCLov0UYGWOor9/P/Pn9rFt3/agJYIhtID4bouiUUurKxtUmYIx5FZh/ybof\nDnu8/dLXh732D8Dkr1kiVEPDAWbP7ubmm1cRGxs75vYlJcU4HEfwegdwOKJmyIRSKkLpiOFJaGg4\nRElJO7feupK4uPG1sTscDhYtKqCtrXbsjZVSaoppEpigpqajFBQ0c/vtq3A4HFf13rlzS/F6dcyA\nUir8NAlMQHPzCbKz69iwYTXx8VdfpeN0OgMNxDqCWKmR+Hy+cIcQNTQJXKWWltNkZp5m06YbSUhI\nmPB+dASxUiM7d+4kZ868itvdEe5QooImgavQ2lpLcvJxNm1aTWJi4qT2ZRuIz+kIYqUCjDE0NBwg\nO/sMd999Le3tu/H5BsMd1oynSWCc2toaSEg4zF13rSI5OXnS+7MjiPNpb68LQnRKTW9+v5/a2t1U\nVHRw551rmT17Njfc4KSx8UC4Q5vxNAmMg8vVRGzsJ9x110pSU1ODtt9588oYGNAqIRXdBge91NZu\np6rK8KlPrT7f0WLZsuvIy2ujvb0xzBHObJoExtDZ2YLfv4+7715Benp6UPftdDrJz0cbiFXUGhjo\no77+A9asSb9stH1cXBy33rqUvr4aBgb6whjlzKZJ4Aq6utoYGNjN5s03kJmZOSXHWLJEG4hVdOrp\n6aa5+X3uuKOEpUuvG3G+raysLG66qZympj16U6YpoklgBH6/n5aWM/T27mLz5uU4nc4pO1ZpaYk2\nEKuo09XVRmfnh9xzz7XMnXvlCRUrK+cyd66fc+f0Fq1TQZPAMMYYWlpqqa9/m7KyRh58cBU5OTlT\nekxtIFbRpq2tgYGBXdx//zKKii69NcnlRIS1a5cSG3scj6czBBFGl2DdT2BaM8bQ3t5AT88R5s5N\nZPnypVN69n+pefPK2L17HzA7ZMdUKhzOnTtJcvIJ7rxz9VW1sSUnJ7N+/XU8//xuEhNvGtc8XWp8\nojoJGGNwuZrweI5QURHHDTcsnvIz/5EMNRB3d7eTlha65KNUqBhjaGw8SH7+Oe64Yy1JSUlXvY+i\noiKWLWtm376DFBUtmoIoo1PUJgGXq5nu7sOUlQkbNy4gLy8vrPEsWVLGa6+d0SSgZhy/309d3R7m\nzu3jllvWXvVcW8Ndf/0i6urexeVqIiurIIhRTj/BGkgXdUmgs7OFrq7DFBX5Wb9+PgUFkfGPZEcQ\nH2Vw0Etc3MS/JEpFksFBL/X1O1m6NJ7Vq1ePeb+NsTgcDm67bSnPPruLlJRM4uMnN3J/uvL5fNTW\n7gjKvqImCXR1tdHRcZjCwgFuvXU+hYWFI3ZJC5f4+HgWLMjj4MFa8vO1bUBNf/39vTQ17WDt2lyq\nqhYE7fvmdDq5+eYy3n57LyUlKyPqexwKPp+PurqPWLkyJSj7m/FJoLu7nY6OI+Tl9XLPPfMoKiqK\n2H+a+fPL2Lu3Bm0gVtNdT08XbW0fcccdFWN2AZ2Iysq5nD37e+rrT5GXFz3fF7/fT339LpYvT+CG\nG6qCss8ZmwTc7g5criM4nd3cddc8iouLJ30pOtWys7PJzzfaQKymta6uNjyej7nnnoXj6gI6ETEx\nMaxbt5Rf/OJ9enpySE4O7mj+SGTbVj6mqiqWlSuXBO1kNrJLxQlqatqPMTvZuDGfT3/6VkpLSyM+\nAQxZsqSMzk4dQaymp7a2erze8Y8BmIyUlBRuu20BLS27Z/z9B4wx1NXtYdEiw403LgtqeTbjrgTm\nzMllyZIkysvLpmVf4mhuIPZ6B+jqaiElJZPExODUd6qp19/fi8fTgcfTitPZxKZNVzcGYDJKS0tY\ntuwc+/cfoqjoupAcM9RsAtjLggVe1q5dEfQTWomU+ThExERKLOH2+9/v5uDBLPLzK8IdSsi43S7a\n2z+msjKVc+fcdHT4EXESG+skNdVJSkpGxLblRBOfbzBQ4Lvw+zswxkVqKhQXZ1JUlEVpacmk77Vx\ntbxeL8899w5e72IyM8Pb1Xsq1NXtY+5cD7fcsvKyE1sRwRgzqS+GJoEI1NbWxrPP1lBSUh3uUELi\n3LmTOBzHWb9+8fkuu729vbS3t9PU1MaZM+20tPRiTBYxMU5SUpykpGRNuyu9/v4e/H4/SUnBm458\nKvn9fnp7u/F4OvB6XYCLuLheZs3KoKgok9zcLDIzM4Nyf43Jamtr45e/3E1e3k04HBO/41+kqa//\nhIqKDm67bRVxcZdX3GgSmMF+9au3GBhYMqMbiH2+Qerr91Je3sMtt1x/xcLE6/XS3t5OS0s7Z860\n09DQic+XBjhJTs4mNTUrIr/8AwN9uFwNeL31pKT0EBsrdHamkJxcTlZWYUS1VfX39+B2u+jr60Ck\nA2M6yctLpqgok4ICW+CnpaVFVMzD1dQc4r33uikpWRHuUIKioeEgJSVt3H77qlEH2IUsCYjIBuCf\nsA3JPzLGfPeS1zOBHwNzgF7gT40xB0VkHvAsYADB9n38e2PM4yMcQ5PAMMePn+CNN7opKloS7lCm\nRE9PF62tu1i1KoelS6+76oLF5/PR0dFBW1s7tbU2MQwMJGKMk4QEJ2lpzrC1K3i9/bS3NzIwUE9y\ncjcLFxZSXj6LnJwcjDE0NTVx4MAZjh/vJiamlOzsUhISQn82bRNUE4OD5zDGRXp6DEVFtlonKyuT\nzMzMEc8+I5Xf7+eVV96nqamUvLzycIczKY2NRygsbGLDhhuvOMI6JElARGKAo8BtQAOwE3jIGHN4\n2DbfA7qNMd8UkfnA/zLGfGqE/dQBK40xtSMcR5PAMAMDA/zkJ2+Rk3PbjGsgbmk5CxzijjuuC1oP\nEmMM3d3dtLW1UV9vk0JXl0Eki5iYLFJSskhJyZyyKqTBQS8uVyP9/Q0kJHRQWZnHnDlF5Obmjprg\n3G43x4+fYe/eWnp6nKSllZORkTulbR/9/T24XI0MDjaSlOSmsjKf0tJ8nE5nyOvyp4LH4+GZZ94n\nNfVGkpPTwh3OhDQ1HSMvr56NG28kPj7+ituGKgmsAh4xxmwMPP86YIZfDYjIS8C3jTEfBJ4fB1Yb\nY1qGbbMeexWwbpTjaBK4xExrIPb5fDQ21lBY6OK2264nLW1qv6RD7QqtrR2cPdtOU1M3Pl8qxmSR\nlOQkNTVrUmfgPt8gHR3N9PTU43C0UVmZy5w5s8jPz7+qZOPz+aivr2ffvtPU1XmJiysjO7skaNVb\nPT3ddHY24vM1kpbWz7XXFlBaWkh2dnbEVu1MxpkzZ3nxxVOUlKybdr9fc/MJnM4zbNp047iScjCS\nwHiu9YqA4WfudcCllW77gPuBD0RkBVAKFAMtw7b5Q+DnEw81+sybV8q+fQeA6Z8E+vo8NDfvYvny\nNG64YV1IqhmSkpIoKiqiqKiIqipb2HZ2duJyuaivb6S29iAtLYaYmCxExne1YPdxjt7eBmJiznHN\nNU7mzSuioGDZhH+n2NhYSktLKS0tpaOjg6NHT1NT8zYDA3lkZJRPqF3I4+mks7MRv7+RrCwfK1YU\nUlJyHU6nc8b3siorK6WqqpmDBw8za9aCcIczbi0tp8nMPM2mTWtCelUWrG/id4DHRGQ3UAPsAc6P\n3hARB7AZ+PqVdrJly5bzj6urq6murg5SeNNTTk4Oubk+3G4XqalZ4Q5nwuxNRGq4665KysvLwhZH\nbGwsTqcTp9PJnMBMBr29vbhcLlpaXJw9e4impq7zVwuJiVmkpmYRH59EV1crbnc9MTHNVFRkUFlZ\nREHBojEv169WZmYmK1YsYelSL2fP1rJnz15qa2OIjy8nJ6eY2NiRv7LGGNxuF11djRjTSE5ODGvW\nFFJcvHTKbo0ayVaurKKu7h06O3PJyMgNdzhjamk5S0rK8TGvALZt28a2bduCeuzxVgdtMcZsCDy/\nrDpohPecAhYZY9yB55uBfzu0j1Heo9VBI5jODcR+v5+mpkM4nU2sX389GRkZ4Q5pTH6///zVQl1d\nO7W1Lrq7Bygry6SychazZs0iISG0vZBaW1s5fPg0Bw+24vPNwuksJzk5Hb/fT3d3G93djUAThYUJ\nVFYWUlRUOOVVbdNBS0sLv/zlXgoKbsbhCG6yDqa2tjoSEg6xefONpKRcXWeGULUJxAJHsA3DjcBH\nwGeMMYeGbZMB9BhjvCLyZ8AaY8znh73+c+BVY8xPrnAcTQIj6O/v56mn3p52DcR2BsmPqapKYNWq\nJZOaQz7cfD5fRIxJ6Ovr4/Tps+zZc4aOjgREeiguTqWyspDCwoKrLkCiQU3NIT744DSQcn6Jj08h\nMdEu4e5W3NbWgMNxgM2bV00ocYe6i+hjXOgi+h0ReRh7RfBE4GrhJ4AfOAB80RjTGXhvMnAGmG2M\n6b7CMTQJjOKDDz7m0KFs8vPLwx3KuHR0nMPj2cttt82Zkhkko50xhra2NlJTU2dEj56pNjAwgMfj\noaenB7fbQ1ubXdrbPfT2+oFkhieIhIQLCWIq209criZgP/feu2rC02zoYLEo0drayi9+cYCSkpvD\nHcoV2VsIHiE9vZY77lge0vs0KzURXq83kBzceDw9tLdfSBAezyDDE4TDkUJsbBwxMbGIxBATEzPs\ncSwxMTHnHw+9PloS6eg4h8+3h/vuWzWpatJQ9Q5SYZadnR3xDcRebz8NDbuprIR1624Keb25UhPh\ncDjIyMgYsSAeHBwcliA8uFwd9PcPMjjox+v14fP5L3rs9foZHPQxOGh/+nx+bOVJDMbEYGvW7c/E\nxD7uuWdFRLST6ZXANHHs2HHeeMNDcXFwbiQxGX6/H7/fh883iM83SH+/B7e7hptvLmHBgvkzvgui\nUuNlvyt+fD7fRY/j4+ODUpWnVwJRpLS0hLi4txkcXDCpBuLBQS8DA314vX0MDnrx+21B7vf7MGYQ\nsIvIILaXr31+4TUfMTGG+Pg44uPjcDhiyc52sGFDFXl5M28GR6Umw1YZxUT09BuRG5m6SEJCAgsW\n5HLoUP1lDcTGGAYHBwKFez9er/1pTB/Qh8jQ434SEmJJTU0gNzeR5OR4EhJsQZ6YGIfD4SA2NpG4\nuLjzS2xs7EXP4+Lipt0oTKXU6LQ6aBppaWnh2Wf3ExeXg0g/0Icx/UA/SUkO0tISSU1NID09kfT0\nRJKSEkhMTCQh4cLPSOjqqJQKDu0dFGWMMZw6dYrY2NiLCvaEhAQ9O1cqCmkSUEqpKBaMJKCnj0op\nFcU0CSilVBTTJKCUUlFMk4BSSkUxTQJKKRXFNAkopVQU0ySglFJRTJOAUkpFMU0CSikVxTQJKKVU\nFNMkoJRSUUyTgFJKRTFNAkopFcU0CSilVBTTJKCUUlFsXElARDaIyGEROSoiXxvh9UwR+bWI7BOR\n7SKyYNhrGSLySxE5JCIHRGRlMH8BpZRSEzdmEhCRGOD7wB3AQuAzIlJ5yWbfAPYYY6qAPwEeH/ba\nY8DLxphrgSrgUDACD4Vt27aFO4TLaEzjE4kxQWTGpTGNTyTGFAzjuRJYARwzxpwxxniBZ4B7Ltlm\nAfAWgDHmCFAuIrkikg6sM8Y8GXht0BjTFbzwp1Yk/tE1pvGJxJggMuPSmMYnEmMKhvEkgSKgdtjz\nusC64fYB9wOIyAqgFCgGKoBWEXlSRHaLyBMikjT5sJVSSgVDsBqGvwNkichu4EvAHsAHxAHLgP9l\njFkG9ABfD9IxlVJKTdKYN5oXkVXAFmPMhsDzrwPGGPPdK7znFLAISAE+NMbMDqxfC3zNGHP3CO/R\nu8wrpdRVmuyN5uPGsc1O4BoRKQMagYeAzwzfQEQygB5jjFdE/gx4xxjjBtwiUisi84wxR4HbgINT\n8YsopZS6emMmAWOMT0S+DLyGrT76kTHmkIg8bF82TwDXAj8RET9wAPjisF18BfiZiDiAk8AXgv1L\nKKWUmpgxq4OUUkrNXCEfMSwiPxKRZhHZP2xdloi8JiJHROR3geqlUMZULCJvBQaz1YjIV8Idl4gk\niMgOEdkTiOmRcMc0LLaYQG+vrREU0+nAYMU9IvJRJMQ10kDJMP9PzQt8PrsDPztF5CsR8Dn9exH5\nRET2i8jPRCQ+AmL6auB7F9by4GrLSxH5TyJyLPA/t348xwjHtBFPYgeeDfd14A1jzHzseIP/FOKY\nBoG/MsYsBFYDXwoMiAtbXMaYfuAWY8xSYAmwMdD9NtyfFcBXubhtJxJi8gPVxpilxpgVERLXpQMl\nD4czJmPM0cDnswxYDniA34QzJhGZBfwlsMwYsxhbRf2ZMMe0EFulfT32u3eXiMwJU0zjLi/FztTw\naWz1/Ebgn0Vk7LZWY0zIF6AM2D/s+WEgP/C4ADgcjriGxfM88KlIiQtIBnYBN4Q7Juz4j9eBamBr\npPz9gFNA9iXrwhYXkA6cGGF92D+rwLHXA++FOyZgFnAGyMImgK3h/u4BfwD8y7Dnfwf8NXa2g5DH\nNN7yEpscvjZsu1eAlWPtP1ImkMszxjQDGGOagLxwBSIi5djsvx37QYctrkC1yx6gCXjdGLMz3DEB\n/4j9QgxvTAp3TATieV1EdorIv46AuEYaKJkc5piG+0Pg6cDjsMVkjGkA/gdwFqgHOo0xb4QzJuAT\nYF2g2iUZ2ASUhDmm4UYrLy8d2FvP5QN7LxMpSeBSYWmtFpFU4FfAV43t4nppHCGNyxjjN7Y6qBhY\nEbhMDVtMInIn0GyM2Qtc6TIzHH+/NcZWc2zCVuetGyGOUMZ16UBJD/ZMLaz/UwCBnnqbgV+OEkMo\n/6cysdPQlGGvClJE5HPhjMkYcxj4LvaK92UuDH69bNNQxTSGScURKUmgWUTyAUSkADgX6gBEJA6b\nAH5qjHkhUuICMHa+pW3AhjDHtAbYLCIngZ8Dt4rIT4GmcH9OxpjGwM8WbHXeCsL7WdUBtcaYXYHn\nz2GTQiT8T20EPjbGtAaehzOmTwEnjTHtxhgfto3ixjDHhDHmSWPM9caYaqADOBLumIYZLY567BXL\nkOLAuisKVxIQLj6T3Ap8PvD4T4AXLn1DCPwYOGiMeWzYurDFJSI5Q63+Yudbuh1bJxm2mIwx3zDG\nlBo7Avwh4C1jzB8BL4YrJgARSQ5cxSEiKdj67hrC+1k1A7UiMi+w6jbsGJpI+F//DDaJDwlnTGeB\nVSKSGGjEHBpQGtbPSURyAz9LgfuwVWfhimm85eVW4KFA76oK4BrgozH3HqrGlmGNFU8DDUA/9h/g\nC9hGoTew2fY1IDPEMa3BXu7txV767caedTvDFRd22o3dgZj2A38bWB+2mC6J72YuNAyHNSZs/fvQ\n364G+HqExFWFHXG/F/g1kBEBMSUDLUDasHXhjukR7AnOfuAngCMCYnoX2zawB9vrLCyf09WWl9ie\nQscDn+f68RxDB4sppVQUi5Q2AaWUUmGgSUAppaKYJgGllIpimgSUUiqKaRJQSqkopklAKaWimCYB\npZSKYpoElFIqiv1/XRn3AdGcjBIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f2f0cbfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 30\n"
     ]
    }
   ],
   "source": [
    "print(sizes)\n",
    "loss = (np.mean(score_loss,axis=1))\n",
    "loss_std = (np.std(score_loss,axis=1))\n",
    "plt.plot(sizes,loss)\n",
    "plt.fill_between(sizes,loss-loss_std,loss+loss_std,alpha=0.3)\n",
    "plt.show()\n",
    "print(\"max: {}\".format(sizes[np.argmin(loss,axis=0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def futureTest2(node_sizes, X,y,ori_dates, numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(ori_dates)\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    model = createModel(node_sizes,X.shape[1])\n",
    "    results = None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            print(\"week{}\".format(w))\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "            history = model.fit(X_train,y_train,verbose=0,nb_epoch=500, validation_split=0.1, callbacks=[earlyCallback])\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = model.predict_proba(X_test)\n",
    "            train_proba =model.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            tresult = np.hstack([np.array([w for i in range(proba.shape[0])]).reshape(proba.shape[0],1),\n",
    "                                 ori_dates[start:last].reshape(proba.shape[0],1),stack,proba,y_test])\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "                results =    tresult\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "                results =  np.vstack([results, tresult])\n",
    "            if verbose == True:\n",
    "                print(\"numOftest {} , loss {}\".format(X_test.shape[0],model.evaluate(X_test,y_test)))               \n",
    "                print (tresult)\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "    start = X.shape[0] +index+1\n",
    "    print(\"start compute precision_mat\")\n",
    "    print(X[0:start,:].shape)\n",
    "    print(y[0:start,:].shape)\n",
    "    _,_, proba_test,proba_y = crossValidate2(node_sizes,X[0:start,:],y[0:start,:],fold=10)\n",
    "    p_matrix = precisionMatrix(np.vstack(proba_test),np.vstack(proba_y))\n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    resultdf= pd.DataFrame(results, columns=['week','DayStamp','HomeTeam','AwayTeam','H_prob','D_prob','A_prob','H','D','A'])\n",
    "    return sum_proba, sum_y,resultdf,p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week0\n",
      "Epoch 00117: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1348/1348 [==============================] - 0s     \n",
      "week1\n",
      "Epoch 00023: early stopping\n",
      "5/5 [==============================] - 0s\n",
      "1343/1343 [==============================] - 0s     \n",
      "week2\n",
      "Epoch 00022: early stopping\n",
      "15/15 [==============================] - 0s\n",
      "1328/1328 [==============================] - 0s     \n",
      "week3\n",
      "Epoch 00024: early stopping\n",
      "13/13 [==============================] - 0s\n",
      "1315/1315 [==============================] - 0s     \n",
      "week4\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1305/1305 [==============================] - 0s     \n",
      "week5\n",
      "Epoch 00021: early stopping\n",
      "12/12 [==============================] - 0s\n",
      "1293/1293 [==============================] - 0s     \n",
      "week6\n",
      "Epoch 00022: early stopping\n",
      "8/8 [==============================] - 0s\n",
      "1285/1285 [==============================] - 0s     \n",
      "week7\n",
      "Epoch 00035: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1275/1275 [==============================] - 0s     \n",
      "week8\n",
      "Epoch 00021: early stopping\n",
      "17/17 [==============================] - 0s\n",
      "1258/1258 [==============================] - 0s     \n",
      "week9\n",
      "Epoch 00021: early stopping\n",
      "3/3 [==============================] - 0s\n",
      "1255/1255 [==============================] - 0s     \n",
      "week10\n",
      "Epoch 00027: early stopping\n",
      "11/11 [==============================] - 0s\n",
      "1244/1244 [==============================] - 0s     \n",
      "week11\n",
      "Epoch 00031: early stopping\n",
      "19/19 [==============================] - 0s\n",
      "1225/1225 [==============================] - 0s     \n",
      "week12\n",
      "Epoch 00026: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1215/1215 [==============================] - 0s     \n",
      "week13\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1205/1205 [==============================] - 0s     \n",
      "week14\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1195/1195 [==============================] - 0s     \n",
      "week15\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1185/1185 [==============================] - 0s     \n",
      "week16\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1175/1175 [==============================] - 0s     \n",
      "week17\n",
      "Epoch 00023: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1165/1165 [==============================] - 0s     \n",
      "week18\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1155/1155 [==============================] - 0s     \n",
      "week19\n",
      "Epoch 00024: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1145/1145 [==============================] - 0s     \n",
      "week20\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1135/1135 [==============================] - 0s     \n",
      "week21\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1125/1125 [==============================] - 0s     \n",
      "week22\n",
      "Epoch 00024: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1115/1115 [==============================] - 0s     \n",
      "week23\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1105/1105 [==============================] - 0s     \n",
      "week24\n",
      "Epoch 00021: early stopping\n",
      "8/8 [==============================] - 0s\n",
      "1097/1097 [==============================] - 0s     \n",
      "start compute precision_mat\n",
      "(1096, 104)\n",
      "(1096, 3)\n",
      "Epoch 00028: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [440 237 308], val_loss: 1.007\n",
      "Epoch 00036: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [440 237 308], val_loss: 0.975\n",
      "Epoch 00041: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [440 237 308], val_loss: 0.932\n",
      "Epoch 00033: early stopping\n",
      "110/110 [==============================] - 0s\n",
      "110/110 [==============================] - 0s\n",
      "986/986 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [440 237 309], val_loss: 1.006\n",
      "Epoch 00036: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [440 238 309], val_loss: 0.938\n",
      "Epoch 00027: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 6, Class dist.: [440 238 309], val_loss: 0.995\n",
      "Epoch 00027: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 7, Class dist.: [440 238 309], val_loss: 0.963\n",
      "Epoch 00035: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 8, Class dist.: [440 238 309], val_loss: 0.990\n",
      "Epoch 00031: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 9, Class dist.: [440 238 309], val_loss: 0.982\n",
      "Epoch 00045: early stopping\n",
      "108/108 [==============================] - 0s\n",
      "108/108 [==============================] - 0s\n",
      "988/988 [==============================] - 0s     \n",
      "Fold: 10, Class dist.: [441 238 309], val_loss: 1.040\n",
      "summary\n",
      "score:\n",
      "0.452107279693\n",
      "2like\n",
      "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
      ">80       0.8     1.0         18        6   0.750000          0        1   \n",
      "60-80     0.6     0.8         12       10   0.545455          4        2   \n",
      "50-60     0.5     0.6         14       14   0.500000          3        5   \n",
      "40-50     0.4     0.5         20       28   0.416667          6       15   \n",
      "30-40     0.3     0.4         27       36   0.428571         21       49   \n",
      "20-30     0.2     0.3         10       21   0.322581         16       43   \n",
      "<20       0.0     0.2          8       37   0.177778         18       78   \n",
      "\n",
      "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
      ">80     0.000000          9        4   0.692308  \n",
      "60-80   0.666667         13       12   0.520000  \n",
      "50-60   0.375000          4        6   0.400000  \n",
      "40-50   0.285714          5       14   0.263158  \n",
      "30-40   0.300000         15       31   0.326087  \n",
      "20-30   0.271186         23       54   0.298701  \n",
      "<20     0.187500         15       56   0.211268  \n",
      "sum precision:[ 0.49635036  0.32692308  0.45833333]\n"
     ]
    }
   ],
   "source": [
    "sum_proba, sum_y,resultdf,p_matrix= futureTest2([55,55],X_scaled,y,X[:,c.dateColumn],numOfWeek=25,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[lower</th>\n",
       "      <th>upper)</th>\n",
       "      <th>h_Correct</th>\n",
       "      <th>h_Wrong</th>\n",
       "      <th>h_Precent</th>\n",
       "      <th>d_Correct</th>\n",
       "      <th>d_Wrong</th>\n",
       "      <th>d_Precent</th>\n",
       "      <th>a_Correct</th>\n",
       "      <th>a_Wrong</th>\n",
       "      <th>a_Precent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&gt;80</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>197</td>\n",
       "      <td>78</td>\n",
       "      <td>0.716364</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-60</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>66</td>\n",
       "      <td>81</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>54</td>\n",
       "      <td>0.611511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-50</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>81</td>\n",
       "      <td>85</td>\n",
       "      <td>0.487952</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>87</td>\n",
       "      <td>0.494186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30-40</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>78</td>\n",
       "      <td>130</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>66</td>\n",
       "      <td>148</td>\n",
       "      <td>0.308411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-30</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>38</td>\n",
       "      <td>152</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>212</td>\n",
       "      <td>602</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>56</td>\n",
       "      <td>165</td>\n",
       "      <td>0.253394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18</td>\n",
       "      <td>75</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>29</td>\n",
       "      <td>178</td>\n",
       "      <td>0.140097</td>\n",
       "      <td>47</td>\n",
       "      <td>297</td>\n",
       "      <td>0.136628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
       ">80       0.8     1.0         11        6   0.647059          0        0   \n",
       "60-80     0.6     0.8        197       78   0.716364          0        0   \n",
       "50-60     0.5     0.6         66       81   0.448980          0        0   \n",
       "40-50     0.4     0.5         81       85   0.487952          0        0   \n",
       "30-40     0.3     0.4         78      130   0.375000         23       52   \n",
       "20-30     0.2     0.3         38      152   0.200000        212      602   \n",
       "<20       0.0     0.2         18       75   0.193548         29      178   \n",
       "\n",
       "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
       ">80          NaN          0        0        NaN  \n",
       "60-80        NaN          4        2   0.666667  \n",
       "50-60        NaN         85       54   0.611511  \n",
       "40-50        NaN         85       87   0.494186  \n",
       "30-40   0.306667         66      148   0.308411  \n",
       "20-30   0.260442         56      165   0.253394  \n",
       "<20     0.140097         47      297   0.136628  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findRecordsBy(self,df):\n",
    "    #print((df['DayStamp']).values)\n",
    "    #date = convertToDate((df['DayStamp']).values)\n",
    "    #df[\"Date\"] = date\n",
    "    \n",
    "    home = df['HomeTeam'].values\n",
    "    away = df['AwayTeam'].values\n",
    "    origin = self.df[[\"Date\",\"HomeTeam\",\"AwayTeam\",\"JocH\",\"JocD\",\"JocA\"]]\n",
    "    origin[\"DayStamp\"]=(pd.to_numeric(origin['Date'])/1e9/24/60/60).values\n",
    "    origin[\"DayStamp\"] = origin[\"DayStamp\"].apply(lambda x: \"%.f\"%(float(x)))\n",
    "    df['DayStamp']=df['DayStamp'].apply(lambda x: \"%.f\"%(float(x)))\n",
    "    return origin.merge(df,left_on=['DayStamp',\"HomeTeam\",\"AwayTeam\"],right_on=[\"DayStamp\",\"HomeTeam\",\"AwayTeam\"],how='inner')\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "withodds = findRecordsBy(c,resultdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def formatMatrixs(oddDf , precisionDf):\n",
    "    proba_mat = oddDf[['H_prob','D_prob','A_prob']].values\n",
    "    def locatePrecision(proba_mat, precisionDf):\n",
    "        precisionMat = precisionDf.values\n",
    "        pre_cols=[4,7,10]\n",
    "        def convert(proba, pre_col = 4):\n",
    "            if proba < 0.2:\n",
    "                return proba\n",
    "            for i in range(precisionMat.shape[0]):\n",
    "                if precisionMat[i,0] <= proba and proba < precisionMat[i,1] :\n",
    "                    if math.isnan(precisionMat[i,pre_col]):\n",
    "                        return proba\n",
    "                    else:\n",
    "                        return precisionMat[i,pre_col]\n",
    "        h_fproba = np.array([ convert(float(proba)) for proba in proba_mat[:,0] ] )\n",
    "        d_fproba = np.array([ convert(float(proba),pre_col=7) for proba in proba_mat[:,1] ] )\n",
    "        a_fproba = np.array([ convert(float(proba),pre_col=10) for proba in proba_mat[:,2] ] )\n",
    "        return h_fproba, d_fproba,a_fproba\n",
    "    h_fproba, d_fproba,a_fproba =locatePrecision(proba_mat,precisionDf)\n",
    "    fproba_mat = np.hstack([h_fproba,d_fproba,a_fproba]).reshape(3,h_fproba.shape[0]).T\n",
    "    odd_mat = oddDf[['JocH','JocD','JocA']].values\n",
    "    win_mat = None\n",
    "    if 'H' in oddDf.columns:\n",
    "        win_mat = oddDf[['H','D','A']].values\n",
    "    return fproba_mat,odd_mat,win_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fproba_mat,odd_mat,win_mat= formatMatrixs(withodds,p_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strategy1(fproba_mat,odd_mat,win_mat):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    print(exp)\n",
    "    maxi = np.argmax(exp,axis=1)\n",
    "    print(maxi)\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    for i in range(maxi.shape[0]):\n",
    "        if exp[i,maxi[i]] > 1:\n",
    "            expectation = expectation+ exp[i,maxi[i]]\n",
    "            spent = spent+1\n",
    "            if maxi[i] == y_true[i]:\n",
    "                income = income + odd_mat[i,maxi[i]]\n",
    "            \n",
    "            \n",
    "        \n",
    "    print(\"Spent:{}, Income:{}, expectation:{}\".format(spent,income,expectation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.03339552e+00   1.04675237e+00   8.94297838e-01]\n",
      " [  8.21849315e-01   9.04600812e-01   6.44336531e-01]\n",
      " [  8.96994536e-01   9.36690647e-01   8.16753927e-01]\n",
      " [  1.00178082e+00   8.27063599e-01   8.51716738e-01]\n",
      " [  6.71511628e-01   1.08776978e+00   1.44656652e+00]\n",
      " [  6.96132597e-01   8.14140731e-01   1.36569106e+00]\n",
      " [  1.71044776e+00   6.39047328e-01   6.70557940e-01]\n",
      " [  7.53767123e-01   9.56292287e-01   8.99754539e-01]\n",
      " [  7.98142077e-01   9.06474820e-01   9.73821990e-01]\n",
      " [  8.80813953e-01   7.88294993e-01   9.32832618e-01]\n",
      " [  9.55161290e-01   3.13147581e-01   4.83380637e-01]\n",
      " [  1.91640922e-01   5.21662380e-01   1.16852630e+00]\n",
      " [  1.21575342e+00   7.62449256e-01   5.33796820e-01]\n",
      " [  1.36740331e+00   1.23884892e+00   6.78901734e-01]\n",
      " [  8.65616438e-01   8.27063599e-01   1.12210300e+00]\n",
      " [  9.94475138e-01   9.51798561e-01   8.56820809e-01]\n",
      " [  8.57734807e-01   8.27063599e-01   9.13005780e-01]\n",
      " [  1.37505661e-01   1.86258842e-01   2.04885686e+00]\n",
      " [  7.98208955e-01   7.75562357e-01   2.19266117e+00]\n",
      " [  9.33606557e-01   1.59804258e+00   4.86921009e-01]\n",
      " [  1.58216418e+00   8.39986468e-01   5.26554066e-01]\n",
      " [  7.73219178e-01   9.69215156e-01   1.24377682e+00]\n",
      " [  5.49180328e-01   1.11798561e+00   1.51416309e+00]\n",
      " [  8.75342466e-01   8.27063599e-01   6.65827831e-01]\n",
      " [  9.69253731e-01   1.05967524e+00   1.22541906e+00]\n",
      " [  8.99657534e-01   8.39986468e-01   6.53730836e-01]\n",
      " [  3.80386740e-01   9.17523681e-01   3.36910569e+00]\n",
      " [  1.05523256e+00   9.66906475e-01   6.94892704e-01]\n",
      " [  6.22465753e-01   1.18890392e+00   1.66541582e+00]\n",
      " [  2.72512452e-01   4.53988700e-01   1.17727273e+00]\n",
      " [  5.89226519e-01   8.01217862e-01   1.26416185e+00]\n",
      " [  1.97704918e+00   9.82138024e-01   4.71204188e-01]\n",
      " [  7.01252885e-01   1.05755396e+00   9.92682927e-01]\n",
      " [  1.07615672e+00   5.64165755e-01   1.51416309e+00]\n",
      " [  9.88524590e-01   9.06474820e-01   7.63350785e-01]\n",
      " [  1.41027397e+00   3.57897199e-01   7.13089005e-01]\n",
      " [  9.34838710e-01   2.44513988e-01   6.28706343e-01]\n",
      " [  2.85179987e-01   2.87325945e-01   1.87727273e+00]\n",
      " [  9.70218579e-01   7.88294993e-01   7.69633508e-01]\n",
      " [  1.09753731e+00   6.86168744e-01   1.48712446e+00]\n",
      " [  9.48387097e-01   1.63693526e-01   2.56565912e-01]\n",
      " [  9.51912568e-01   7.54695535e-01   8.10471204e-01]\n",
      " [  8.36438356e-01   8.78755074e-01   1.13562232e+00]\n",
      " [  1.35410448e+00   3.63497608e-01   9.05793991e-01]\n",
      " [  5.20114092e-01   4.51777467e-01   1.04363636e+00]\n",
      " [  1.12604478e+00   9.82138024e-01   3.13354857e-01]\n",
      " [  3.94726338e-01   6.40660628e-01   1.01818182e+00]\n",
      " [  8.23770492e-01   1.39031401e+00   7.57081545e-01]\n",
      " [  6.85684932e-01   1.04675237e+00   1.25185250e+00]\n",
      " [  3.11647685e-01   4.88010579e-01   2.06818182e+00]\n",
      " [  6.46780822e-01   1.13721245e+00   2.00085837e+00]\n",
      " [  7.02950820e-01   9.66906475e-01   9.59871245e-01]\n",
      " [  7.45856354e-01   9.06474820e-01   7.00523560e-01]\n",
      " [  5.47616327e-01   2.29622126e-01   1.17727273e+00]\n",
      " [  8.37209302e-01   9.36690647e-01   6.95598212e-01]\n",
      " [  7.43224044e-01   9.21582734e-01   1.06806283e+00]\n",
      " [  9.76380597e-01   6.34155348e-01   1.21117437e+00]\n",
      " [  8.96994536e-01   9.66906475e-01   6.89484979e-01]\n",
      " [  6.93313953e-01   8.91677943e-01   1.39248927e+00]\n",
      " [  5.75581395e-01   1.32949640e+00   2.09549356e+00]\n",
      " [  1.03339552e+00   2.56110741e-01   1.51416309e+00]\n",
      " [  9.81104651e-01   7.88294993e-01   7.89527897e-01]\n",
      " [  9.02616279e-01   5.80904518e-01   1.02094241e+00]\n",
      " [  4.20103878e-01   6.18247151e-01   1.14545455e+00]\n",
      " [  1.10466418e+00   1.00798376e+00   7.59614448e-01]\n",
      " [  6.14088398e-01   3.70572278e-01   1.62272727e+00]\n",
      " [  2.45354256e-01   1.08552097e+00   8.90909091e-01]\n",
      " [  7.58132958e-02   5.00004508e-01   1.44688863e+00]\n",
      " [  7.36918605e-01   6.32403348e-01   1.41361257e+00]\n",
      " [  7.87808219e-01   9.04600812e-01   9.16679677e-01]\n",
      " [  1.08328358e+00   6.37020170e-01   8.60793816e-01]\n",
      " [  8.53060109e-01   7.88294993e-01   8.79581152e-01]\n",
      " [  1.12063953e+00   7.36603518e-01   8.41884817e-01]\n",
      " [  7.06395349e-01   9.17523681e-01   1.25729614e+00]\n",
      " [  9.09383562e-01   5.05342014e-01   1.13089005e+00]\n",
      " [  1.21823204e+00   3.89631589e-01   9.73636364e-01]\n",
      " [  8.85174419e-01   7.62449256e-01   1.11518325e+00]\n",
      " [  1.61287121e-01   1.27670658e-01   3.06937829e+00]\n",
      " [  7.60405710e-01   8.52909337e-01   1.08292683e+00]\n",
      " [  8.31575342e-01   8.27063599e-01   1.24377682e+00]\n",
      " [  6.73661202e-01   1.57670450e+00   1.02746781e+00]\n",
      " [  8.45303867e-01   5.70589891e-01   1.19121951e+00]\n",
      " [  7.08563536e-01   1.41272921e+00   6.08369099e-01]\n",
      " [  3.34407225e-01   1.70552871e+00   8.11158798e-01]\n",
      " [  1.99383562e+00   8.78755074e-01   4.70472103e-01]\n",
      " [  9.15697674e-01   8.14140731e-01   8.38197425e-01]\n",
      " [  5.45058140e-01   1.25351827e+00   2.50107296e+00]\n",
      " [  8.71366120e-01   8.27063599e-01   8.23036649e-01]\n",
      " [  5.71147541e-01   1.04244604e+00   1.48712446e+00]\n",
      " [  7.92671233e-01   9.43369418e-01   7.90387785e-01]\n",
      " [  7.76162791e-01   1.01223022e+00   1.06802575e+00]\n",
      " [  9.19365672e-01   2.29741203e-01   1.43122288e+00]\n",
      " [  5.00604312e-01   4.36724567e-01   1.27272727e+00]\n",
      " [  8.75342466e-01   9.82014388e-01   3.37894499e-01]\n",
      " [  9.28064516e-01   9.09550488e-02   2.22816122e-01]\n",
      " [  4.64917127e-01   1.39262190e+00   1.01394850e+00]\n",
      " [  1.93850746e+00   9.82014388e-01   2.00374419e-01]\n",
      " [  5.96775956e-01   1.57006081e+00   8.67056707e-01]\n",
      " [  7.82945205e-01   5.57346869e-01   1.21673820e+00]\n",
      " [  5.71220930e-01   1.11136671e+00   2.27124464e+00]\n",
      " [  4.70169378e-01   4.34346958e-01   1.05000000e+00]\n",
      " [  8.19767442e-01   8.39986468e-01   9.86909871e-01]\n",
      " [  7.45856354e-01   5.67389138e-01   1.32357724e+00]\n",
      " [  1.07703488e+00   1.67862546e+00   2.65069789e-02]\n",
      " [  1.33633880e+00   2.32479954e-01   1.14308943e+00]\n",
      " [  8.16448087e-01   9.21582734e-01   9.26701571e-01]\n",
      " [  5.22346156e-01   3.30033470e-01   1.25917685e+00]\n",
      " [  9.51912568e-01   8.76258993e-01   8.16753927e-01]\n",
      " [  1.63517442e+00   8.27063599e-01   5.87434555e-01]\n",
      " [  6.85684932e-01   4.13146099e-01   1.85214592e+00]\n",
      " [  8.33843284e-01   1.13053545e+00   1.96443340e+00]\n",
      " [  1.21935484e+00   2.90106805e-01   1.78560455e-01]\n",
      " [  1.09836066e+00   8.01217862e-01   6.84816754e-01]\n",
      " [  8.38882059e-01   1.08776978e+00   7.25722543e-01]\n",
      " [  1.01991143e-01   1.24971159e-01   1.59812311e+00]\n",
      " [  1.09011628e+00   5.68221164e-01   8.10471204e-01]\n",
      " [  9.76380597e-01   1.11136671e+00   3.60905512e-01]\n",
      " [  8.60753425e-01   8.52909337e-01   6.56655521e-01]\n",
      " [  8.72093023e-01   5.43427213e-01   1.09947644e+00]\n",
      " [  1.38006874e-02   1.03258871e-02   1.92609039e+00]\n",
      " [  8.85174419e-01   9.21582734e-01   9.19313305e-01]\n",
      " [  1.46075581e+00   7.88294993e-01   5.54291845e-01]\n",
      " [  7.02950820e-01   9.36690647e-01   1.00042918e+00]\n",
      " [  2.64921875e-01   3.27873094e+00   3.53095103e-01]\n",
      " [  1.24720149e+00   1.08776978e+00   9.91997821e-02]\n",
      " [  5.31850800e-01   9.95511957e-02   1.32363636e+00]\n",
      " [  9.81104651e-01   9.51798561e-01   7.65193133e-01]\n",
      " [  4.88372093e-01   1.70581867e+00   3.78540773e+00]\n",
      " [  8.78688525e-01   7.62449256e-01   8.73298429e-01]\n",
      " [  1.24480874e+00   8.52909337e-01   6.06282723e-01]\n",
      " [  8.08011050e-01   7.88294993e-01   1.26341463e+00]\n",
      " [  8.55890411e-01   9.51798561e-01   6.33354516e-01]\n",
      " [  9.55000000e-01   6.93945902e-01   1.33838864e+00]\n",
      " [  9.59302326e-01   8.52909337e-01   8.79581152e-01]\n",
      " [  8.15406977e-01   8.27063599e-01   1.17801047e+00]\n",
      " [  3.36839276e-01   6.08010713e-01   1.15818182e+00]\n",
      " [  5.71823204e-01   9.06474820e-01   9.04712042e-01]\n",
      " [  2.62886292e-01   3.29362514e-01   1.65454545e+00]\n",
      " [  8.19767442e-01   8.39986468e-01   1.14659686e+00]\n",
      " [  9.14516129e-01   4.12867550e-02   1.98349910e-01]\n",
      " [  1.36835821e+00   8.91677943e-01   1.43820427e-01]\n",
      " [  5.88424658e-01   2.09989834e+00   4.77747994e-01]\n",
      " [  8.67704918e-01   9.21582734e-01   8.57591623e-01]\n",
      " [  1.07273224e+00   2.40450630e-01   1.29349593e+00]\n",
      " [  5.62500000e-01   1.13721245e+00   2.40643777e+00]\n",
      " [  9.19365672e-01   7.58491150e-01   1.12789330e+00]\n",
      " [  3.72440835e-02   9.47863490e-03   1.44707633e+00]\n",
      " [  1.74608209e+00   9.21582734e-01   1.20295787e-01]\n",
      " [  1.83761191e-02   2.64767806e+00   3.12102716e-01]\n",
      " [  6.76243094e-01   1.39656712e+00   7.06806283e-01]\n",
      " [  5.88424658e-01   9.80976135e-01   2.70386266e+00]\n",
      " [  1.07703488e+00   5.20977542e-01   1.22670520e+00]\n",
      " [  1.15327869e+00   5.82327513e-01   9.73872832e-01]\n",
      " [  9.14246575e-01   6.15072458e-01   9.46351931e-01]\n",
      " [  9.01644878e-02   1.67874487e-01   1.39238843e+00]\n",
      " [  3.21011487e-01   6.61458778e-01   1.08181818e+00]\n",
      " [  8.50290698e-01   8.39986468e-01   9.19313305e-01]\n",
      " [  4.17907541e-01   2.69245367e+00   1.85551586e-02]\n",
      " [  2.37645349e+00   6.88521758e-01   6.92947977e-01]\n",
      " [  5.14534884e-01   1.69208633e+00   2.11249776e+00]\n",
      " [  8.94193548e-01   9.10416106e-02   1.02643023e+00]\n",
      " [  9.19365672e-01   1.42014388e+00   3.90208983e-01]\n",
      " [  5.19890710e-01   1.19352518e+00   1.99476440e+00]\n",
      " [  8.28488372e-01   8.39986468e-01   9.73390558e-01]\n",
      " [  8.67704918e-01   8.39986468e-01   8.16753927e-01]\n",
      " [  1.46094763e-01   2.29402950e-01   1.35498400e+00]\n",
      " [  6.56506849e-01   7.75505200e-01   1.96030043e+00]\n",
      " [  3.12508962e-01   1.93470579e+00   6.21888412e-01]\n",
      " [  9.95806452e-01   2.20081717e-01   2.70304743e-01]\n",
      " [  1.96451613e+00   2.18196591e-01   2.31163478e-01]\n",
      " [  2.43119490e-01   7.07718101e-01   2.86363636e+00]\n",
      " [  6.33977901e-01   1.71763033e+00   4.54450054e-01]\n",
      " [  1.61620302e-01   7.62449256e-01   1.64181818e+00]\n",
      " [  5.34535519e-01   4.99109919e-01   2.76242775e+00]\n",
      " [  1.01914179e+00   3.03913258e-01   1.62231760e+00]\n",
      " [  8.68865753e-02   2.13363158e-01   1.31686422e+00]\n",
      " [  2.67957864e-01   2.10250926e-01   1.93464269e+00]\n",
      " [  9.22529302e-02   2.10837483e-01   1.61495983e+00]\n",
      " [  1.24645161e+00   1.98217381e-01   1.22647891e-01]\n",
      " [  1.11179104e+00   2.80117504e-01   1.52356021e+00]\n",
      " [  6.36627907e-01   9.69215156e-01   1.94764398e+00]\n",
      " [  1.00258065e+00   2.92420697e-01   5.71834257e-01]\n",
      " [  8.87419355e-01   2.15686502e-01   4.45616917e-02]\n",
      " [  1.24692010e+00   3.63485859e-01   8.52727273e-01]\n",
      " [  7.48904110e-01   6.67449895e-01   1.29785408e+00]\n",
      " [  5.71147541e-01   9.56292287e-01   1.57068063e+00]\n",
      " [  4.50327869e-01   1.22767253e+00   5.15028902e+00]\n",
      " [  1.16712329e+00   1.45483962e+00   4.79532979e-02]\n",
      " [  1.20580110e+00   1.69298437e+00   4.29914163e-01]\n",
      " [  1.33633880e+00   8.39986468e-01   8.80231214e-01]\n",
      " [  1.21156716e+00   3.32882882e-01   5.79300523e-01]\n",
      " [  9.02616279e-01   9.97122302e-01   8.24678112e-01]\n",
      " [  1.01598837e+00   9.66906475e-01   7.30042918e-01]\n",
      " [  4.83278689e-01   1.12428958e+00   2.51308901e+00]\n",
      " [  6.32267442e-01   1.19352518e+00   1.59527897e+00]\n",
      " [  9.34838710e-01   4.73811065e-01   1.60080189e-01]\n",
      " [  9.07458564e-01   1.40130129e+00   5.90575916e-01]\n",
      " [  1.55806452e+00   2.49231873e-04   8.58008229e-04]\n",
      " [  1.60478805e-02   3.16636171e-02   1.55143930e+00]\n",
      " [  5.16695956e-02   3.84867281e-02   2.18171324e+00]\n",
      " [  8.48097015e-01   5.69696929e-01   2.83905579e+00]\n",
      " [  6.95628415e-01   9.82014388e-01   9.73390558e-01]\n",
      " [  1.04068493e+00   9.66906475e-01   2.53780581e-01]\n",
      " [  1.32559701e+00   9.97122302e-01   1.78083093e-01]\n",
      " [  1.30813953e+00   8.39986468e-01   5.73218884e-01]\n",
      " [  8.96994536e-01   9.51798561e-01   6.97596567e-01]\n",
      " [  4.06700091e-01   4.02966645e-01   1.60363636e+00]\n",
      " [  6.83701657e-01   5.09227797e-01   1.52727273e+00]\n",
      " [  6.77322404e-01   9.82014388e-01   1.02746781e+00]\n",
      " [  4.89779006e-01   9.36690647e-01   1.11518325e+00]\n",
      " [  1.35464481e+00   5.12579687e-01   8.42774566e-01]\n",
      " [  1.43962687e+00   1.48283965e-01   8.38197425e-01]\n",
      " [  9.55000000e-01   3.23276963e-01   9.60548416e-01]\n",
      " [  4.62430939e-01   2.23629055e+00   2.62862159e-01]\n",
      " [  1.59193548e+00   4.22450248e-02   2.69058820e-01]\n",
      " [  5.78794826e-03   1.19922328e-02   1.45414041e+00]\n",
      " [  1.58216418e+00   2.27860844e-01   7.62489270e-01]\n",
      " [  1.11774194e+00   7.08793098e-01   7.46402484e-03]\n",
      " [  8.46774194e-01   4.15042508e-03   9.52196633e-02]\n",
      " [  1.28633721e+00   5.80225754e-01   6.81675393e-01]\n",
      " [  8.89931507e-01   1.98124890e-01   1.14659686e+00]\n",
      " [  8.69477612e-01   1.34397835e+00   1.05636127e+00]\n",
      " [  6.88306011e-01   8.01217862e-01   1.20942408e+00]\n",
      " [  3.38475084e-02   2.72890878e-01   2.22618645e+00]\n",
      " [  9.07978142e-01   9.82014388e-01   7.79057592e-01]\n",
      " [  6.51693989e-01   1.02733813e+00   1.22513089e+00]\n",
      " [  7.85806452e-01   5.16097235e-03   8.99702269e-02]\n",
      " [  6.07759563e-01   8.78755074e-01   2.15375723e+00]\n",
      " [  7.23837209e-01   1.45023282e+00   4.35141271e-01]\n",
      " [  4.84806630e-01   1.74854800e+00   5.79523471e-01]\n",
      " [  5.64364641e-01   1.55447647e+00   7.84120172e-01]\n",
      " [  1.28633721e+00   1.58360463e+00   2.37734147e-01]\n",
      " [  1.51089552e+00   4.20896562e-01   4.22614129e-01]\n",
      " [  7.40883978e-01   9.36690647e-01   6.91099476e-01]\n",
      " [  3.29293566e-01   4.52305037e-01   1.84545455e+00]\n",
      " [  1.51939891e+00   7.96070185e-02   1.05636364e+00]\n",
      " [  7.50546448e-01   9.97122302e-01   9.73821990e-01]\n",
      " [  1.77500000e+00   1.46931089e-01   5.68586387e-01]\n",
      " [  1.05523256e+00   9.51798561e-01   4.56004996e-01]\n",
      " [  7.65483871e-01   1.72685177e-01   1.96131371e+00]\n",
      " [  2.22383721e+00   1.82276691e-01   9.14471545e-01]\n",
      " [  5.05245902e-01   1.23884892e+00   2.16753927e+00]\n",
      " [  1.31847015e+00   8.39986468e-01   2.17787395e-01]\n",
      " [  1.09041045e+00   9.56292287e-01   4.14368177e-01]\n",
      " [  1.09836066e+00   9.21582734e-01   6.00257511e-01]\n",
      " [  7.71802326e-01   8.91677943e-01   1.05450644e+00]\n",
      " [  1.62492537e+00   9.97122302e-01   6.86004929e-02]\n",
      " [  7.61530055e-01   9.21582734e-01   1.03664921e+00]\n",
      " [  5.78469945e-01   1.08776978e+00   1.53926702e+00]\n",
      " [  1.80405645e-03   1.83662510e+00   1.47485549e+00]\n",
      " [  8.26716418e-01   8.23329985e-01   3.24463519e+00]\n",
      " [  2.48495970e-02   6.60106134e-04   2.78893182e+00]\n",
      " [  1.65343284e+00   6.15739717e-01   3.40222833e-02]\n",
      " [  1.46100746e+00   2.93872774e-01   9.89528796e-01]\n",
      " [  3.65085643e-01   1.43844364e-01   1.51902822e+00]\n",
      " [  1.16516129e+00   2.90539179e-03   7.48058494e-02]\n",
      " [  4.96400194e-03   5.28669753e-01   1.83152432e+00]\n",
      " [  7.99354839e-01   2.04358448e-04   5.42034303e-03]\n",
      " [  1.40956284e+00   8.65832206e-01   8.42774566e-01]\n",
      " [  1.39110927e-01   1.62740107e+00   8.90406504e-01]\n",
      " [  1.28283582e+00   4.51823559e-01   1.08154506e+00]]\n",
      "[1 1 1 0 2 2 0 1 2 2 0 2 0 0 2 0 2 2 2 1 0 2 2 0 2 0 2 0 2 2 2 0 1 2 0 0 0\n",
      " 2 0 2 0 0 2 0 2 0 2 1 2 2 2 1 1 2 1 2 2 1 2 2 2 0 2 2 0 2 1 2 2 2 0 2 0 2\n",
      " 2 0 2 2 2 2 1 2 1 1 0 0 2 0 2 1 2 2 2 1 0 1 0 1 2 2 2 2 2 1 0 2 2 0 0 2 2\n",
      " 0 0 1 2 0 1 0 2 2 1 0 2 1 0 2 0 2 0 0 2 1 2 0 2 2 1 2 2 0 0 1 1 2 2 2 2 0\n",
      " 1 1 2 2 0 2 2 2 2 1 0 2 2 1 2 2 0 2 2 1 0 0 2 1 2 2 2 2 2 2 0 2 2 0 0 0 2\n",
      " 2 2 1 1 0 0 1 0 2 2 0 1 0 2 2 2 1 0 0 0 1 2 2 2 2 0 0 2 1 0 2 0 0 0 0 2 1\n",
      " 2 2 1 2 0 2 1 1 1 1 0 1 2 0 1 0 0 2 0 2 0 0 0 2 0 2 2 1 2 2 0 0 2 0 2 0 0\n",
      " 1 0]\n",
      "Spent:205, Income:258.19, expectation:322.188139700667\n"
     ]
    }
   ],
   "source": [
    "strategy1(fproba_mat,odd_mat,win_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy3(fproba_mat,odd_mat,win_mat,info, z = 0.5  ):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    total = fproba_mat.shape[0]\n",
    "    withdraw = 0\n",
    "    buy = 0\n",
    "    homeTeam = info[\"HomeTeam\"].values\n",
    "    awayTeam = info[\"AwayTeam\"].values\n",
    "    receipt = []\n",
    "    def chooseFrom(match, chosable):\n",
    "        max_prob = -1\n",
    "        c = 0\n",
    "        for choice in chosable:\n",
    "            if fproba_mat[match,choice] > max_prob:\n",
    "                c = choice\n",
    "                max_prob = fproba_mat[match,choice]\n",
    "        return c\n",
    "    for match in range(exp.shape[0]):\n",
    "        chosable = []\n",
    "        for choose in range(3):\n",
    "            if exp[match,choose]> 1 and fproba_mat[match,choose] >= z:\n",
    "                chosable.append(choose)\n",
    "                \n",
    "        if len(chosable) == 0:\n",
    "            withdraw= withdraw+1\n",
    "            continue\n",
    "        choice = chooseFrom(match,chosable)\n",
    "        expectation = expectation+ exp[match,choice]\n",
    "        spent = spent+1\n",
    "        buy =buy +1 \n",
    "        receipt.append(np.hstack([homeTeam[match],awayTeam[match], odd_mat[match,choice],choice,y_true[match],fproba_mat[match,:]]))\n",
    "        if choice == y_true[match]:\n",
    "            income = income + odd_mat[match,choice]\n",
    "    print(buy)\n",
    "    print(\"Spent:{}, Income:{}, expectation:{}, withdraw:{}(total:{})\".\n",
    "          format(spent,income,expectation,withdraw,total))\n",
    "    receiptDf = pd.DataFrame(receipt,columns=[\"home\",\"away\",\"odd of choice\",\"choice\",\"result\",\"Hp\",\"Dp\",\"Ap\"])\n",
    "    return spent,income,expectation,withdraw,total,receiptDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "Spent:90, Income:91.14999999999998, expectation:139.70355912289673, withdraw:171(total:261)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy3(fproba_mat,odd_mat,win_mat,withodds,z=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "receipt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_range = range(30,80,2)\n",
    "spents = []\n",
    "incomes = []\n",
    "exps = []\n",
    "withs = []\n",
    "for z in z_range:\n",
    "    fz = z/100\n",
    "    print(z)\n",
    "    spent,income,expectation,withdraw,total,_ = strategy3(fproba_mat,odd_mat,win_mat,withodds,z=fz) \n",
    "    spents.append(spent)\n",
    "    incomes.append(income)\n",
    "    exps.append(expectation)\n",
    "    withs.append(withdraw)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(z_range, spents)\n",
    "plt.plot(z_range, incomes,color='green')\n",
    "plt.plot(z_range, np.array(incomes)/np.array(spents)*100,color='red')\n",
    "plt.plot(z_range,exps,color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy4(fproba_mat,odd_mat,win_mat,info, z = 0.2 ,e =1.8 ):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    y_true=None\n",
    "    if win_mat is None:\n",
    "        y_true = np.zeros(shape=(fproba_mat.shape[0],))\n",
    "    else:\n",
    "        y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    total = fproba_mat.shape[0]\n",
    "    withdraw = 0\n",
    "    buy = 0\n",
    "    homeTeam = info[\"HomeTeam\"].values\n",
    "    awayTeam = info[\"AwayTeam\"].values\n",
    "    receipt = []\n",
    "    def chooseFrom(match, chosable):\n",
    "        max_prob = -1\n",
    "        c = 0\n",
    "        for choice in chosable:\n",
    "            if fproba_mat[match,choice] > max_prob:\n",
    "                c = choice\n",
    "                max_prob = fproba_mat[match,choice]\n",
    "        return c\n",
    "    for match in range(exp.shape[0]):\n",
    "        chosable = []\n",
    "        for choose in range(3):\n",
    "            if exp[match,choose]> e and fproba_mat[match,choose] >= z:\n",
    "                chosable.append(choose)\n",
    "                \n",
    "        if len(chosable) == 0:\n",
    "            withdraw= withdraw+1\n",
    "            continue\n",
    "        choice = chooseFrom(match,chosable)\n",
    "        expectation = expectation+ exp[match,choice]\n",
    "        spent = spent+1\n",
    "        buy =buy +1 \n",
    "        receipt.append(np.hstack([homeTeam[match],awayTeam[match], odd_mat[match,choice],choice,y_true[match],fproba_mat[match,:]]))\n",
    "        if choice == y_true[match]:\n",
    "            income = income + odd_mat[match,choice]\n",
    "    print(buy)\n",
    "    print(\"Spent:{}, Income:{}, expectation:{}, withdraw:{}(total:{})\".\n",
    "          format(spent,income,expectation,withdraw,total))\n",
    "    receiptDf = pd.DataFrame(receipt,columns=[\"home\",\"away\",\"odd of choice\",\"choice\",\"result\",\"Hp\",\"Dp\",\"Ap\"])\n",
    "    return spent,income,expectation,withdraw,total,receiptDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Spent:64, Income:90.91000000000001, expectation:133.52804745108983, withdraw:197(total:261)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,win_mat,withodds,z=0.2,e=1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_node_sizes=[55,55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPrecisionMaxtrix(node_sizes,c):\n",
    "    X, y = c.getH7(removeInsufficient=True)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    _,_, proba_test,proba_y = crossValidate2(node_sizes,X_scaled,y,fold=10)\n",
    "    p_matrix = precisionMatrix(np.vstack(proba_test),np.vstack(proba_y)) \n",
    "    return p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "Epoch 00087: early stopping\n",
      "137/137 [==============================] - 0s     \n",
      "137/137 [==============================] - 0s     \n",
      "1221/1221 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [539 298 384], val_loss: 1.028\n",
      "Epoch 00101: early stopping\n",
      "137/137 [==============================] - 0s     \n",
      "137/137 [==============================] - 0s     \n",
      "1221/1221 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [539 298 384], val_loss: 1.030\n",
      "Epoch 00087: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [539 299 384], val_loss: 0.948\n",
      "Epoch 00090: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [539 299 384], val_loss: 0.963\n",
      "Epoch 00097: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [539 299 384], val_loss: 1.015\n",
      "Epoch 00074: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 6, Class dist.: [539 299 384], val_loss: 1.001\n",
      "Epoch 00046: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 7, Class dist.: [539 299 384], val_loss: 0.984\n",
      "Epoch 00060: early stopping\n",
      "135/135 [==============================] - 0s     \n",
      "135/135 [==============================] - 0s     \n",
      "1223/1223 [==============================] - 0s     \n",
      "Fold: 8, Class dist.: [539 299 385], val_loss: 1.052\n",
      "Epoch 00037: early stopping\n",
      "135/135 [==============================] - 0s     \n",
      "135/135 [==============================] - 0s     \n",
      "1223/1223 [==============================] - 0s     \n",
      "Fold: 9, Class dist.: [539 299 385], val_loss: 1.034\n",
      "Epoch 00027: early stopping\n",
      "134/134 [==============================] - 0s     \n",
      "134/134 [==============================] - 0s     \n",
      "1224/1224 [==============================] - 0s     \n",
      "Fold: 10, Class dist.: [540 299 385], val_loss: 1.027\n"
     ]
    }
   ],
   "source": [
    "g_p_matrix= getPrecisionMaxtrix(g_node_sizes,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainedModel(node_sizes,c):\n",
    "    X, y = c.getH7(removeInsufficient=True)\n",
    "    X_scaled = StandardScaler().fit_transform(X) \n",
    "    model = createModel(node_sizes,X_scaled.shape[1])\n",
    "    earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "    history = model.fit(X_scaled,y,verbose=0,nb_epoch=500, validation_split=0.1, callbacks=[earlyCallback])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "Epoch 00092: early stopping\n"
     ]
    }
   ],
   "source": [
    "g_model = getTrainedModel(g_node_sizes,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def predictFuture(node_sizes,c,p_matrix,model):\n",
    "    X_test,y_test = c.getH7(removeInsufficient = True,future=True)\n",
    "    X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "    proba_y = model.predict_proba(X_test_scaled)\n",
    "   \n",
    "    def getTestDf (X_test,proba_y,c):\n",
    "        decoded = oneHotDecode(c, X_test)\n",
    "        homeNames = c.inverseTeamMapping(decoded[:,0])\n",
    "        awayNames = c.inverseTeamMapping(decoded[:,1])\n",
    "        names = np.array([homeNames,awayNames]).T\n",
    "        return pd.DataFrame(np.hstack([names,proba_y]),columns=['HomeTeam','AwayTeam','H_prob','D_prob','A_prob'])\n",
    "   \n",
    "    test_df = getTestDf(X_test,proba_y,c)\n",
    "    test_df = test_df.sort(columns=\"HomeTeam\")\n",
    "    originDf = c.df[c.df[\"Future\"]==1].sort(columns=\"HomeTeam\")\n",
    "    test_df['JocH']=originDf['JocH'].values\n",
    "    test_df['JocD']=originDf['JocD'].values\n",
    "    test_df['JocA']=originDf['JocA'].values\n",
    "    fproba_mat,odd_mat,_= formatMatrixs(test_df,p_matrix)\n",
    "    return fproba_mat,odd_mat,test_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "7/7 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:15: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:16: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "fproba_mat,odd_mat,test_df=predictFuture(g_node_sizes,c,g_p_matrix,g_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Spent:1, Income:0, expectation:1.6114285714285712, withdraw:6(total:7)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,None,test_df,z=0.2,e=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "      <th>odd of choice</th>\n",
       "      <th>choice</th>\n",
       "      <th>result</th>\n",
       "      <th>Hp</th>\n",
       "      <th>Dp</th>\n",
       "      <th>Ap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2436548223350254</td>\n",
       "      <td>0.24871794871794872</td>\n",
       "      <td>0.5714285714285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        home       away odd of choice choice result                  Hp  \\\n",
       "0  Liverpool  Tottenham          2.82      2    0.0  0.2436548223350254   \n",
       "\n",
       "                    Dp                  Ap  \n",
       "0  0.24871794871794872  0.5714285714285714  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipt.to_csv(\"2016-4-3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
