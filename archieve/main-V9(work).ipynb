{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    from IPython.core.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "import sys\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotCurve(train_mean, train_std,test_mean,test_std,sizes):\n",
    "    plt.plot(sizes, train_mean, \n",
    "            color='blue', marker='o', \n",
    "            markersize=5, \n",
    "            label='training accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                  train_mean + train_std,\n",
    "                   train_mean - train_std, alpha=0.15, color='blue')\n",
    "\n",
    "    plt.plot(sizes, test_mean, \n",
    "              color='green', linestyle='--', \n",
    "              marker='s', markersize=5, \n",
    "             label='validation accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                      test_mean + test_std,\n",
    "                     test_mean - test_std, \n",
    "                    alpha=0.15, color='green')\n",
    "    plt.xlabel('x_range')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "def likehoodScore(proba,y):\n",
    "    return np.sum(proba * y)/proba.shape[0]\n",
    "\n",
    "def firstNScore(n, pred, y):\n",
    "    backup = np.array(pred, copy =True)\n",
    "    for r in range(pred.shape[0]):\n",
    "        row = backup[r]\n",
    "        s = np.sort(row)\n",
    "        for c in range(pred.shape[1]):\n",
    "            temp = backup[r][c]\n",
    "            backup[r][c] = False\n",
    "            for j in range(1,n+1):\n",
    "                if temp == s[-j]:\n",
    "                    backup[r][c] = True\n",
    "                    break\n",
    "    res = np.sum(np.logical_and(backup,y))/pred.shape[0]\n",
    "    return res               \n",
    "\n",
    "def oneHotDecode(self, X_sample):\n",
    "    result=None\n",
    "    fiPos = 0\n",
    "    colIndex = 0\n",
    "    while colIndex < X_sample.shape[1]:\n",
    "        if fiPos < len(self.ohe.n_values_) and colIndex == self.ohe.feature_indices_[fiPos]:                \n",
    "            start = self.ohe.feature_indices_[fiPos]\n",
    "            end_ = start+ self.ohe.n_values_[fiPos]\n",
    "            #print(\"start{} end{}\".format(start,end_))\n",
    "            classes = np.argmax(X_sample[:,start:end_],axis=1).reshape(X_sample.shape[0],1)\n",
    "            if result is None:\n",
    "                result = classes\n",
    "            else:\n",
    "                result=np.hstack([result,classes])\n",
    "            colIndex = end_\n",
    "            fiPos = fiPos +1\n",
    "        else:\n",
    "            if result is None:\n",
    "                result = X_sample[:,colIndex:colIndex+1]\n",
    "            else:\n",
    "                result=np.hstack([result, X_sample[:,colIndex:colIndex+1]])\n",
    "            colIndex = colIndex +1\n",
    "        \n",
    "    return result \n",
    "def convertToDate(dayStamps):\n",
    "    res = [] \n",
    "    for v in dayStamps:\n",
    "        res.append(datetime.datetime.fromtimestamp(float(v)*24*60*60))\n",
    "    return res\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def precisionMatrix(proba, y):\n",
    "    def _precisionClassify(df,proba, wins, c =0 ):\n",
    "        for indx, v in enumerate(proba):\n",
    "            row = 0\n",
    "            col = 0\n",
    "            if wins[indx] == c:\n",
    "                col = 0\n",
    "            else:\n",
    "                col =1\n",
    "            if v <0.2:\n",
    "                row =6 \n",
    "            elif v < 0.3 and  v >=0.2:\n",
    "                row =5 \n",
    "            elif v < 0.4 and v >= 0.3:\n",
    "                row = 4 \n",
    "            elif v < 0.5 and v >= 0.4:\n",
    "                row = 3 \n",
    "            elif v < 0.6 and v >= 0.5:\n",
    "                row = 2 \n",
    "            elif v < 0.8 and v >= 0.6:\n",
    "                row = 1\n",
    "            df.iloc[row,col] = df.iloc[row,col]+1 \n",
    "        df[df.columns[2]] = df[df.columns[0]] /(df[df.columns[1]] + df[df.columns[0]])\n",
    "        return df\n",
    "    rowHeader = ['>80','60-80','50-60','40-50','30-40','20-30','<20']\n",
    "    df = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['h_Correct', 'h_Wrong','h_Precent'])\n",
    "    hproba = proba[:,0]\n",
    "    wins = np.argmax(y,axis=1)\n",
    "    df = _precisionClassify(df,hproba,wins)\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['d_Correct', 'd_Wrong','d_Precent'])\n",
    "    dproba = proba[:,1]\n",
    "    df = df.join(_precisionClassify(temp,dproba,wins,c=1))\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['a_Correct', 'a_Wrong','a_Precent'])\n",
    "    aproba = proba[:,2]\n",
    "    df = df.join(_precisionClassify(temp,aproba,wins,c=2))\n",
    "    \n",
    "    bound = pd.DataFrame(np.array([[0.8,1.0],[0.6,0.8],[0.5,0.6],[0.4,0.5],[0.3,0.4],[0.2,0.3],[0,0.2]] )\n",
    "                                ,index=rowHeader, columns=['[lower', 'upper)'])\n",
    "            \n",
    "    return bound.join(df)\n",
    "       \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "def futureTest(mlp, X,y,numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(X[:,c.dateColumn])\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            mlp.fit(X_train,y_train)\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = mlp.predict_proba(X_test)\n",
    "            train_proba =mlp.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "            if verbose == True:\n",
    "                print(\"week{}\".format(w))\n",
    "                print(\"numOftest {} , score {}\".format(X_test.shape[0],mlp.score(X_test,y_test)))\n",
    "                print(np.hstack([stack[errorIndx],proba[errorIndx],y_test[errorIndx]]))\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "        \n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    return firstNScore(1,sum_train_proba,sum_train_y), score, like2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "class FootballDataHelper:\n",
    "    def __init__ (self, recentNum=5):\n",
    "        self.win_mapping = {'H':0, 'D':1,'A':2}\n",
    "        self.recentNum = recentNum\n",
    "        self.df = None\n",
    "        self.teamsData={}\n",
    "        self.session = 0\n",
    "        #self.hiddensCount = 2\n",
    "       \n",
    "    def addColumns(self,df, addition):    \n",
    "        dates = df[\"Date\"].drop_duplicates().values\n",
    "        col_adds = []\n",
    "        for colAdd in addition.columns:\n",
    "             if colAdd not in df.columns:\n",
    "                    df[colAdd]=np.zeros(shape=(df.shape[0],))\n",
    "                    col_adds.append(colAdd)\n",
    "        for date in dates:\n",
    "            dateAddition= addition[addition['Date'] == date].sort(columns='HomeTeam')\n",
    "            dateDf  = df [df['Date']==date].sort(columns='HomeTeam')\n",
    "            for col in col_adds:\n",
    "                dateDf[col] = dateAddition[col].values\n",
    "            df.update(dateDf)\n",
    "        return df\n",
    "            \n",
    "    def saveDf(self,filename):\n",
    "        self.df.to_csv(filename,index=False)\n",
    "    def loadDf(self,filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])    \n",
    "        self.df = df\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "    def readFootBallData(self,year): \n",
    "        filename = \"dataSet/E{}.csv\".format(year)\n",
    "        df = pd.read_csv(filename)\n",
    "        #df = df.drop(df.columns[range(23,df.shape[1])], axis=1)\n",
    "        #df = df.drop(\"Div\",axis=1)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "        df['session'] = pd.Series(np.ones(shape=(df.shape[0],))*self.session, index=df.index)\n",
    "        self.session = self.session +1\n",
    "        \n",
    "        matchDetail = pd.read_csv(\"dataSet/match{}.csv\".format(year))\n",
    "        matchDetail['Date'] =pd.to_datetime(matchDetail['Date'])\n",
    "        df = self.addColumns(df,matchDetail)\n",
    "        \n",
    "        df[\"Future\"] = np.zeros(shape=(df.shape[0],))\n",
    "        \n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = pd.concat([self.df,df])\n",
    "            \n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "    def readFuture(self):\n",
    "        filename = \"dataSet/future.csv\"\n",
    "        df = pd.read_csv(filename)\n",
    "        #df = df.drop(df.columns[range(23,df.shape[1])], axis=1)\n",
    "        #df = df.drop(\"Div\",axis=1)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df[\"Future\"] = np.ones(shape=(df.shape[0],))\n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = pd.concat([self.df,df])\n",
    "            \n",
    "        self.df['HTR']=self.df['HTR'].fillna('D')\n",
    "        self.df = self.df.fillna(0)\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    "        \n",
    "    def getTeam(self,dataFrame, teamName):       \n",
    "        return dataFrame[(dataFrame[\"HomeTeam\"] == teamName) | (dataFrame[\"AwayTeam\"] == teamName)]\n",
    " \n",
    "        \n",
    "    def previousRecords(self,team, date , recentNum):\n",
    "        prev = team[( team[\"Date\"] < date)]\n",
    "        \n",
    "        if prev.shape[0] < recentNum :\n",
    "            #print(\"less than min Num\")\n",
    "            return None\n",
    "        else:\n",
    "            return prev.iloc[-recentNum:]\n",
    "    def readPredict (self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "  \n",
    "    def inverseTeamMapping (self, col):\n",
    "        inverseMap ={}\n",
    "        for name in self.teamsMap.keys():        \n",
    "            inverseMap[self.teamsMap[name]] = name\n",
    "        res =[]\n",
    "        for idex, v in enumerate(col):\n",
    "            res.append(inverseMap[v])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def readTeamMatch(self, teamName):\n",
    "        df = pd.read_csv('teams/'+teamName+'.csv')\n",
    "        df['1'] = pd.to_datetime(df['1'],yearfirst=True)\n",
    "        #df['1']= (pd.to_numeric(df['1'])/1e9/24/60/60)\n",
    "        self.teamsData[teamName]=df.sort(['1'],ascending=[False])\n",
    "        self.teamsById[self.teamsMap[teamName]]=self.teamsData[teamName]\n",
    "    \n",
    "    def commonMapping(self, X):\n",
    "        X['HomeTeam'] = X['HomeTeam'].map(self.teamsMap)\n",
    "        X['AwayTeam'] = X['AwayTeam'].map(self.teamsMap)\n",
    "        X['Referee']=X['Referee'].map(self.refereesMap).fillna(0)\n",
    "        X['HTR'] = X['HTR'].map(self.win_mapping)\n",
    "        X['FTR'] = X['FTR'].map(self.win_mapping)\n",
    "        return X\n",
    "    def initData(self, X, target,encode):\n",
    "        X  = X.sort_values(by=\"Date\")\n",
    "        isInput = False\n",
    "        if target is None:\n",
    "            target =X      \n",
    "        else:\n",
    "            if self.ohe is None:\n",
    "                raise Exception(\"Not yet get train data\")\n",
    "            isInput = True\n",
    "            if encode == True:\n",
    "                target = self.commonMapping(target)\n",
    "        y=None\n",
    "        if encode == True:    \n",
    "            X =self.commonMapping(X)\n",
    "            y = []\n",
    "            for v in target['FTR']:\n",
    "                y.append(range(3)==v)\n",
    "        else:\n",
    "            y = target['FTR'].values\n",
    "        target_date = (pd.to_numeric(target['Date'])/1e9/24/60/60).values\n",
    "        return isInput, X,y, target, target_date\n",
    "   \n",
    "    def aggregate(self,recents,nonExpand,isInput,encode):\n",
    "        res =None\n",
    "        if encode == True:\n",
    "            if isInput==False:\n",
    "                self.ohe = OneHotEncoder(categorical_features='all')\n",
    "                self.ohe.fit(recents)\n",
    "            res = self.ohe.transform(recents).toarray()\n",
    "        else:\n",
    "            res = np.array(recents)\n",
    "        self.dateColumn = res.shape[1]\n",
    "        res = np.hstack([res,nonExpand])\n",
    "        return res\n",
    "  \n",
    "    def getH7(self,removeInsufficient=False, target=None,encode = True,future =0):\n",
    "        #Simple recent win,draw, lose \n",
    "        df = self.df\n",
    "        if removeInsufficient == True:\n",
    "            df= df[df['Sufficient'] == 1]\n",
    "        df=df[df['Future']==future]\n",
    "        \n",
    "        isInput, X, y,target, target_date = self.initData(df,target,encode)\n",
    "        resy=[]\n",
    "        resx=[]\n",
    "        print(\"start format\")\n",
    "        recents = X[['HomeTeam','AwayTeam','Referee']].values\n",
    "        haccp = X['HAccP'].values.reshape(X.shape[0],1)\n",
    "        aaccp = X['AAccP'].values.reshape(X.shape[0],1)\n",
    "        homeRecent = np.hstack([X[['HWin','HDraw','HLose']].values,\n",
    "                                (X['HScore'].values - X['HConcede'].values).reshape(X.shape[0],1)])\n",
    "        awayRecent = np.hstack([X[['AWin','ADraw','ALose']].values,\n",
    "                                (X['AScore'].values - X['AConcede'].values).reshape(X.shape[0],1)])\n",
    "        homeMoral = X['HMoral'].values.reshape(X.shape[0],1)\n",
    "        awayMoral = X['AMoral'].values.reshape(X.shape[0],1)\n",
    "        target_date = target_date.reshape(X.shape[0],1)\n",
    "     \n",
    "        nonExpand =np.hstack([target_date,X[['HRestDay','ARestDay','HS_Acc','AS_Acc','HST_Acc','AST_Acc',\n",
    "                                            'H_poss_Acc','A_poss_Acc','H_atk_3rd_tot_Acc','A_atk_3rd_tot_Acc',\n",
    "                                             'H_atk_3rd_Acc','A_atk_3rd_Acc','H_interceptions_Acc','A_interceptions_Acc'\n",
    "                                            ]].values,haccp-aaccp,(haccp+1)/(aaccp+1),\n",
    "                                homeRecent,awayRecent, homeMoral - awayMoral + haccp - aaccp])\n",
    "        res = self.aggregate(recents,nonExpand,future,encode)\n",
    "        print(\"finish\")\n",
    "        sys.stdout.flush()\n",
    "        return res, np.array(y)\n",
    "    def _getRank(self,x, X,teamName,recentNum):\n",
    "        team = self.getTeam(X,teamName)\n",
    "        prev = team[team['Date'] < x['Date']].values      \n",
    "        for i in range(recentNum):\n",
    "            pass\n",
    "    def initRanking(self, n = 20):\n",
    "        defaultPt = 1\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HPoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"APoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        hpoints= df[\"HPoints\"].values\n",
    "        apoints=df[\"APoints\"].values\n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hpoints[i] = 3\n",
    "                apoints[i] = 0\n",
    "            elif ftr[i] == 'D':\n",
    "                hpoints[i] = 1\n",
    "                apoints[i] = 1\n",
    "            else :\n",
    "                hpoints[i] = 0\n",
    "                apoints[i] = 3\n",
    "        df[\"HPoints\"]=hpoints\n",
    "        df[\"APoints\"]=apoints\n",
    "        for teamName in self.teamsMap.keys():\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hpoints = team['HPoints'].values\n",
    "            apoints = team['APoints'].values\n",
    "            psum = 0\n",
    "            haccp = team['HAccP'].values\n",
    "            aaccp = team['AAccP'].values\n",
    "        \n",
    "            for  i in range(0,n):\n",
    "                if i < hpoints.shape[0]:\n",
    "                    psum = psum + (hpoints[i] if hometeam[i] == teamName else apoints[i] ) \n",
    "                else:\n",
    "                    psum = psum + defaultPt        \n",
    "                    \n",
    "        \n",
    "            for j in range(team.shape[0]):\n",
    "\n",
    "                if j+n < hpoints.shape[0]:                     \n",
    "                    psum = psum + (hpoints[j+n] if hometeam[j+n]==teamName else apoints[j+n])\n",
    "                else:\n",
    "                    psum = psum + defaultPt \n",
    "                \n",
    "                psum = psum - (hpoints[j] if hometeam[j]==teamName else apoints[j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    haccp[j]=psum\n",
    "                else:\n",
    "                    aaccp[j]=psum\n",
    "            team['HAccp']=haccp\n",
    "            team['AAccP']=aaccp\n",
    "            #print(team[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "            df.update(team)\n",
    "            \n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initRecentData(self, n =5):\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HDraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ADraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HLose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ALose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "                \n",
    "        df[\"HScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HRestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ARestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        #general\n",
    "        generalList = ['HS','AS','HST','AST','H_poss' ,'A_poss' ,'H_atk_3rd', 'A_atk_3rd','H_atk_3rd_tot','A_atk_3rd_tot'\n",
    "                       ,'H_interceptions' ,'A_interceptions']\n",
    "        generalOutput = []\n",
    "        for attr in generalList:\n",
    "            temp = attr+'_Acc'\n",
    "            df[temp]=pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "            generalOutput.append(temp)\n",
    "        #\n",
    "        df[\"Sufficient\"] = pd.Series(np.ones(shape=(df.shape[0],)))\n",
    "        \n",
    "      \n",
    "            \n",
    "        \n",
    "        hscore = df['FTHG'].values\n",
    "        ascore = df['FTAG'].values\n",
    "        hconcede = df ['FTAG'].values\n",
    "        aconcede = df['FTHG'].values\n",
    "               \n",
    "        hwin = df['HWin'].values\n",
    "        awin = df['AWin'].values\n",
    "        hlose = df['HLose'].values\n",
    "        alose = df['ALose'].values\n",
    "        hdraw = df['HDraw'].values\n",
    "        adraw = df['ADraw'].values\n",
    "        hmoral = df['HMoral'].values\n",
    "        amoral = df['AMoral'].values\n",
    "        \n",
    "        rankRatio = (df['HAccP'].values+1) / (df['AAccP'].values +1)\n",
    "        \n",
    "        \n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hwin[i] = 1\n",
    "                hmoral[i] = 3 * 1/rankRatio[i]\n",
    "                alose[i]= 1\n",
    "                amoral[i] = -3 * 1/rankRatio[i]\n",
    "            elif ftr[i] == 'D':\n",
    "                hdraw[i] = 1\n",
    "                hmoral[i] = 1 * 1/rankRatio[i]\n",
    "                adraw[i] = 1\n",
    "                amoral[i] = 1 * rankRatio[i]\n",
    "            else :\n",
    "                hlose[i] = 1\n",
    "                hmoral[i] = -3*rankRatio[i]\n",
    "                awin [i] = 1\n",
    "                amoral[i] = 3*rankRatio[i]\n",
    "        \n",
    "        \n",
    "        df[\"HWin\"]=hwin\n",
    "        df[\"AWin\"]=awin\n",
    "        df[\"HDraw\"]=hdraw\n",
    "        df[\"ADraw\"]=adraw\n",
    "        df[\"HLose\"]=hlose\n",
    "        df[\"ALose\"]=alose\n",
    "        df[\"HScore\"]=hscore\n",
    "        df[\"AScore\"]=ascore\n",
    "        df[\"HConcede\"]=hconcede\n",
    "        df[\"AConcede\"]=aconcede\n",
    "        df[\"HMoral\"] = hmoral\n",
    "        df[\"AMoral\"] = amoral\n",
    "        \n",
    "        \n",
    "        \n",
    "        for teamName in self.teamsMap.keys():\n",
    "            print(teamName)\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hwin = team[\"HWin\"].values\n",
    "            awin = team[\"AWin\"].values\n",
    "            hlose= team[\"HLose\"].values\n",
    "            alose = team[\"ALose\"].values\n",
    "            hdraw = team[\"HDraw\"].values\n",
    "            adraw = team[\"ADraw\"].values\n",
    "            hscore = team[\"HScore\"].values\n",
    "            ascore = team[\"AScore\"].values\n",
    "            hconcede = team[\"HConcede\"].values\n",
    "            aconcede = team[\"AConcede\"].values\n",
    "            hmoral = team[\"HMoral\"].values\n",
    "            amoral = team[\"AMoral\"].values\n",
    "            hrestday = team[\"HRestDay\"].values\n",
    "            arestday = team[\"ARestDay\"].values\n",
    "            \n",
    "            #general\n",
    "            hs= team[\"HS\"].values\n",
    "            as_ = team[\"AS\"].values\n",
    "            hst = team[\"HST\"].values\n",
    "            ast = team[\"AST\"].values\n",
    "            original_list =[]\n",
    "            output_list=[]\n",
    "            for indx, o_attr in enumerate(generalList):\n",
    "                original_list.append(team[o_attr].values)\n",
    "                output_list.append(team[generalOutput[indx]].values)\n",
    "            \n",
    "            \n",
    "            matchDate =team['Date'].values\n",
    "            sufficient = team['Sufficient'].values\n",
    "            teamMatchesDate = self.teamsData[teamName].sort('1',ascending=False)['1'].values\n",
    "            \n",
    "            restday = 0\n",
    "            winsum =0 \n",
    "            losesum=0\n",
    "            drawsum=0\n",
    "            scoresum =0\n",
    "            concedesum=0\n",
    "            moralsum = 0\n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            teamAttrSum_list=[0 for i in range(int(len(original_list)/2))]\n",
    "            for  i in range(0,n):\n",
    "                if i < team.shape[0]:\n",
    "                    scoresum = scoresum + (hscore[i] if hometeam[i] == teamName else ascore[i])\n",
    "                    winsum = winsum + (hwin[i] if hometeam[i] == teamName else awin[i])\n",
    "                    losesum= losesum + (hlose[i] if hometeam[i] == teamName else alose[i])\n",
    "                    drawsum= drawsum + (hdraw[i] if hometeam[i] == teamName else adraw[i])\n",
    "                    concedesum = concedesum+ (hconcede[i] if hometeam[i] == teamName else aconcede[i])\n",
    "                    moralsum= moralsum+ (hmoral[i] if hometeam[i] == teamName else amoral[i])\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        teamAttrSum_list[attrIndx] +=(original_list[2*attrIndx][i] if hometeam[i] == teamName else original_list[2*attrIndx+1][i])\n",
    "                else:\n",
    "                    # + 0\n",
    "                    pass\n",
    "            dateIndx = 0\n",
    "            for j in range(team.shape[0]):\n",
    "                while True:\n",
    "                    if dateIndx >= teamMatchesDate.shape[0]:\n",
    "                        sufficient[j] = False\n",
    "                        break\n",
    "                    if teamMatchesDate[dateIndx] < matchDate[j] :\n",
    "                        restday = (matchDate[j] - teamMatchesDate[dateIndx])/np.timedelta64(1,'D')\n",
    "                        break\n",
    "                    else:\n",
    "                        dateIndx = dateIndx + 1\n",
    "                \n",
    "                if j+n < team.shape[0]:                     \n",
    "                    scoresum = scoresum + (hscore[j+n] if hometeam[j+n] == teamName else ascore[j+n])\n",
    "                    winsum = winsum + (hwin[j+n] if hometeam[j+n] == teamName else awin[j+n])\n",
    "                    losesum= losesum + (hlose[j+n] if hometeam[j+n] == teamName else alose[j+n])\n",
    "                    drawsum= drawsum + (hdraw[j+n] if hometeam[j+n] == teamName else adraw[j+n])\n",
    "                    concedesum = concedesum+ (hconcede[j+n] if hometeam[j+n] == teamName else aconcede[j+n])\n",
    "                    moralsum= moralsum+ (hmoral[j+n] if hometeam[j+n] == teamName else amoral[j+n])\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        teamAttrSum_list[attrIndx] += (original_list[2*attrIndx][j+n] if hometeam[j+n] == teamName else original_list[2*attrIndx+1][j+n])\n",
    "                else:\n",
    "                    sufficient[j] = False\n",
    "                    \n",
    "                \n",
    "                scoresum = scoresum - (hscore[j] if hometeam[j] == teamName else ascore[j])\n",
    "                winsum = winsum - (hwin[j] if hometeam[j] == teamName else awin[j])\n",
    "                losesum= losesum - (hlose[j] if hometeam[j] == teamName else alose[j])\n",
    "                drawsum= drawsum - (hdraw[j] if hometeam[j] == teamName else adraw[j])\n",
    "                concedesum = concedesum - (hconcede[j] if hometeam[j] == teamName else aconcede[j])\n",
    "                moralsum= moralsum - (hmoral[j] if hometeam[j] == teamName else amoral[j])\n",
    "                for attrIndx in range(len(teamAttrSum_list)):\n",
    "                    teamAttrSum_list[attrIndx] -=  (original_list[2*attrIndx][j] if hometeam[j] == teamName else original_list[2*attrIndx+1][j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    hscore[j] = scoresum\n",
    "                    hwin[j] = winsum\n",
    "                    hlose[j] = losesum\n",
    "                    hdraw[j] = drawsum\n",
    "                    hconcede[j] = concedesum\n",
    "                    hmoral[j] = moralsum\n",
    "                    hrestday[j] = restday\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        output_list[2*attrIndx][j] = teamAttrSum_list[attrIndx]\n",
    "                else:\n",
    "                    ascore[j] = scoresum\n",
    "                    awin[j] = winsum\n",
    "                    alose[j] = losesum\n",
    "                    adraw[j] = drawsum\n",
    "                    aconcede[j] = concedesum\n",
    "                    amoral[j] = moralsum\n",
    "                    arestday[j] = restday\n",
    "                    for attrIndx in range(len(teamAttrSum_list)):\n",
    "                        output_list[2*attrIndx+1][j] = teamAttrSum_list[attrIndx]\n",
    "            team[\"HWin\"]=hwin\n",
    "            team[\"AWin\"]=awin\n",
    "            team[\"HDraw\"]=hdraw\n",
    "            team[\"ADraw\"]=adraw\n",
    "            team[\"HLose\"]=hlose\n",
    "            team[\"ALose\"]=alose\n",
    "            team[\"HScore\"]=hscore\n",
    "            team[\"AScore\"]=ascore\n",
    "            team[\"HConcede\"]=hconcede\n",
    "            team[\"AConcede\"]=aconcede\n",
    "            team[\"HMoral\"] = hmoral\n",
    "            team[\"AMoral\"] = amoral\n",
    "            team['Sufficient'] = sufficient\n",
    "            for indx in range(len(output_list)):\n",
    "                team[generalOutput[indx]]= output_list[indx]\n",
    "        \n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            df.update(team)\n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initTeamData(self):\n",
    "        self.teamsData={}\n",
    "        self.teamsById={}\n",
    "        for name in self.teamsMap.keys():\n",
    "            self.readTeamMatch(name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "c = FootballDataHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:23: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:24: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#c.readFootBallData(\"E0_1112.csv\")\n",
    "c.readFootBallData(2012)\n",
    "c.readFootBallData(2013)\n",
    "c.readFootBallData(2014)\n",
    "c.readFootBallData(2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>A_Poss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>37.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>Man City</td>\n",
       "      <td>45.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>46.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>44.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Everton</td>\n",
       "      <td>46.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>40.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>Watford</td>\n",
       "      <td>47.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>49.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>50.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>2016-03-13</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>62.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Norwich</td>\n",
       "      <td>66.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>53.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>61.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>51.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Everton</td>\n",
       "      <td>58.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Man City</td>\n",
       "      <td>28.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>50.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>44.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>2016-03-05</td>\n",
       "      <td>Watford</td>\n",
       "      <td>49.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>65.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>44.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Man United</td>\n",
       "      <td>38.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>37.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2012-09-15</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>48.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2012-09-02</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>31.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>55.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>38.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>Man City</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>51.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>66.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>37.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Norwich</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Man United</td>\n",
       "      <td>40.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>47.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2012-08-25</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>60.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012-08-22</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>28.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012-08-20</td>\n",
       "      <td>Everton</td>\n",
       "      <td>69.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-08-19</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012-08-19</td>\n",
       "      <td>Man City</td>\n",
       "      <td>35.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>48.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Reading</td>\n",
       "      <td>47.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>QPR</td>\n",
       "      <td>50.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>West Brom</td>\n",
       "      <td>59.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>65.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>29.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        HomeTeam  A_Poss\n",
       "1442 2016-03-20       Tottenham    37.2\n",
       "1441 2016-03-20     Southampton    50.8\n",
       "1440 2016-03-20       Newcastle    40.1\n",
       "1439 2016-03-20        Man City    45.5\n",
       "1436 2016-03-19         Swansea    46.2\n",
       "1434 2016-03-19  Crystal Palace    44.1\n",
       "1435 2016-03-19         Everton    46.1\n",
       "1433 2016-03-19         Chelsea    40.6\n",
       "1437 2016-03-19         Watford    47.9\n",
       "1438 2016-03-19       West Brom    49.9\n",
       "1432 2016-03-14       Leicester    50.1\n",
       "1431 2016-03-13     Aston Villa    62.2\n",
       "1430 2016-03-12           Stoke    39.4\n",
       "1429 2016-03-12         Norwich    66.1\n",
       "1428 2016-03-12     Bournemouth    56.0\n",
       "1427 2016-03-06       West Brom    53.4\n",
       "1426 2016-03-06  Crystal Palace    61.5\n",
       "1422 2016-03-05     Southampton    34.9\n",
       "1418 2016-03-05         Chelsea    51.3\n",
       "1419 2016-03-05         Everton    58.5\n",
       "1420 2016-03-05        Man City    28.8\n",
       "1421 2016-03-05       Newcastle    50.7\n",
       "1423 2016-03-05         Swansea    44.2\n",
       "1424 2016-03-05       Tottenham    48.2\n",
       "1425 2016-03-05         Watford    49.8\n",
       "1417 2016-03-02        West Ham    65.1\n",
       "1416 2016-03-02           Stoke    44.7\n",
       "1415 2016-03-02      Man United    38.8\n",
       "1414 2016-03-02       Liverpool    50.5\n",
       "1413 2016-03-02         Arsenal    37.4\n",
       "...         ...             ...     ...\n",
       "36   2012-09-15      Sunderland    65.9\n",
       "28   2012-09-02     Southampton    54.9\n",
       "27   2012-09-02       Newcastle    48.3\n",
       "26   2012-09-02       Liverpool    47.0\n",
       "25   2012-09-01           Wigan    31.3\n",
       "24   2012-09-01        West Ham    54.9\n",
       "23   2012-09-01       West Brom    55.7\n",
       "22   2012-09-01       Tottenham    38.4\n",
       "21   2012-09-01         Swansea    36.2\n",
       "20   2012-09-01        Man City    40.1\n",
       "18   2012-08-26       Liverpool    51.3\n",
       "19   2012-08-26           Stoke    66.7\n",
       "17   2012-08-25       Tottenham    41.0\n",
       "16   2012-08-25         Swansea    37.8\n",
       "15   2012-08-25     Southampton    50.8\n",
       "14   2012-08-25         Norwich    44.9\n",
       "13   2012-08-25      Man United    40.1\n",
       "12   2012-08-25         Chelsea    47.5\n",
       "11   2012-08-25     Aston Villa    60.8\n",
       "10   2012-08-22         Chelsea    28.4\n",
       "9    2012-08-20         Everton    69.2\n",
       "8    2012-08-19           Wigan    48.0\n",
       "7    2012-08-19        Man City    35.9\n",
       "2    2012-08-18       Newcastle    48.1\n",
       "4    2012-08-18         Reading    47.4\n",
       "3    2012-08-18             QPR    50.1\n",
       "1    2012-08-18          Fulham    40.2\n",
       "5    2012-08-18       West Brom    59.5\n",
       "6    2012-08-18        West Ham    65.8\n",
       "0    2012-08-18         Arsenal    29.9\n",
       "\n",
       "[1443 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.df[['Date','HomeTeam','A_Poss']].sort(columns='Date',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "c.readFuture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:140: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c.initTeamData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:224: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df = c.initRanking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:284: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:400: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:479: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:480: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:481: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:484: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:485: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:486: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:487: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:488: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:489: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:492: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burnley\n",
      "West Ham\n",
      "West Brom\n",
      "Aston Villa\n",
      "Arsenal\n",
      "Crystal Palace\n",
      "Norwich\n",
      "Man United\n",
      "Hull\n",
      "Chelsea\n",
      "QPR\n",
      "Watford\n",
      "Cardiff\n",
      "Fulham\n",
      "Everton\n",
      "Sunderland\n",
      "Swansea\n",
      "Reading\n",
      "Man City\n",
      "Leicester\n",
      "Southampton\n",
      "Bournemouth\n",
      "Stoke\n",
      "Tottenham\n",
      "Newcastle\n",
      "Wigan\n",
      "Liverpool\n"
     ]
    }
   ],
   "source": [
    "df=c.initRecentData(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.saveDf('dataSet/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.loadDf('dataSet/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y=c.getH7(removeInsufficient=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "def plotErrorDate(X_test, X_err, dateCol = 10):\n",
    "    X_test_date =np.sort(X_test[:,c.dateColumn])\n",
    "    X_date=[]\n",
    "    y_date=[]\n",
    "    for v in X_test_date:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_date) ==0  or X_date[-1] != date:\n",
    "            X_date.append(date)\n",
    "            y_date.append(1)\n",
    "        else:\n",
    "            y_date[-1] = y_date[-1] +1\n",
    "    plt.plot_date(X_date,y_date,xdate=True)\n",
    "    X_err_d = np.sort(X_err[:,c.dateColumn])\n",
    "    X_err_date=[]\n",
    "    y_err_date = []\n",
    "    for v in X_err_d:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_err_date) ==0  or X_err_date[-1] != date:\n",
    "            X_err_date.append(date)\n",
    "            y_err_date.append(1)\n",
    "        else:\n",
    "            y_err_date[-1] = y_err_date[-1] +1\n",
    "    plt.plot_date(X_err_date,y_err_date,xdate=True,color='red')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "g_hiddenNodes = int(45)\n",
    "g_alpha = 0 \n",
    "clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_hiddenNodes], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "              learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=1000,early_stopping = False,verbose = 3)\n",
    "mlp = Pipeline([ ('scl', StandardScaler()),('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"start learning\")\n",
    "sys.stdout.flush()\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=mlp, \n",
    "                       X=X, \n",
    "                      y=y, \n",
    "                      train_sizes=np.linspace(0.1, 1.0, 4), \n",
    "                      cv=4,\n",
    "                     n_jobs=1,verbose=3)\n",
    "print(\"finishing\")   \n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "print(test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learningRes = np.vstack([train_sizes,train_mean,train_std,test_mean,test_std]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learningDf = pd.DataFrame(learningRes,columns=['size','train_mean','train_std','test_mean','test_std'])\n",
    "print(learningDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotCurve(train_mean,train_std,test_mean,test_std,train_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate(mlp, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    firstNScores = []\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "\n",
    "        mlp.fit(X[train], y[train])\n",
    "        score = mlp.score(X[test], y[test])\n",
    "        firstNScores.append(firstNScore(2, mlp.predict_proba(X[test]), y[test]))\n",
    "        train_scores.append(mlp.score(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, firstNScores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def lamda_test(mlp, X, y, lamdas):\n",
    "    \n",
    "    train_scores=[]\n",
    "    test_scores=[]\n",
    "    for lamda in lamdas:\n",
    "        clf.set_params(alpha= lamda)\n",
    "        train_s, test_s, firstNScores = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_s, test_s, firstNScores =futureTest(mlp,X,y,numOfWeek=20) \n",
    "        train_scores.append(train_s)\n",
    "        test_scores.append(test_s)\n",
    "        print(\"lamda: {}, train: {}, test: {}\".format(lamda, \n",
    "                    np.mean(train_s), np.mean(test_s)) )\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plotCurve(train_mean,train_std,test_mean,test_std,lamdas)\n",
    "    return np.array(train_scores),np.array(test_scores)\n",
    "\n",
    "l_range = []\n",
    "for i in range(0,50):\n",
    "    l_range.append(2*i)\n",
    "train_scores,test_scores = lamda_test(mlp,X,y,np.array(l_range))\n",
    "#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "alphaRes = np.vstack([l_range,train_mean,train_std,test_mean,test_std]).T\n",
    "alphaDf = pd.DataFrame(alphaRes,columns=['alpha','train_mean','train_std','test_mean','test_std'])\n",
    "print(alphaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testNodeSize(start ,end):\n",
    "    node_range = range(start,end,5)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    for node in node_range:   \n",
    "        print(\"start node:{}\".format(node))\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[node], activation='logistic', algorithm='l-bfgs', alpha=0, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores,first2 = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_scores,test_scores , first2= futureTest(mlp, X,y,numOfWeek = 10)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        print(\"Node {}: train_mean {}  v.s. test_mean {}\".format(node,np.mean(train_scores),np.mean(test_scores)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(node_range))\n",
    "    return node_range, train_means,train_std,test_means,test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node_range, train_means,train_std,test_means,test_std=testNodeSize(1,X.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodeRes = np.vstack([node_range,train_means,train_std,test_means,test_std]).T\n",
    "nodeDf = pd.DataFrame(nodeRes,columns=['nodeNum','train_mean','train_std','test_mean','test_std'])\n",
    "print(nodeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.set_params(alpha=g_alpha)\n",
    "print(clf)\n",
    "train_score, test_score, first2 = futureTest(mlp,X,y,numOfWeek = 30, verbose=True)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testRecentNum(start, end):\n",
    "    recent_range = range(start,end)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    first2_mean=[]\n",
    "    for recent in recent_range:\n",
    "        print(\"start recent:{}\".format(recent))\n",
    "        X,y = c.getH7(recent)\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_alpha], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores, first2 = crossValidate(mlp,X,y,fold=10)\n",
    "        #train_scores,test_scores, first2 = futureTest(mlp, X,y,numOfWeek = 15)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        first2_mean.append(np.mean(first2))\n",
    "        print(\"recent {}: train_mean {}  v.s. test_mean {} , first2_mean {}\".format(\n",
    "                recent,np.mean(train_scores),np.mean(test_scores),np.mean(first2)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(recent_range))\n",
    "    return train_means,train_std,test_means,test_std,first2_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_means,train_std,test_means,test_std,first2_mean=testRecentNum(1 ,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "proba = mlp.predict_proba(X_test)\n",
    "precisionMatrix(proba,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#future\n",
    "mlp.fit(X,y)\n",
    "inData = c.readPredict(\"future.csv\")\n",
    "print(inData)\n",
    "X_in, y_in = c.getH6(5,target=inData)\n",
    "res = mlp.predict(X_in)\n",
    "proba= mlp.predict_proba(X_in)\n",
    "print(mlp.score(X_in,y_in))\n",
    "print (np.hstack([proba,y_in]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "return\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient=True, encode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1358, 29)\n",
      "                home         away        Referee   time HRestTime ARestTime  \\\n",
      "0             Fulham     Man City       M Halsey  15612         7         3   \n",
      "1            Norwich    Liverpool        M Jones  15612         2         2   \n",
      "2            Everton  Southampton      L Probert  15612         3         3   \n",
      "3              Stoke      Swansea         J Moss  15612         7         3   \n",
      "4            Arsenal      Chelsea     M Atkinson  15612         2         3   \n",
      "5         Man United    Tottenham          C Foy  15612         2         2   \n",
      "6        Aston Villa    West Brom       A Taylor  15613         4         3   \n",
      "7                QPR     West Ham  M Clattenburg  15614         4         5   \n",
      "8              Wigan      Everton       K Friend  15619         7         7   \n",
      "9          West Brom          QPR        M Jones  15619         6         4   \n",
      "10          Man City   Sunderland      L Probert  15619         2         7   \n",
      "11           Chelsea      Norwich       A Taylor  15619         3         7   \n",
      "12          West Ham      Arsenal         P Dowd  15619         4         2   \n",
      "13           Swansea      Reading         M Dean  15619         7         7   \n",
      "14       Southampton       Fulham  M Clattenburg  15620         8         8   \n",
      "15         Tottenham  Aston Villa    N Swarbrick  15620         2         7   \n",
      "16         Liverpool        Stoke        L Mason  15620         2         8   \n",
      "17         Newcastle   Man United         H Webb  15620         2         4   \n",
      "18           Norwich      Arsenal      L Probert  15633        14        13   \n",
      "19          West Ham  Southampton    N Swarbrick  15633        13        13   \n",
      "20         West Brom     Man City  M Clattenburg  15633        14        14   \n",
      "21           Swansea        Wigan        M Jones  15633        14        14   \n",
      "22        Man United        Stoke       A Taylor  15633        13        13   \n",
      "23            Fulham  Aston Villa          C Foy  15633        13        13   \n",
      "24         Liverpool      Reading         R East  15633        13        14   \n",
      "25         Tottenham      Chelsea         M Dean  15633        13        14   \n",
      "26               QPR      Everton         J Moss  15634        15        15   \n",
      "27        Sunderland    Newcastle     M Atkinson  15634        15        14   \n",
      "28          Man City      Swansea     M Atkinson  15640         2         7   \n",
      "29           Arsenal          QPR       A Taylor  15640         2         6   \n",
      "...              ...          ...            ...    ...       ...       ...   \n",
      "1328           Stoke    Newcastle    N Swarbrick  16862         4        17   \n",
      "1329      Man United      Watford        M Jones  16862         3         4   \n",
      "1330        West Ham    Tottenham     A Marriner  16862         4         3   \n",
      "1331         Arsenal      Swansea       R Madley  16862         3         3   \n",
      "1332       Liverpool     Man City     M Atkinson  16862         2         2   \n",
      "1333         Chelsea        Stoke  M Clattenburg  16865         3         2   \n",
      "1334     Southampton   Sunderland    N Swarbrick  16865         3         3   \n",
      "1335       Newcastle  Bournemouth      P Tierney  16865         2         3   \n",
      "1336         Everton     West Ham       A Taylor  16865         3         2   \n",
      "1337         Swansea      Norwich       C Pawson  16865         2         3   \n",
      "1338       Tottenham      Arsenal       M Oliver  16865         2         2   \n",
      "1339         Watford    Leicester         J Moss  16865         2         3   \n",
      "1340        Man City  Aston Villa        L Mason  16865         2         3   \n",
      "1341       West Brom   Man United         M Dean  16866         4         3   \n",
      "1342  Crystal Palace    Liverpool     A Marriner  16866         4         3   \n",
      "1343     Bournemouth      Swansea         R East  16872         7         7   \n",
      "1344           Stoke  Southampton        L Mason  16872         7         7   \n",
      "1345         Norwich     Man City         J Moss  16872         7         7   \n",
      "1346     Aston Villa    Tottenham       A Taylor  16873         8         2   \n",
      "1347       Leicester    Newcastle       C Pawson  16874         8         9   \n",
      "1348       West Brom      Norwich       A Taylor  16879        12         7   \n",
      "1349         Watford        Stoke       C Pawson  16879         6         7   \n",
      "1350         Chelsea     West Ham       R Madley  16879         6         5   \n",
      "1351         Everton      Arsenal  M Clattenburg  16879         6         2   \n",
      "1352  Crystal Palace    Leicester        M Jones  16879         7         4   \n",
      "1353         Swansea  Aston Villa         M Dean  16879         7         5   \n",
      "1354       Tottenham  Bournemouth    N Swarbrick  16880         2         8   \n",
      "1355       Newcastle   Sunderland     M Atkinson  16880         5        15   \n",
      "1356        Man City   Man United       M Oliver  16880         4         2   \n",
      "1357     Southampton    Liverpool         R East  16880         8         2   \n",
      "\n",
      "     HS_Acc AS_Acc HST_Acc AST_Acc ... HWin HDraw HLose H goal Diff AWin  \\\n",
      "0        74     79      56      53 ...    3     0     2           5    2   \n",
      "1        55     79      27      40 ...    0     3     2          -6    0   \n",
      "2        94     59      52      31 ...    3     1     1           4    1   \n",
      "3        46     64      24      41 ...    0     4     1          -1    2   \n",
      "4        71     69      33      38 ...    2     3     0           7    4   \n",
      "5        75     85      41      50 ...    4     0     1           6    2   \n",
      "6        55     58      33      35 ...    1     1     3          -4    3   \n",
      "7        54     61      32      37 ...    0     2     3          -8    2   \n",
      "8        55     95      29      54 ...    1     1     3          -4    3   \n",
      "9        57     47      32      30 ...    2     2     1           0    0   \n",
      "10       80     28      50      17 ...    2     3     0           3    1   \n",
      "11       74     61      40      32 ...    4     1     0           6    0   \n",
      "12       70     71      43      34 ...    2     2     1           1    2   \n",
      "13       60     42      33      18 ...    1     1     3          -4    0   \n",
      "14       58     70      27      50 ...    1     0     4          -7    2   \n",
      "15       83     64      50      35 ...    3     2     0           4    1   \n",
      "16       75     50      39      26 ...    1     2     2           0    1   \n",
      "17       62     78      29      42 ...    1     3     1          -1    4   \n",
      "18       55     75      32      42 ...    0     2     3          -7    3   \n",
      "19       70     57      40      24 ...    2     2     1           2    1   \n",
      "20       60     95      35      61 ...    3     1     1           1    3   \n",
      "21       75     53      42      27 ...    0     2     3          -7    0   \n",
      "22       70     48      40      25 ...    4     0     1           8    1   \n",
      "23       70     65      44      37 ...    2     1     2           0    1   \n",
      "24       76     41      36      21 ...    1     2     2           0    0   \n",
      "25       88     71      50      42 ...    4     1     0           6    4   \n",
      "26       54     92      32      54 ...    0     1     4          -5    2   \n",
      "27       33     64      21      27 ...    1     3     1          -2    1   \n",
      "28       97     79      62      43 ...    3     2     0           5    1   \n",
      "29       77     59      42      33 ...    2     1     2           5    0   \n",
      "...     ...    ...     ...     ... ...  ...   ...   ...         ...  ...   \n",
      "1328     46     70      14      26 ...    2     0     3          -6    2   \n",
      "1329     59     52      24      12 ...    2     1     2           2    2   \n",
      "1330     73    107      20      43 ...    2     2     1           2    5   \n",
      "1331     77     67      28      18 ...    2     1     2           1    1   \n",
      "1332     73     80      27      17 ...    2     1     2           4    2   \n",
      "1333     69     52      20      14 ...    3     2     0           6    3   \n",
      "1334     48     69      13      19 ...    2     1     2          -1    1   \n",
      "1335     52     56      19      17 ...    1     0     4          -8    2   \n",
      "1336     96     77      28      20 ...    3     0     2           6    3   \n",
      "1337     71     57      15      12 ...    1     2     2          -1    0   \n",
      "1338     92     86      34      31 ...    4     0     1           5    2   \n",
      "1339     52     69      10      24 ...    1     2     2          -1    3   \n",
      "1340     67     41      15      12 ...    1     1     3          -5    1   \n",
      "1341     51     65      16      26 ...    2     2     1           1    3   \n",
      "1342     68     65      19      30 ...    0     2     3          -3    3   \n",
      "1343     64     63      18      14 ...    2     1     2           0    2   \n",
      "1344     58     50      19      15 ...    3     1     1           1    2   \n",
      "1345     55     73      13      21 ...    0     1     4          -5    2   \n",
      "1346     37    101      11      39 ...    1     0     4         -11    3   \n",
      "1347     70     55      25      20 ...    3     1     1           3    1   \n",
      "1348     43     48      11       9 ...    3     1     1           2    0   \n",
      "1349     51     60      11      18 ...    1     1     3          -2    3   \n",
      "1350     77     69      23      19 ...    3     2     0           6    3   \n",
      "1351     91     75      34      24 ...    3     0     2           6    2   \n",
      "1352     68     66      20      19 ...    0     2     3          -3    3   \n",
      "1353     56     38      15      10 ...    2     0     3          -1    0   \n",
      "1354     94     59      38      18 ...    3     1     1           3    3   \n",
      "1355     61     69      19      20 ...    1     0     4          -7    1   \n",
      "1356     82     57      22      22 ...    1     1     3          -2    2   \n",
      "1357     53     71      17      28 ...    2     1     2          -1    3   \n",
      "\n",
      "     ADraw ALose A goal diff moraldiff + h-a  y  \n",
      "0        3     0           3         -5.4577  A  \n",
      "1        2     3          -6         4.26691  A  \n",
      "2        0     4          -6         20.4973  H  \n",
      "3        1     2           3        -0.98787  H  \n",
      "4        1     0           7         -7.3312  A  \n",
      "5        2     1           2         7.24231  A  \n",
      "6        1     1           3        -16.1433  D  \n",
      "7        2     1           1        -16.4869  A  \n",
      "8        1     1           5        -18.8485  D  \n",
      "9        2     3          -4         17.7318  H  \n",
      "10       4     0           1         5.22302  H  \n",
      "11       3     2          -4         26.8163  H  \n",
      "12       2     1           6         1.33957  A  \n",
      "13       2     3          -5         3.22343  D  \n",
      "14       0     3          -1        -9.28572  D  \n",
      "15       2     2          -3         17.1502  H  \n",
      "16       3     1           1        -5.98228  D  \n",
      "17       0     1           6         -7.8114  A  \n",
      "18       1     1           8        -21.3046  H  \n",
      "19       1     3          -5         12.8002  H  \n",
      "20       2     0           6        -6.20904  A  \n",
      "21       2     3          -6        0.178857  H  \n",
      "22       3     1           1         9.43603  H  \n",
      "23       2     2          -3         5.66887  H  \n",
      "24       2     3          -5         7.88105  H  \n",
      "25       1     0           7        -2.72658  A  \n",
      "26       2     1           3        -23.5168  D  \n",
      "27       3     1          -2       -0.225469  D  \n",
      "28       1     3          -6         24.2804  H  \n",
      "29       2     3          -3         12.0548  H  \n",
      "...    ...   ...         ...             ... ..  \n",
      "1328     0     3          -6         3.14025  H  \n",
      "1329     2     1           1        -4.32401  H  \n",
      "1330     0     0           8        -19.9385  H  \n",
      "1331     2     2          -1         13.1779  A  \n",
      "1332     1     2           2        -2.28404  H  \n",
      "1333     0     2          -2         9.61243  D  \n",
      "1334     2     2          -1         1.78991  D  \n",
      "1335     1     2          -1        -12.8138  A  \n",
      "1336     1     1           3        -10.8407  A  \n",
      "1337     1     4          -7         19.0105  H  \n",
      "1338     1     2           1         13.5843  D  \n",
      "1339     1     1           4        -19.0615  A  \n",
      "1340     0     4          -9         13.3512  H  \n",
      "1341     1     1           4        -4.45653  H  \n",
      "1342     1     1           8        -24.5246  A  \n",
      "1343     1     2           0         1.36626  H  \n",
      "1344     1     2          -1         8.06201  A  \n",
      "1345     0     3          -1        -21.0893  D  \n",
      "1346     1     1           2        -30.6047  A  \n",
      "1347     0     4          -9         31.6266  H  \n",
      "1348     2     3          -3         20.5266  A  \n",
      "1349     1     1           3        -10.9102  A  \n",
      "1350     1     1           2         1.42972  D  \n",
      "1351     1     2           1        -2.52855  A  \n",
      "1352     1     1           2        -33.3654  A  \n",
      "1353     0     5         -15         16.7183  H  \n",
      "1354     1     1           3         3.93801  H  \n",
      "1355     3     1           0        -15.3967  D  \n",
      "1356     1     2           0        -4.08739  A  \n",
      "1357     1     1           8           -9.09  H  \n",
      "\n",
      "[1358 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "df = pd.DataFrame(np.hstack([X,y.reshape(y.shape[0],1)]))\n",
    "\n",
    "df.columns = ['home','away','Referee','time','HRestTime','ARestTime','HS_Acc','AS_Acc','HST_Acc','AST_Acc',\n",
    "                                            'H_Poss_Acc','A_Poss_Acc','H_atkPass_tot_Acc','A_atkPass_tot_Acc'\n",
    "                                             ,'H_atkPass_Ok_Acc','A_atkPass_OK_Acc','H_ins_Acc','A_ins_Acc'\n",
    "              ,'HAccP - AAccP','H/A','HWin','HDraw','HLose','H goal Diff',\n",
    "'AWin','ADraw','ALose','A goal diff','moraldiff + h-a',\n",
    "              'y']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('dataSet/V9.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "def splitData(X,y):\n",
    "    X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_test, X_val, y_test,y_val = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "def createModel(hidSize, inputDim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidSize[0], input_dim=inputDim, init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(hidSize[1], init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(3, init='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd = SGD(lr=1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "    return model\n",
    "earlyCallback = EarlyStopping(patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate2(node_sizes, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    proba_test = []\n",
    "    proba_y=[]\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "        earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "        model = createModel(node_sizes,X.shape[1])\n",
    "        history = model.fit(X[train],y[train],verbose=0,nb_epoch=500, validation_split=0.1,show_accuracy=True, callbacks=[earlyCallback])\n",
    "      #  firstNScores.append(firstNScore(2, model.predict_proba(X[test]), y[test]))\n",
    "        score = model.evaluate(X[test],y[test])\n",
    "        proba_test.append(model.predict_proba(X[test]))\n",
    "        proba_y.append(y[test])\n",
    "        train_scores.append(model.evaluate(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, val_loss: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, proba_test,proba_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNodeNum(X,y, sizes):\n",
    "    train_loss=[] \n",
    "    score_loss=[]\n",
    "    for s in sizes:\n",
    "        train_scores,scores,  proba_test,proba_y= crossValidate2([s,s],X,y,fold=5)\n",
    "        print(\"size:{} , val_loss_mean:{}\".format(s,np.mean(scores)))\n",
    "        train_loss.append(train_scores)\n",
    "        score_loss.append(scores)\n",
    "    return train_loss,score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.996\n",
      "Epoch 00119: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.966\n",
      "Epoch 00078: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.983\n",
      "Epoch 00057: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00045: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.034\n",
      "size:15 , val_loss_mean:0.994878006496512\n",
      "Epoch 00090: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.997\n",
      "Epoch 00064: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.961\n",
      "Epoch 00108: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00056: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.000\n",
      "Epoch 00044: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.032\n",
      "size:20 , val_loss_mean:0.997879133147474\n",
      "Epoch 00057: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.985\n",
      "Epoch 00109: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00106: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00081: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.008\n",
      "Epoch 00044: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.042\n",
      "size:25 , val_loss_mean:1.002979383102754\n",
      "Epoch 00064: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00066: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.957\n",
      "Epoch 00044: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.985\n",
      "Epoch 00053: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.993\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.035\n",
      "size:30 , val_loss_mean:0.9919062859579861\n",
      "Epoch 00047: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.985\n",
      "Epoch 00063: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:35 , val_loss_mean:0.9929705902102708\n",
      "Epoch 00048: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.984\n",
      "Epoch 00098: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.987\n",
      "Epoch 00052: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.991\n",
      "Epoch 00042: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.040\n",
      "size:40 , val_loss_mean:0.9931346279183522\n",
      "Epoch 00043: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.984\n",
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00089: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.995\n",
      "Epoch 00042: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.033\n",
      "size:45 , val_loss_mean:0.9946483096903833\n",
      "Epoch 00074: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.995\n",
      "Epoch 00091: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.985\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00027: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.038\n",
      "size:50 , val_loss_mean:0.9952394351575709\n",
      "Epoch 00050: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00080: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00035: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00039: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.040\n",
      "size:55 , val_loss_mean:0.9950326937267647\n",
      "Epoch 00070: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00084: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.965\n",
      "Epoch 00066: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.992\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00027: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:60 , val_loss_mean:0.9955750316447294\n",
      "Epoch 00056: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.988\n",
      "Epoch 00040: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00042: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:65 , val_loss_mean:0.9943353890837905\n",
      "Epoch 00072: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.989\n",
      "Epoch 00080: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.962\n",
      "Epoch 00046: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00038: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 1.000\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:70 , val_loss_mean:0.9954743319077854\n",
      "Epoch 00069: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.990\n",
      "Epoch 00061: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.966\n",
      "Epoch 00045: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.989\n",
      "Epoch 00044: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00030: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.034\n",
      "size:75 , val_loss_mean:0.9945887298112925\n",
      "Epoch 00087: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 1.010\n",
      "Epoch 00074: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.972\n",
      "Epoch 00037: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.984\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.997\n",
      "Epoch 00028: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.039\n",
      "size:80 , val_loss_mean:1.000475552932563\n",
      "Epoch 00045: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.990\n",
      "Epoch 00059: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.963\n",
      "Epoch 00064: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00035: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.994\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.054\n",
      "size:85 , val_loss_mean:0.999098368823273\n",
      "Epoch 00036: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.983\n",
      "Epoch 00056: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.964\n",
      "Epoch 00040: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.993\n",
      "Epoch 00033: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00036: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.051\n",
      "size:90 , val_loss_mean:0.9974389820081926\n",
      "Epoch 00058: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.999\n",
      "Epoch 00071: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.954\n",
      "Epoch 00041: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.987\n",
      "Epoch 00047: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.996\n",
      "Epoch 00029: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:95 , val_loss_mean:0.9948091308396705\n",
      "Epoch 00057: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [479 265 341], val_loss: 0.995\n",
      "Epoch 00067: early stopping\n",
      "273/273 [==============================] - 0s     \n",
      "273/273 [==============================] - 0s     \n",
      "1085/1085 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [479 265 341], val_loss: 0.965\n",
      "Epoch 00040: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [479 266 342], val_loss: 0.992\n",
      "Epoch 00032: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1087/1087 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [479 266 342], val_loss: 0.999\n",
      "Epoch 00026: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1088/1088 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [480 266 342], val_loss: 1.037\n",
      "size:100 , val_loss_mean:0.997403977341019\n"
     ]
    }
   ],
   "source": [
    "sizes= range(15,X.shape[1],5)\n",
    "train_loss,score_loss= testNodeNum(X_scaled,y,sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(15, 104, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npZSKYpoElFIqiv1/XRn3AdGcjBIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f2f0cbfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 30\n"
     ]
    }
   ],
   "source": [
    "print(sizes)\n",
    "loss = (np.mean(score_loss,axis=1))\n",
    "loss_std = (np.std(score_loss,axis=1))\n",
    "plt.plot(sizes,loss)\n",
    "plt.fill_between(sizes,loss-loss_std,loss+loss_std,alpha=0.3)\n",
    "plt.show()\n",
    "print(\"max: {}\".format(sizes[np.argmin(loss,axis=0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def futureTest2(node_sizes, X,y,ori_dates, numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(ori_dates)\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    model = createModel(node_sizes,X.shape[1])\n",
    "    results = None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            print(\"week{}\".format(w))\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "            history = model.fit(X_train,y_train,verbose=0,nb_epoch=500, validation_split=0.1, callbacks=[earlyCallback])\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = model.predict_proba(X_test)\n",
    "            train_proba =model.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            tresult = np.hstack([np.array([w for i in range(proba.shape[0])]).reshape(proba.shape[0],1),\n",
    "                                 ori_dates[start:last].reshape(proba.shape[0],1),stack,proba,y_test])\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "                results =    tresult\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "                results =  np.vstack([results, tresult])\n",
    "            if verbose == True:\n",
    "                print(\"numOftest {} , loss {}\".format(X_test.shape[0],model.evaluate(X_test,y_test)))               \n",
    "                print (tresult)\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "    start = X.shape[0] +index+1\n",
    "    print(\"start compute precision_mat\")\n",
    "    print(X[0:start,:].shape)\n",
    "    print(y[0:start,:].shape)\n",
    "    _,_, proba_test,proba_y = crossValidate2(node_sizes,X[0:start,:],y[0:start,:],fold=10)\n",
    "    p_matrix = precisionMatrix(np.vstack(proba_test),np.vstack(proba_y))\n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    resultdf= pd.DataFrame(results, columns=['week','DayStamp','HomeTeam','AwayTeam','H_prob','D_prob','A_prob','H','D','A'])\n",
    "    return sum_proba, sum_y,resultdf,p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week0\n",
      "Epoch 00109: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1348/1348 [==============================] - 0s     \n",
      "week1\n",
      "Epoch 00028: early stopping\n",
      "5/5 [==============================] - 0s\n",
      "1343/1343 [==============================] - 0s     \n",
      "week2\n",
      "Epoch 00024: early stopping\n",
      "15/15 [==============================] - 0s\n",
      "1328/1328 [==============================] - 0s     \n",
      "week3\n",
      "Epoch 00021: early stopping\n",
      "13/13 [==============================] - 0s\n",
      "1315/1315 [==============================] - 0s     \n",
      "week4\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1305/1305 [==============================] - 0s     \n",
      "week5\n",
      "Epoch 00022: early stopping\n",
      "12/12 [==============================] - 0s\n",
      "1293/1293 [==============================] - 0s     \n",
      "week6\n",
      "Epoch 00021: early stopping\n",
      "8/8 [==============================] - 0s\n",
      "1285/1285 [==============================] - 0s     \n",
      "week7\n",
      "Epoch 00029: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1275/1275 [==============================] - 0s     \n",
      "week8\n",
      "Epoch 00023: early stopping\n",
      "17/17 [==============================] - 0s\n",
      "1258/1258 [==============================] - 0s     \n",
      "week9\n",
      "Epoch 00021: early stopping\n",
      "3/3 [==============================] - 0s\n",
      "1255/1255 [==============================] - 0s     \n",
      "week10\n",
      "Epoch 00021: early stopping\n",
      "11/11 [==============================] - 0s\n",
      "1244/1244 [==============================] - 0s     \n",
      "week11\n",
      "Epoch 00023: early stopping\n",
      "19/19 [==============================] - 0s\n",
      "1225/1225 [==============================] - 0s     \n",
      "week12\n",
      "Epoch 00028: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1215/1215 [==============================] - 0s     \n",
      "week13\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1205/1205 [==============================] - 0s     \n",
      "week14\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1195/1195 [==============================] - 0s     \n",
      "week15\n",
      "Epoch 00022: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1185/1185 [==============================] - 0s     \n",
      "week16\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1175/1175 [==============================] - 0s     \n",
      "week17\n",
      "Epoch 00025: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1165/1165 [==============================] - 0s     \n",
      "week18\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1155/1155 [==============================] - 0s     \n",
      "week19\n",
      "Epoch 00029: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1145/1145 [==============================] - 0s     \n",
      "week20\n",
      "Epoch 00034: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1135/1135 [==============================] - 0s     \n",
      "week21\n",
      "Epoch 00023: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1125/1125 [==============================] - 0s     \n",
      "week22\n",
      "Epoch 00024: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1115/1115 [==============================] - 0s     \n",
      "week23\n",
      "Epoch 00021: early stopping\n",
      "10/10 [==============================] - 0s\n",
      "1105/1105 [==============================] - 0s     \n",
      "week24\n",
      "Epoch 00021: early stopping\n",
      "8/8 [==============================] - 0s\n",
      "1097/1097 [==============================] - 0s     \n",
      "start compute precision_mat\n",
      "(1096, 104)\n",
      "(1096, 3)\n",
      "Epoch 00033: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [440 237 308], val_loss: 1.014\n",
      "Epoch 00039: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [440 237 308], val_loss: 0.969\n",
      "Epoch 00035: early stopping\n",
      "111/111 [==============================] - 0s\n",
      "111/111 [==============================] - 0s\n",
      "985/985 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [440 237 308], val_loss: 0.941\n",
      "Epoch 00034: early stopping\n",
      "110/110 [==============================] - 0s\n",
      "110/110 [==============================] - 0s\n",
      "986/986 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [440 237 309], val_loss: 1.002\n",
      "Epoch 00026: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [440 238 309], val_loss: 0.937\n",
      "Epoch 00032: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 6, Class dist.: [440 238 309], val_loss: 0.992\n",
      "Epoch 00029: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 7, Class dist.: [440 238 309], val_loss: 0.956\n",
      "Epoch 00033: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 8, Class dist.: [440 238 309], val_loss: 0.993\n",
      "Epoch 00035: early stopping\n",
      "109/109 [==============================] - 0s\n",
      "109/109 [==============================] - 0s\n",
      "987/987 [==============================] - 0s     \n",
      "Fold: 9, Class dist.: [440 238 309], val_loss: 0.978\n",
      "Epoch 00054: early stopping\n",
      "108/108 [==============================] - 0s\n",
      "108/108 [==============================] - 0s\n",
      "988/988 [==============================] - 0s     \n",
      "Fold: 10, Class dist.: [441 238 309], val_loss: 1.057\n",
      "summary\n",
      "score:\n",
      "0.48275862069\n",
      "2like\n",
      "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
      ">80       0.8     1.0         22        8   0.733333          0        1   \n",
      "60-80     0.6     0.8         14       14   0.500000          2        1   \n",
      "50-60     0.5     0.6         15       16   0.483871          3        4   \n",
      "40-50     0.4     0.5         16       25   0.390244          9       16   \n",
      "30-40     0.3     0.4         24       33   0.421053         17       45   \n",
      "20-30     0.2     0.3          8       17   0.320000         16       41   \n",
      "<20       0.0     0.2         10       39   0.204082         21       85   \n",
      "\n",
      "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
      ">80     0.000000         16        8   0.666667  \n",
      "60-80   0.666667          7        8   0.466667  \n",
      "50-60   0.428571          4        7   0.363636  \n",
      "40-50   0.360000          6       13   0.315789  \n",
      "30-40   0.274194         14       32   0.304348  \n",
      "20-30   0.280702         20       48   0.294118  \n",
      "<20     0.198113         17       61   0.217949  \n",
      "sum precision:[ 0.51351351  0.36363636  0.49275362]\n"
     ]
    }
   ],
   "source": [
    "sum_proba, sum_y,resultdf,p_matrix= futureTest2([55,55],X_scaled,y,X[:,c.dateColumn],numOfWeek=25,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[lower</th>\n",
       "      <th>upper)</th>\n",
       "      <th>h_Correct</th>\n",
       "      <th>h_Wrong</th>\n",
       "      <th>h_Precent</th>\n",
       "      <th>d_Correct</th>\n",
       "      <th>d_Wrong</th>\n",
       "      <th>d_Precent</th>\n",
       "      <th>a_Correct</th>\n",
       "      <th>a_Wrong</th>\n",
       "      <th>a_Precent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&gt;80</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>206</td>\n",
       "      <td>86</td>\n",
       "      <td>0.705479</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-60</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>66</td>\n",
       "      <td>78</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89</td>\n",
       "      <td>55</td>\n",
       "      <td>0.618056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-50</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>84</td>\n",
       "      <td>88</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "      <td>0.480519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30-40</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>71</td>\n",
       "      <td>119</td>\n",
       "      <td>0.373684</td>\n",
       "      <td>29</td>\n",
       "      <td>66</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>62</td>\n",
       "      <td>137</td>\n",
       "      <td>0.311558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-30</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>40</td>\n",
       "      <td>133</td>\n",
       "      <td>0.231214</td>\n",
       "      <td>209</td>\n",
       "      <td>598</td>\n",
       "      <td>0.258984</td>\n",
       "      <td>54</td>\n",
       "      <td>173</td>\n",
       "      <td>0.237885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>101</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>26</td>\n",
       "      <td>168</td>\n",
       "      <td>0.134021</td>\n",
       "      <td>52</td>\n",
       "      <td>301</td>\n",
       "      <td>0.147309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
       ">80       0.8     1.0          2        2   0.500000          0        0   \n",
       "60-80     0.6     0.8        206       86   0.705479          0        0   \n",
       "50-60     0.5     0.6         66       78   0.458333          0        0   \n",
       "40-50     0.4     0.5         84       88   0.488372          0        0   \n",
       "30-40     0.3     0.4         71      119   0.373684         29       66   \n",
       "20-30     0.2     0.3         40      133   0.231214        209      598   \n",
       "<20       0.0     0.2         20      101   0.165289         26      168   \n",
       "\n",
       "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
       ">80          NaN          0        0        NaN  \n",
       "60-80        NaN         12        7   0.631579  \n",
       "50-60        NaN         89       55   0.618056  \n",
       "40-50        NaN         74       80   0.480519  \n",
       "30-40   0.305263         62      137   0.311558  \n",
       "20-30   0.258984         54      173   0.237885  \n",
       "<20     0.134021         52      301   0.147309  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findRecordsBy(self,df):\n",
    "    #print((df['DayStamp']).values)\n",
    "    #date = convertToDate((df['DayStamp']).values)\n",
    "    #df[\"Date\"] = date\n",
    "    \n",
    "    home = df['HomeTeam'].values\n",
    "    away = df['AwayTeam'].values\n",
    "    origin = self.df[[\"Date\",\"HomeTeam\",\"AwayTeam\",\"JocH\",\"JocD\",\"JocA\"]]\n",
    "    origin[\"DayStamp\"]=(pd.to_numeric(origin['Date'])/1e9/24/60/60).values\n",
    "    origin[\"DayStamp\"] = origin[\"DayStamp\"].apply(lambda x: \"%.f\"%(float(x)))\n",
    "    df['DayStamp']=df['DayStamp'].apply(lambda x: \"%.f\"%(float(x)))\n",
    "    return origin.merge(df,left_on=['DayStamp',\"HomeTeam\",\"AwayTeam\"],right_on=[\"DayStamp\",\"HomeTeam\",\"AwayTeam\"],how='inner')\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "withodds = findRecordsBy(c,resultdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def formatMatrixs(oddDf , precisionDf):\n",
    "    proba_mat = oddDf[['H_prob','D_prob','A_prob']].values\n",
    "    def locatePrecision(proba_mat, precisionDf):\n",
    "        precisionMat = precisionDf.values\n",
    "        pre_cols=[4,7,10]\n",
    "        def convert(proba, pre_col = 4):\n",
    "            if proba < 0.2:\n",
    "                return proba\n",
    "            for i in range(precisionMat.shape[0]):\n",
    "                if precisionMat[i,0] <= proba and proba < precisionMat[i,1] :\n",
    "                    if math.isnan(precisionMat[i,pre_col]):\n",
    "                        return proba\n",
    "                    else:\n",
    "                        return precisionMat[i,pre_col]\n",
    "        h_fproba = np.array([ convert(float(proba)) for proba in proba_mat[:,0] ] )\n",
    "        d_fproba = np.array([ convert(float(proba),pre_col=7) for proba in proba_mat[:,1] ] )\n",
    "        a_fproba = np.array([ convert(float(proba),pre_col=10) for proba in proba_mat[:,2] ] )\n",
    "        return h_fproba, d_fproba,a_fproba\n",
    "    h_fproba, d_fproba,a_fproba =locatePrecision(proba_mat,precisionDf)\n",
    "    fproba_mat = np.hstack([h_fproba,d_fproba,a_fproba]).reshape(3,h_fproba.shape[0]).T\n",
    "    odd_mat = oddDf[['JocH','JocD','JocA']].values\n",
    "    win_mat = None\n",
    "    if 'H' in oddDf.columns:\n",
    "        win_mat = oddDf[['H','D','A']].values\n",
    "    return fproba_mat,odd_mat,win_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fproba_mat,odd_mat,win_mat= formatMatrixs(withodds,p_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strategy1(fproba_mat,odd_mat,win_mat):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    print(exp)\n",
    "    maxi = np.argmax(exp,axis=1)\n",
    "    print(maxi)\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    for i in range(maxi.shape[0]):\n",
    "        if exp[i,maxi[i]] > 1:\n",
    "            expectation = expectation+ exp[i,maxi[i]]\n",
    "            spent = spent+1\n",
    "            if maxi[i] == y_true[i]:\n",
    "                income = income + odd_mat[i,maxi[i]]\n",
    "            \n",
    "            \n",
    "        \n",
    "    print(\"Spent:{}, Income:{}, expectation:{}\".format(spent,income,expectation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.64583333e-01   1.04888476e+00   9.19771802e-01]\n",
      " [  5.66473988e-01   9.46315789e-01   8.10050251e-01]\n",
      " [  1.00604651e+00   8.28748451e-01   7.49339207e-01]\n",
      " [  8.25348837e-01   1.06842105e+00   6.85135487e-01]\n",
      " [  7.10416667e-01   9.58240397e-01   1.18942731e+00]\n",
      " [  9.25833333e-01   7.89900867e-01   8.20704846e-01]\n",
      " [  8.14631579e-01   9.15789474e-01   9.65829146e-01]\n",
      " [  5.75473684e-01   1.09894737e+00   1.27268722e+00]\n",
      " [  1.10000000e+00   6.64499426e-01   5.89955947e-01]\n",
      " [  5.45024949e-01   8.15799257e-01   1.43368421e+00]\n",
      " [  7.05000000e-01   2.88245204e-01   3.66039077e-01]\n",
      " [  1.38570482e-01   5.95355666e-01   1.15540935e+00]\n",
      " [  1.27167630e+00   1.25157895e+00   6.96753247e-01]\n",
      " [  8.15833333e-01   8.28748451e-01   7.55112301e-01]\n",
      " [  1.76369863e+00   7.64002478e-01   4.63625319e-01]\n",
      " [  7.97687861e-01   8.28748451e-01   9.37012987e-01]\n",
      " [  9.24855491e-01   9.61578947e-01   8.79350649e-01]\n",
      " [  7.90136986e-01   8.51365060e-01   1.97585016e+00]\n",
      " [  1.56616438e+00   8.41697646e-01   4.99625045e-01]\n",
      " [  1.26986301e+00   8.28748451e-01   4.29011282e-01]\n",
      " [  1.33410357e-01   2.32364131e-01   2.01801379e+00]\n",
      " [  7.76511628e-01   1.14473684e+00   8.68098715e-01]\n",
      " [  6.23333333e-01   1.06183395e+00   1.46337968e+00]\n",
      " [  9.52894737e-01   1.54538126e+00   4.62003601e-01]\n",
      " [  5.60526316e-01   1.12947368e+00   1.33215859e+00]\n",
      " [  5.86666667e-01   1.19132590e+00   2.04581498e+00]\n",
      " [  4.09297527e-01   8.54646840e-01   1.16842105e+00]\n",
      " [  8.47916667e-01   8.41697646e-01   8.92070485e-01]\n",
      " [  3.53757225e-01   9.19392813e-01   3.46111111e+00]\n",
      " [  9.04315789e-01   9.76842105e-01   6.11365639e-01]\n",
      " [  1.06527397e+00   6.32094269e-01   1.33215859e+00]\n",
      " [  1.00894737e+00   9.15789474e-01   5.78061674e-01]\n",
      " [  3.38888526e-01   1.53021057e+00   7.92857143e-01]\n",
      " [  1.24855491e+00   9.84138786e-01   7.20779221e-01]\n",
      " [  5.47976879e-01   8.02850062e-01   1.66875000e+00]\n",
      " [  6.90000000e-01   2.75979795e-01   6.00001842e-01]\n",
      " [  1.32916667e+00   5.15645201e-01   7.07236181e-01]\n",
      " [  9.71578947e-01   7.56232962e-01   1.23974026e+00]\n",
      " [  3.75705352e-01   6.12238792e-01   1.86315789e+00]\n",
      " [  7.05833333e-01   9.19392813e-01   1.30837004e+00]\n",
      " [  9.90263158e-01   7.89900867e-01   7.63316583e-01]\n",
      " [  7.00000000e-01   1.76087029e-01   2.14034063e-01]\n",
      " [  8.40000000e-01   8.80545229e-01   9.99118943e-01]\n",
      " [  1.82421455e-01   3.37241708e-01   1.41434704e+00]\n",
      " [  1.11465753e+00   6.18740222e-01   2.00081556e-01]\n",
      " [  8.70833333e-01   5.29241375e-01   7.96916300e-01]\n",
      " [  5.96923057e-02   2.28661612e-01   2.91396320e+00]\n",
      " [  4.43930636e-01   1.37283554e+00   8.44493392e-01]\n",
      " [  6.09583333e-01   8.38523984e-01   1.76035242e+00]\n",
      " [  3.81943210e-01   6.46592970e-01   1.01052632e+00]\n",
      " [  6.88604651e-01   1.23631579e+00   1.25471013e+00]\n",
      " [  8.40789474e-01   1.41427622e+00   6.66079295e-01]\n",
      " [  6.93641618e-01   9.15789474e-01   1.07155844e+00]\n",
      " [  7.17473684e-01   1.28827630e+00   7.07070170e-01]\n",
      " [  5.22293696e-01   3.29177056e-01   1.16842105e+00]\n",
      " [  9.66506849e-01   4.88512892e-01   1.61762115e+00]\n",
      " [  4.69364162e-01   9.31052632e-01   1.05929648e+00]\n",
      " [  9.15526316e-01   1.35773010e+00   6.06607930e-01]\n",
      " [  5.94157895e-01   1.05315789e+00   1.22511013e+00]\n",
      " [  4.93263158e-01   1.34315789e+00   1.84361233e+00]\n",
      " [  7.25000000e-01   2.43081844e-01   7.22010934e-01]\n",
      " [  1.01093023e+00   8.02850062e-01   1.01256281e+00]\n",
      " [  1.09883721e+00   9.31052632e-01   6.94625551e-01]\n",
      " [  4.40691769e-01   8.41697646e-01   1.13684211e+00]\n",
      " [  1.09349315e+00   1.01003717e+00   6.09946504e-01]\n",
      " [  1.14287671e+00   6.52304076e-01   5.80617511e-01]\n",
      " [  8.70684211e-01   9.31052632e-01   8.72361809e-01]\n",
      " [  2.20659534e-01   2.23459014e-01   2.14129819e+00]\n",
      " [  1.07232877e+00   6.89399717e-01   6.29358007e-01]\n",
      " [  1.21780110e-01   5.45181097e-01   1.40459972e+00]\n",
      " [  2.79416077e-01   1.08773234e+00   8.84210526e-01]\n",
      " [  1.25511628e+00   7.38104089e-01   8.34974874e-01]\n",
      " [  7.74583333e-01   5.12304589e-01   1.40201005e+00]\n",
      " [  1.14287671e+00   6.39311972e-01   7.28184796e-01]\n",
      " [  8.57083333e-01   4.65976149e-01   1.12160804e+00]\n",
      " [  1.90659105e-01   3.14492559e-01   2.81205118e+00]\n",
      " [  6.48890729e-01   8.54646840e-01   1.11250000e+00]\n",
      " [  7.58578947e-01   7.64002478e-01   1.10603015e+00]\n",
      " [  1.13294798e+00   4.81625319e-01   9.45625000e-01]\n",
      " [  8.35116279e-01   9.76842105e-01   1.09427313e+00]\n",
      " [  6.30692488e-01   5.62733439e-01   1.25052632e+00]\n",
      " [  6.87578947e-01   1.36792854e+00   9.03964758e-01]\n",
      " [  6.58959538e-01   1.46952574e+00   5.35242291e-01]\n",
      " [  1.92832539e-01   1.30007571e+00   1.44155844e+00]\n",
      " [  1.87916667e+00   8.80545229e-01   4.13920705e-01]\n",
      " [  1.02558140e+00   8.15799257e-01   9.65829146e-01]\n",
      " [  6.45000000e-01   2.39673018e-01   1.11847477e+00]\n",
      " [  6.10465116e-01   1.48052632e+00   2.20044053e+00]\n",
      " [  5.50289017e-01   9.76842105e-01   8.16281407e-01]\n",
      " [  5.82947368e-01   1.05315789e+00   1.30837004e+00]\n",
      " [  1.70858352e-01   8.28748451e-01   1.26315789e+00]\n",
      " [  7.47083333e-01   1.11421053e+00   8.03987497e-01]\n",
      " [  6.65157895e-01   1.02263158e+00   9.39647577e-01]\n",
      " [  6.09105263e-01   1.42827928e+00   1.17753304e+00]\n",
      " [  1.26986301e+00   9.92105263e-01   2.97287613e-01]\n",
      " [  4.32369942e-01   1.59146204e+00   1.16834171e+00]\n",
      " [  6.85000000e-01   1.08457519e-01   1.27544224e-01]\n",
      " [  1.32837209e+00   1.35727334e+00   2.73864673e-01]\n",
      " [  6.39767442e-01   1.31263158e+00   1.99823789e+00]\n",
      " [  1.13582192e+00   4.96341486e-01   1.07048458e+00]\n",
      " [  2.37833787e-01   3.61104964e-01   1.38952641e+00]\n",
      " [  7.02526316e-01   9.92105263e-01   1.13718593e+00]\n",
      " [  6.93641618e-01   7.89900867e-01   1.35972222e+00]\n",
      " [  1.20627907e+00   1.68713057e+00   9.47754693e-02]\n",
      " [  1.78255814e+00   2.32852912e-01   1.17430556e+00]\n",
      " [  1.83139535e+00   4.95833778e-01   8.98571429e-01]\n",
      " [  3.46969733e-01   1.80969243e-01   1.37955154e+00]\n",
      " [  8.33315789e-01   9.31052632e-01   7.01762115e-01]\n",
      " [  1.12105263e+00   9.46315789e-01   6.79195980e-01]\n",
      " [  9.94726027e-01   4.10683773e-01   1.62951542e+00]\n",
      " [  9.00000000e-01   1.34518476e-01   9.39198542e-02]\n",
      " [  8.25410959e-01   8.13369647e-01   1.47060174e+00]\n",
      " [  9.71578947e-01   8.85263158e-01   6.18502203e-01]\n",
      " [  6.37853017e-01   9.32342007e-01   9.57986111e-01]\n",
      " [  1.93401882e-01   2.43036830e-01   1.50403104e+00]\n",
      " [  7.17473684e-01   9.46315789e-01   8.80176211e-01]\n",
      " [  9.76744186e-01   5.85876787e-01   1.09045226e+00]\n",
      " [  8.11250000e-01   8.54646840e-01   9.63436123e-01]\n",
      " [  1.22093023e+00   7.89900867e-01   8.03819095e-01]\n",
      " [  1.53541667e+00   7.89900867e-01   4.87665198e-01]\n",
      " [  1.32006787e-01   4.69099820e-02   1.83839131e+00]\n",
      " [  7.58578947e-01   9.31052632e-01   8.08810573e-01]\n",
      " [  6.85000000e-01   5.52140303e-01   1.34974731e-01]\n",
      " [  1.27052632e+00   1.00736842e+00   6.01306533e-01]\n",
      " [  8.54651163e-01   1.90862131e+00   1.37204155e-01]\n",
      " [  7.51445087e-01   7.89900867e-01   1.00909091e+00]\n",
      " [  1.12105263e+00   3.15585675e-01   1.31368421e+00]\n",
      " [  6.70000000e-01   1.50926861e-01   3.94880511e-01]\n",
      " [  8.40789474e-01   9.61578947e-01   6.73215859e-01]\n",
      " [  5.13333333e-01   1.70929368e+00   2.73326913e+00]\n",
      " [  3.07514451e-01   3.21940400e+00   2.96413992e-01]\n",
      " [  8.96842105e-01   9.00526316e-01   8.66130653e-01]\n",
      " [  8.06666667e-01   9.61578947e-01   6.14722835e-01]\n",
      " [  8.22105263e-01   8.54646840e-01   8.72361809e-01]\n",
      " [  7.36098321e-01   4.69333805e-01   1.14947368e+00]\n",
      " [  9.13255814e-01   8.28748451e-01   1.16834171e+00]\n",
      " [  5.66473988e-01   5.84328341e-01   1.60694444e+00]\n",
      " [  8.85631579e-01   9.31052632e-01   6.49427313e-01]\n",
      " [  8.59473684e-01   1.21330968e+00   6.85110132e-01]\n",
      " [  1.35452055e+00   6.66482035e-01   4.39016841e-01]\n",
      " [  8.53630137e-01   1.55684211e+00   2.96412460e-01]\n",
      " [  6.75000000e-01   2.10631993e-01   3.19500191e-01]\n",
      " [  9.18139535e-01   5.86625341e-01   1.13718593e+00]\n",
      " [  1.09489474e+00   1.66525632e-01   1.32881944e+00]\n",
      " [  6.45000000e-01   3.07932513e-02   1.97454769e-01]\n",
      " [  6.30000000e-01   1.34315789e+00   2.11718062e+00]\n",
      " [  2.27382015e-02   2.48590018e-02   1.44497003e+00]\n",
      " [  1.29855452e-01   1.74998501e+00   7.78894472e-01]\n",
      " [  1.19651163e+00   9.31052632e-01   4.82763975e-01]\n",
      " [  1.01642105e+00   1.00736842e+00   7.01005025e-01]\n",
      " [  1.17710526e+00   9.61578947e-01   6.48040201e-01]\n",
      " [  1.20627907e+00   5.99160714e-01   8.16281407e-01]\n",
      " [  8.53630137e-01   7.70761582e-01   1.89184994e+00]\n",
      " [  1.32630137e+00   4.60678872e-01   5.37753880e-01]\n",
      " [  2.02691581e-02   9.54893717e-02   1.43866648e+00]\n",
      " [  2.03657895e+00   1.01003717e+00   7.11168831e-01]\n",
      " [  1.73261633e-01   1.84989464e-01   1.53900649e+00]\n",
      " [  9.52325581e-01   9.92105263e-01   8.08810573e-01]\n",
      " [  2.39551610e-01   2.82449898e+00   8.94954480e-02]\n",
      " [  5.30631579e-01   1.20578947e+00   1.97839196e+00]\n",
      " [  9.31232877e-01   3.07744062e-01   1.95066079e+00]\n",
      " [  5.91250000e-01   1.97770187e+00   5.54385380e-01]\n",
      " [  5.40833333e-01   1.70947368e+00   1.64797436e+00]\n",
      " [  8.70833333e-01   8.41697646e-01   8.56387665e-01]\n",
      " [  1.15744186e+00   8.41697646e-01   8.10050251e-01]\n",
      " [  8.46390347e-02   1.14595463e-01   1.41426298e+00]\n",
      " [  6.75000000e-01   1.45679417e-01   1.01617692e+00]\n",
      " [  2.86991955e-01   1.42207790e+00   2.16233766e+00]\n",
      " [  7.35000000e-01   3.19166817e-02   4.78227758e-02]\n",
      " [  6.70520231e-01   1.49816796e+00   5.47136564e-01]\n",
      " [  5.89595376e-01   1.97518026e+00   4.40229213e-01]\n",
      " [  5.11567737e-03   1.31722367e-01   2.45968300e+00]\n",
      " [  1.45000000e+00   2.19201464e-01   1.11576888e-01]\n",
      " [  5.45578947e-01   9.97087980e-01   1.83819095e+00]\n",
      " [  7.15000000e-01   5.28770819e-02   4.34871867e-01]\n",
      " [  7.13023256e-01   9.71189591e-01   1.47488987e+00]\n",
      " [  6.55000000e-01   2.44452925e-01   2.59152789e-02]\n",
      " [  7.40000000e-01   4.39300460e-01   2.78931198e-01]\n",
      " [  1.10054795e+00   1.73559417e-01   9.36564729e-01]\n",
      " [  2.45801052e-02   1.15943938e-01   2.21869625e+00]\n",
      " [  8.43333333e-01   1.00736842e+00   4.93912641e-01]\n",
      " [  2.44329914e-01   2.53617835e-01   1.52796139e+00]\n",
      " [  1.02377097e-01   1.75977105e-01   1.32619848e+00]\n",
      " [  7.05833333e-01   6.83779845e-01   1.14185022e+00]\n",
      " [  5.82947368e-01   9.58240397e-01   1.55778894e+00]\n",
      " [  9.47639115e-01   7.68804440e-01   8.46315789e-01]\n",
      " [  4.59631579e-01   1.45000000e+00   3.42713568e+00]\n",
      " [  7.73526316e-01   1.00736842e+00   9.50251256e-01]\n",
      " [  1.19931507e+00   3.83994596e-01   9.75330396e-01]\n",
      " [  1.13790698e+00   9.76842105e-01   6.42290749e-01]\n",
      " [  1.78255814e+00   5.23828428e-01   9.03376623e-01]\n",
      " [  1.12138728e+00   1.65209795e+00   3.78237885e-01]\n",
      " [  8.96842105e-01   1.87161314e+00   1.85474576e-01]\n",
      " [  6.05000000e-01   4.33308527e-01   2.49246231e+00]\n",
      " [  6.64583333e-01   1.02298637e+00   1.40352423e+00]\n",
      " [  6.90000000e-01   1.99914592e-01   1.61776959e-01]\n",
      " [  1.50972603e+00   9.76842105e-01   2.23733895e-01]\n",
      " [  7.10000000e-01   9.92105263e-01   8.56387665e-01]\n",
      " [  8.43930636e-01   9.92105263e-01   5.85728643e-01]\n",
      " [  5.95000000e-01   4.56263998e-01   1.16407050e+00]\n",
      " [  1.00874255e-03   4.26445869e-03   1.56788118e+00]\n",
      " [  1.15000000e+00   1.85714384e-02   2.63182626e-02]\n",
      " [  3.83905314e-02   7.55227089e-02   2.16626734e+00]\n",
      " [  9.30000000e-01   3.80540230e-01   2.57548109e-01]\n",
      " [  3.25552694e-01   1.79513299e+00   6.60502513e-01]\n",
      " [  9.04315789e-01   8.54646840e-01   1.21090909e+00]\n",
      " [  9.15526316e-01   9.61578947e-01   8.03819095e-01]\n",
      " [  6.45397170e-02   3.37903798e-01   2.07335143e+00]\n",
      " [  6.91315789e-01   9.92105263e-01   9.03964758e-01]\n",
      " [  1.01000000e+00   1.92135145e-01   3.24519438e-01]\n",
      " [  6.70000000e-01   1.35014322e-01   3.11616648e-01]\n",
      " [  7.36157895e-01   9.46315789e-01   1.10603015e+00]\n",
      " [  1.80697674e+00   1.36473496e-01   8.64935065e-01]\n",
      " [  4.30057803e-01   2.26477604e+00   2.13352661e-01]\n",
      " [  1.17500000e+00   5.07975100e-02   1.38742426e-01]\n",
      " [  1.30197901e-02   1.28965948e-02   1.45189470e+00]\n",
      " [  8.93720930e-01   4.38632047e-01   1.75389610e+00]\n",
      " [  7.02526316e-01   8.02850062e-01   1.19949749e+00]\n",
      " [  6.10000000e-01   3.67348987e-01   4.87255864e-02]\n",
      " [  1.56616438e+00   7.19612353e-02   6.70837004e-01]\n",
      " [  1.35208333e+00   8.28748451e-01   4.15443622e-01]\n",
      " [  6.25000000e-01   1.60458200e-01   2.47386447e-01]\n",
      " [  1.16404110e+00   1.09894737e+00   9.86235864e-02]\n",
      " [  3.10000094e-01   9.76842105e-01   1.52659722e+00]\n",
      " [  9.26736842e-01   8.41697646e-01   7.72663317e-01]\n",
      " [  8.69302326e-01   1.03789474e+00   9.27753304e-01]\n",
      " [  3.39485955e-01   1.70695796e+00   6.89867841e-01]\n",
      " [  9.71666667e-01   4.84145842e-01   6.94625551e-01]\n",
      " [  5.80000000e-01   4.61295113e-02   2.15173166e-01]\n",
      " [  1.10236842e+00   1.39503461e+00   5.11453744e-01]\n",
      " [  8.10697674e-01   1.63627501e+00   3.37408018e-01]\n",
      " [  7.28684211e-01   1.48339252e+00   8.08810573e-01]\n",
      " [  6.20315789e-01   8.80545229e-01   1.43316583e+00]\n",
      " [  1.11357895e+00   9.46315789e-01   6.85427136e-01]\n",
      " [  3.71164348e-01   9.61578947e-01   1.39350649e+00]\n",
      " [  1.67291667e+00   2.27463927e-01   8.69740260e-01]\n",
      " [  1.18186047e+00   8.15799257e-01   8.16281407e-01]\n",
      " [  9.03488372e-01   1.55270901e+00   2.42450181e-01]\n",
      " [  1.00116279e+00   1.59678782e+00   2.49596006e-01]\n",
      " [  7.97191781e-01   5.49318071e-01   3.56828194e+00]\n",
      " [  2.07500000e+00   9.74558303e-02   2.88131184e-01]\n",
      " [  5.15684211e-01   1.06183395e+00   2.14974874e+00]\n",
      " [  2.49069767e+00   2.90477177e-01   7.30389610e-01]\n",
      " [  1.24869863e+00   4.11302483e-01   9.27753304e-01]\n",
      " [  1.12105263e+00   9.31052632e-01   5.28105727e-01]\n",
      " [  1.07938356e+00   6.69553046e-01   1.04865170e-01]\n",
      " [  7.77263158e-01   9.31052632e-01   1.02814070e+00]\n",
      " [  5.90421053e-01   1.09894737e+00   1.52663317e+00]\n",
      " [  1.11348837e+00   1.54190421e+00   3.18382582e-01]\n",
      " [  1.68882115e-01   1.04962772e-02   2.60338756e+00]\n",
      " [  1.13302326e+00   1.43917839e+00   2.66420441e-01]\n",
      " [  5.79228871e-02   2.28415146e+00   7.49339207e-01]\n",
      " [  5.66511628e-01   1.55390335e+00   2.85462555e+00]\n",
      " [  1.00116279e+00   8.41697646e-01   7.49339207e-01]\n",
      " [  9.47976879e-01   1.68491142e-01   1.10526316e+00]\n",
      " [  8.60000000e-01   7.21381791e-03   1.90266100e-01]\n",
      " [  8.25000000e-01   8.41697646e-01   9.51541850e-01]\n",
      " [  1.85357105e-01   1.31115089e+00   1.37208333e+00]\n",
      " [  1.21387283e+00   4.32384468e-01   9.34736842e-01]\n",
      " [  5.90000000e-01   2.73064724e-01   8.19556806e-02]\n",
      " [  7.27258379e-01   5.85909194e-01   1.13684211e+00]]\n",
      "[1 1 0 1 2 0 2 2 0 2 0 2 0 1 0 2 1 2 0 0 2 1 2 1 2 2 2 2 2 1 2 0 1 0 2 0 0\n",
      " 2 2 2 0 0 2 2 0 0 2 1 2 2 2 1 2 1 2 2 2 1 2 2 0 2 0 2 0 0 1 2 0 2 1 0 2 0\n",
      " 2 2 2 2 0 2 2 1 1 2 0 0 2 2 1 2 2 1 1 1 0 1 0 1 2 0 2 2 2 1 0 0 2 1 0 2 0\n",
      " 2 0 2 2 1 2 2 0 0 2 1 0 0 1 2 2 0 1 2 1 1 1 2 2 2 2 1 1 0 1 0 2 2 0 2 2 1\n",
      " 0 0 0 0 2 0 2 0 2 1 1 2 2 1 1 0 0 2 2 2 0 1 1 2 0 2 0 2 0 0 0 2 1 2 2 2 2\n",
      " 0 2 1 0 0 0 1 1 2 2 0 0 1 1 2 2 0 2 0 1 2 1 2 1 0 0 2 0 1 0 2 2 2 0 0 0 0\n",
      " 0 2 0 1 1 0 0 1 1 1 2 0 2 0 0 1 1 2 0 2 0 0 0 0 2 2 1 2 1 1 2 0 2 0 2 2 0\n",
      " 0 2]\n",
      "Spent:205, Income:237.01000000000005, expectation:310.21796278721484\n"
     ]
    }
   ],
   "source": [
    "strategy1(fproba_mat,odd_mat,win_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy3(fproba_mat,odd_mat,win_mat,info, z = 0.5  ):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    total = fproba_mat.shape[0]\n",
    "    withdraw = 0\n",
    "    buy = 0\n",
    "    homeTeam = info[\"HomeTeam\"].values\n",
    "    awayTeam = info[\"AwayTeam\"].values\n",
    "    receipt = []\n",
    "    def chooseFrom(match, chosable):\n",
    "        max_prob = -1\n",
    "        c = 0\n",
    "        for choice in chosable:\n",
    "            if fproba_mat[match,choice] > max_prob:\n",
    "                c = choice\n",
    "                max_prob = fproba_mat[match,choice]\n",
    "        return c\n",
    "    for match in range(exp.shape[0]):\n",
    "        chosable = []\n",
    "        for choose in range(3):\n",
    "            if exp[match,choose]> 1 and fproba_mat[match,choose] >= z:\n",
    "                chosable.append(choose)\n",
    "                \n",
    "        if len(chosable) == 0:\n",
    "            withdraw= withdraw+1\n",
    "            continue\n",
    "        choice = chooseFrom(match,chosable)\n",
    "        expectation = expectation+ exp[match,choice]\n",
    "        spent = spent+1\n",
    "        buy =buy +1 \n",
    "        receipt.append(np.hstack([homeTeam[match],awayTeam[match], odd_mat[match,choice],choice,y_true[match],fproba_mat[match,:]]))\n",
    "        if choice == y_true[match]:\n",
    "            income = income + odd_mat[match,choice]\n",
    "    print(buy)\n",
    "    print(\"Spent:{}, Income:{}, expectation:{}, withdraw:{}(total:{})\".\n",
    "          format(spent,income,expectation,withdraw,total))\n",
    "    receiptDf = pd.DataFrame(receipt,columns=[\"home\",\"away\",\"odd of choice\",\"choice\",\"result\",\"Hp\",\"Dp\",\"Ap\"])\n",
    "    return spent,income,expectation,withdraw,total,receiptDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "Spent:90, Income:91.14999999999998, expectation:139.70355912289673, withdraw:171(total:261)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy3(fproba_mat,odd_mat,win_mat,withodds,z=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "receipt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_range = range(30,80,2)\n",
    "spents = []\n",
    "incomes = []\n",
    "exps = []\n",
    "withs = []\n",
    "for z in z_range:\n",
    "    fz = z/100\n",
    "    print(z)\n",
    "    spent,income,expectation,withdraw,total,_ = strategy3(fproba_mat,odd_mat,win_mat,withodds,z=fz) \n",
    "    spents.append(spent)\n",
    "    incomes.append(income)\n",
    "    exps.append(expectation)\n",
    "    withs.append(withdraw)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(z_range, spents)\n",
    "plt.plot(z_range, incomes,color='green')\n",
    "plt.plot(z_range, np.array(incomes)/np.array(spents)*100,color='red')\n",
    "plt.plot(z_range,exps,color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy4(fproba_mat,odd_mat,win_mat,info, z = 0.2 ,e =1.8 ):\n",
    "    exp = odd_mat * fproba_mat\n",
    "    y_true=None\n",
    "    if win_mat is None:\n",
    "        y_true = np.zeros(shape=(fproba_mat.shape[0],))\n",
    "    else:\n",
    "        y_true = np.argmax(win_mat,axis=1)\n",
    "    spent = 0\n",
    "    expectation = 0\n",
    "    income = 0\n",
    "    total = fproba_mat.shape[0]\n",
    "    withdraw = 0\n",
    "    buy = 0\n",
    "    homeTeam = info[\"HomeTeam\"].values\n",
    "    awayTeam = info[\"AwayTeam\"].values\n",
    "    receipt = []\n",
    "    def chooseFrom(match, chosable):\n",
    "        max_prob = -1\n",
    "        c = 0\n",
    "        for choice in chosable:\n",
    "            if fproba_mat[match,choice] > max_prob:\n",
    "                c = choice\n",
    "                max_prob = fproba_mat[match,choice]\n",
    "        return c\n",
    "    for match in range(exp.shape[0]):\n",
    "        chosable = []\n",
    "        for choose in range(3):\n",
    "            if exp[match,choose]> e and fproba_mat[match,choose] >= z:\n",
    "                chosable.append(choose)\n",
    "                \n",
    "        if len(chosable) == 0:\n",
    "            withdraw= withdraw+1\n",
    "            continue\n",
    "        choice = chooseFrom(match,chosable)\n",
    "        expectation = expectation+ exp[match,choice]\n",
    "        spent = spent+1\n",
    "        buy =buy +1 \n",
    "        receipt.append(np.hstack([homeTeam[match],awayTeam[match], odd_mat[match,choice],choice,y_true[match],fproba_mat[match,:]]))\n",
    "        if choice == y_true[match]:\n",
    "            income = income + odd_mat[match,choice]\n",
    "    #print(buy)\n",
    "    #print(\"Spent:{}, Income:{}, expectation:{}, withdraw:{}(total:{})\".\n",
    "    #      format(spent,income,expectation,withdraw,total))\n",
    "    receiptDf = pd.DataFrame(receipt,columns=[\"home\",\"away\",\"odd of choice\",\"choice\",\"result\",\"Hp\",\"Dp\",\"Ap\"])\n",
    "    return spent,income,expectation,withdraw,total,receiptDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,win_mat,withodds,z=0.2,e=1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_z_range = np.arange(0,1,0.05)\n",
    "g_e_range = np.arange(1,2,0.05)\n",
    "g_incomes = []\n",
    "g_spents =[]\n",
    "g_withdraws=[]\n",
    "g_exps=[]\n",
    "g_total = 0\n",
    "for e in g_e_range:\n",
    "    for z in g_z_range:\n",
    "        spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,win_mat,withodds,z=z,e=e)\n",
    "        g_spents.append(spent)\n",
    "        g_incomes.append(income)\n",
    "        g_withdraws.append(withdraw)\n",
    "        g_exps.append(expectation)\n",
    "        g_total= total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05401d4cc0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEKCAYAAADDzOROAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWKVdwXkgiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0543e4f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVwAAAEKCAYAAABewe3GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwDUzy4kD18wsJ/8fJGVwFjyDPO0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f053ba88d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEKCAYAAADDzOROAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngRzCZmYFcgibmRXIIWxmViCHsJlZgf4/53fH5svenvcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f053bafa748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWMAAAEKCAYAAADHOTRzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqwGHsZkwgsOaAAAACElEQVRZDfx/wD971MgrgI0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0540056b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05401d4cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "g_n_incomes = np.array(g_incomes).reshape(len(g_e_range),len(g_z_range))\n",
    "g_n_spent = np.array(g_spents).reshape(len(g_e_range),len(g_z_range))\n",
    "g_n_withdraws = np.array(g_withdraws).reshape(len(g_e_range),len(g_z_range))\n",
    "g_n_exps = np.array(g_exps).reshape(len(g_e_range),len(g_z_range))\n",
    "Z, E =np.meshgrid(g_z_range,g_e_range)\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_incomes)\n",
    "plt.title(\"Income\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_incomes-g_n_spent)\n",
    "plt.title(\"Net Income\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_withdraws)\n",
    "plt.title(\"Withdraws\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()\n",
    "CS = plt.contour = plt.contourf(Z,E,g_n_exps - g_n_incomes)\n",
    "plt.title(\"Exps - income\")\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_node_sizes=[55,55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPrecisionMaxtrix(node_sizes,c):\n",
    "    X, y = c.getH7(removeInsufficient=True)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    _,_, proba_test,proba_y = crossValidate2(node_sizes,X_scaled,y,fold=10)\n",
    "    p_matrix = precisionMatrix(np.vstack(proba_test),np.vstack(proba_y)) \n",
    "    return p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "Epoch 00040: early stopping\n",
      "137/137 [==============================] - 0s     \n",
      "137/137 [==============================] - 0s     \n",
      "1221/1221 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [539 298 384], val_loss: 1.004\n",
      "Epoch 00079: early stopping\n",
      "137/137 [==============================] - 0s     \n",
      "137/137 [==============================] - 0s     \n",
      "1221/1221 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [539 298 384], val_loss: 1.016\n",
      "Epoch 00070: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [539 299 384], val_loss: 0.954\n",
      "Epoch 00089: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [539 299 384], val_loss: 0.948\n",
      "Epoch 00108: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [539 299 384], val_loss: 1.027\n",
      "Epoch 00058: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 6, Class dist.: [539 299 384], val_loss: 0.998\n",
      "Epoch 00083: early stopping\n",
      "136/136 [==============================] - 0s     \n",
      "136/136 [==============================] - 0s     \n",
      "1222/1222 [==============================] - 0s     \n",
      "Fold: 7, Class dist.: [539 299 384], val_loss: 1.006\n",
      "Epoch 00030: early stopping\n",
      "135/135 [==============================] - 0s     \n",
      "135/135 [==============================] - 0s     \n",
      "1223/1223 [==============================] - 0s     \n",
      "Fold: 8, Class dist.: [539 299 385], val_loss: 1.044\n",
      "Epoch 00049: early stopping\n",
      "135/135 [==============================] - 0s     \n",
      "135/135 [==============================] - 0s     \n",
      "1223/1223 [==============================] - 0s     \n",
      "Fold: 9, Class dist.: [539 299 385], val_loss: 1.042\n",
      "Epoch 00040: early stopping\n",
      "134/134 [==============================] - 0s     \n",
      "134/134 [==============================] - 0s     \n",
      "1224/1224 [==============================] - 0s     \n",
      "Fold: 10, Class dist.: [540 299 385], val_loss: 1.021\n"
     ]
    }
   ],
   "source": [
    "g_p_matrix= getPrecisionMaxtrix(g_node_sizes,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[lower</th>\n",
       "      <th>upper)</th>\n",
       "      <th>h_Correct</th>\n",
       "      <th>h_Wrong</th>\n",
       "      <th>h_Precent</th>\n",
       "      <th>d_Correct</th>\n",
       "      <th>d_Wrong</th>\n",
       "      <th>d_Precent</th>\n",
       "      <th>a_Correct</th>\n",
       "      <th>a_Wrong</th>\n",
       "      <th>a_Precent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&gt;80</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>194</td>\n",
       "      <td>91</td>\n",
       "      <td>0.680702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-60</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>102</td>\n",
       "      <td>95</td>\n",
       "      <td>0.517766</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>50</td>\n",
       "      <td>0.590164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-50</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>101</td>\n",
       "      <td>130</td>\n",
       "      <td>0.437229</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>71</td>\n",
       "      <td>103</td>\n",
       "      <td>0.408046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30-40</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>86</td>\n",
       "      <td>134</td>\n",
       "      <td>0.390909</td>\n",
       "      <td>95</td>\n",
       "      <td>201</td>\n",
       "      <td>0.320946</td>\n",
       "      <td>76</td>\n",
       "      <td>157</td>\n",
       "      <td>0.326180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-30</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>56</td>\n",
       "      <td>143</td>\n",
       "      <td>0.281407</td>\n",
       "      <td>190</td>\n",
       "      <td>598</td>\n",
       "      <td>0.241117</td>\n",
       "      <td>92</td>\n",
       "      <td>238</td>\n",
       "      <td>0.278788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>37</td>\n",
       "      <td>156</td>\n",
       "      <td>0.191710</td>\n",
       "      <td>46</td>\n",
       "      <td>222</td>\n",
       "      <td>0.171642</td>\n",
       "      <td>67</td>\n",
       "      <td>349</td>\n",
       "      <td>0.161058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       [lower  upper)  h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  \\\n",
       ">80       0.8     1.0         23       10   0.696970          0        0   \n",
       "60-80     0.6     0.8        194       91   0.680702          0        0   \n",
       "50-60     0.5     0.6        102       95   0.517766          0        0   \n",
       "40-50     0.4     0.5        101      130   0.437229          1        5   \n",
       "30-40     0.3     0.4         86      134   0.390909         95      201   \n",
       "20-30     0.2     0.3         56      143   0.281407        190      598   \n",
       "<20       0.0     0.2         37      156   0.191710         46      222   \n",
       "\n",
       "       d_Precent  a_Correct  a_Wrong  a_Precent  \n",
       ">80          NaN          1        2   0.333333  \n",
       "60-80        NaN         48       32   0.600000  \n",
       "50-60        NaN         72       50   0.590164  \n",
       "40-50   0.166667         71      103   0.408046  \n",
       "30-40   0.320946         76      157   0.326180  \n",
       "20-30   0.241117         92      238   0.278788  \n",
       "<20     0.171642         67      349   0.161058  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainedModel(node_sizes,c):\n",
    "    X, y = c.getH7(removeInsufficient=True)\n",
    "    X_scaled = StandardScaler().fit_transform(X) \n",
    "    model = createModel(node_sizes,X_scaled.shape[1])\n",
    "    earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "    history = model.fit(X_scaled,y,verbose=0,nb_epoch=500, validation_split=0.1, callbacks=[earlyCallback])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "Epoch 00092: early stopping\n"
     ]
    }
   ],
   "source": [
    "g_model = getTrainedModel(g_node_sizes,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def predictFuture(node_sizes,c,p_matrix,model):\n",
    "    X_test,y_test = c.getH7(removeInsufficient = True,future=True)\n",
    "    X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "    proba_y = model.predict_proba(X_test_scaled)\n",
    "   \n",
    "    def getTestDf (X_test,proba_y,c):\n",
    "        decoded = oneHotDecode(c, X_test)\n",
    "        homeNames = c.inverseTeamMapping(decoded[:,0])\n",
    "        awayNames = c.inverseTeamMapping(decoded[:,1])\n",
    "        names = np.array([homeNames,awayNames]).T\n",
    "        return pd.DataFrame(np.hstack([names,proba_y]),columns=['HomeTeam','AwayTeam','H_prob','D_prob','A_prob'])\n",
    "   \n",
    "    test_df = getTestDf(X_test,proba_y,c)\n",
    "    test_df = test_df.sort(columns=\"HomeTeam\")\n",
    "    originDf = c.df[c.df[\"Future\"]==1].sort(columns=\"HomeTeam\")\n",
    "    test_df['JocH']=originDf['JocH'].values\n",
    "    test_df['JocD']=originDf['JocD'].values\n",
    "    test_df['JocA']=originDf['JocA'].values\n",
    "    fproba_mat,odd_mat,_= formatMatrixs(test_df,p_matrix)\n",
    "    return fproba_mat,odd_mat,test_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n",
      "7/7 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:15: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:16: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "fproba_mat,odd_mat,test_df=predictFuture(g_node_sizes,c,g_p_matrix,g_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Spent:1, Income:0, expectation:1.6114285714285712, withdraw:6(total:7)\n"
     ]
    }
   ],
   "source": [
    "spent,income,expectation,withdraw,total,receipt = strategy4(fproba_mat,odd_mat,None,test_df,z=0.2,e=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "      <th>odd of choice</th>\n",
       "      <th>choice</th>\n",
       "      <th>result</th>\n",
       "      <th>Hp</th>\n",
       "      <th>Dp</th>\n",
       "      <th>Ap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2436548223350254</td>\n",
       "      <td>0.24871794871794872</td>\n",
       "      <td>0.5714285714285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        home       away odd of choice choice result                  Hp  \\\n",
       "0  Liverpool  Tottenham          2.82      2    0.0  0.2436548223350254   \n",
       "\n",
       "                    Dp                  Ap  \n",
       "0  0.24871794871794872  0.5714285714285714  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipt.to_csv(\"2016-4-3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
