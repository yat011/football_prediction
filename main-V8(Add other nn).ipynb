{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    from IPython.core.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "import sys\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotCurve(train_mean, train_std,test_mean,test_std,sizes):\n",
    "    plt.plot(sizes, train_mean, \n",
    "            color='blue', marker='o', \n",
    "            markersize=5, \n",
    "            label='training accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                  train_mean + train_std,\n",
    "                   train_mean - train_std, alpha=0.15, color='blue')\n",
    "\n",
    "    plt.plot(sizes, test_mean, \n",
    "              color='green', linestyle='--', \n",
    "              marker='s', markersize=5, \n",
    "             label='validation accuracy')\n",
    "    plt.fill_between(sizes, \n",
    "                      test_mean + test_std,\n",
    "                     test_mean - test_std, \n",
    "                    alpha=0.15, color='green')\n",
    "    plt.xlabel('x_range')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "def likehoodScore(proba,y):\n",
    "    return np.sum(proba * y)/proba.shape[0]\n",
    "\n",
    "def firstNScore(n, pred, y):\n",
    "    backup = np.array(pred, copy =True)\n",
    "    for r in range(pred.shape[0]):\n",
    "        row = backup[r]\n",
    "        s = np.sort(row)\n",
    "        for c in range(pred.shape[1]):\n",
    "            temp = backup[r][c]\n",
    "            backup[r][c] = False\n",
    "            for j in range(1,n+1):\n",
    "                if temp == s[-j]:\n",
    "                    backup[r][c] = True\n",
    "                    break\n",
    "    res = np.sum(np.logical_and(backup,y))/pred.shape[0]\n",
    "    return res               \n",
    "\n",
    "def oneHotDecode(self, X_sample):\n",
    "    result=None\n",
    "    fiPos = 0\n",
    "    colIndex = 0\n",
    "    while colIndex < X_sample.shape[1]:\n",
    "        if fiPos < len(self.ohe.n_values_) and colIndex == self.ohe.feature_indices_[fiPos]:                \n",
    "            start = self.ohe.feature_indices_[fiPos]\n",
    "            end_ = start+ self.ohe.n_values_[fiPos]\n",
    "            #print(\"start{} end{}\".format(start,end_))\n",
    "            classes = np.argmax(X_sample[:,start:end_],axis=1).reshape(X_sample.shape[0],1)\n",
    "            if result is None:\n",
    "                result = classes\n",
    "            else:\n",
    "                result=np.hstack([result,classes])\n",
    "            colIndex = end_\n",
    "            fiPos = fiPos +1\n",
    "        else:\n",
    "            if result is None:\n",
    "                result = X_sample[:,colIndex:colIndex+1]\n",
    "            else:\n",
    "                result=np.hstack([result, X_sample[:,colIndex:colIndex+1]])\n",
    "            colIndex = colIndex +1\n",
    "        \n",
    "    return result \n",
    "def convertToDate(dayStamps):\n",
    "    res = [] \n",
    "    for v in dayStamps:\n",
    "        res.append(datetime.datetime.fromtimestamp(v*24*60*60))\n",
    "    return res\n",
    "\n",
    "#TODO confusion matrix ,e.g. 100 - 90 % precision\n",
    "def _precisionClassify(df,proba, wins, c =0 ):\n",
    "    for indx, v in enumerate(proba):\n",
    "        row = 0\n",
    "        col = 0\n",
    "        if wins[indx] == c:\n",
    "            col = 0\n",
    "        else:\n",
    "            col =1\n",
    "        if v <0.2:\n",
    "            row =6 \n",
    "        elif v < 0.3 and  v >=0.2:\n",
    "            row =5 \n",
    "        elif v < 0.4 and v >= 0.3:\n",
    "            row = 4 \n",
    "        elif v < 0.5 and v >= 0.4:\n",
    "            row = 3 \n",
    "        elif v < 0.6 and v >= 0.5:\n",
    "            row = 2 \n",
    "        elif v < 0.8 and v >= 0.6:\n",
    "            row = 1\n",
    "        df.iloc[row,col] = df.iloc[row,col]+1 \n",
    "    df[df.columns[2]] = df[df.columns[0]] /(df[df.columns[1]] + df[df.columns[0]])\n",
    "    return df\n",
    "    \n",
    "\n",
    "def precisionMatrix(proba, y):\n",
    "    rowHeader = ['>80','60-80','50-60','40-50','30-40','20-30','<20']\n",
    "    df = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['h_Correct', 'h_Wrong','h_Precent'])\n",
    "    hproba = proba[:,0]\n",
    "    wins = np.argmax(y,axis=1)\n",
    "    df = _precisionClassify(df,hproba,wins)\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['d_Correct', 'd_Wrong','d_Precent'])\n",
    "    dproba = proba[:,1]\n",
    "    df = df.join(_precisionClassify(temp,dproba,wins,c=1))\n",
    "    temp = pd.DataFrame(np.zeros(shape=(7,3)),index=rowHeader, columns=['a_Correct', 'a_Wrong','a_Precent'])\n",
    "    aproba = proba[:,2]\n",
    "    df = df.join(_precisionClassify(temp,aproba,wins,c=2))\n",
    "    return df\n",
    "       \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "def futureTest(mlp, X,y,numOfWeek = 10,verbose=False):\n",
    "    decoded = oneHotDecode(c, X)\n",
    "    dates = convertToDate(X[:,c.dateColumn])\n",
    "    dates = [d - timedelta(days=2) for d in dates]    \n",
    "    weeks  = [ v.isocalendar()[1] for v in dates]\n",
    "    thisWeek = weeks[-1]\n",
    "    start = -1\n",
    "    last = X.shape[0]\n",
    "    index = -1\n",
    "    w = 0\n",
    "    sum_proba =None \n",
    "    sum_y =None\n",
    "    sum_train_proba=None\n",
    "    sum_train_y=None\n",
    "    while w < numOfWeek:\n",
    "        if thisWeek != weeks[index]:\n",
    "            start = X.shape[0] +index+1\n",
    "            X_train = X[0:start, :]\n",
    "            X_test = X[start:last,:]\n",
    "            y_train = y[0:start,:]\n",
    "            y_test = y[start:last,:]\n",
    "            mlp.fit(X_train,y_train)\n",
    "            decoded = oneHotDecode(c,X_test)\n",
    "            home = np.array([c.inverseTeamMapping(decoded[:,0])]).reshape(X_test.shape[0],1)\n",
    "            away = np.array([c.inverseTeamMapping(decoded[:,1])]).reshape(X_test.shape[0],1)\n",
    "            stack = np.hstack([home,away])\n",
    "            proba = mlp.predict_proba(X_test)\n",
    "            train_proba =mlp.predict_proba(X_train)\n",
    "            errorIndx = np.argmax(proba,axis=1) != np.argmax(y_test,axis=1)\n",
    "            if sum_proba is None:\n",
    "                sum_proba = proba\n",
    "                sum_y = y_test\n",
    "                sum_train_proba = train_proba\n",
    "                sum_train_y = y_train\n",
    "            else:\n",
    "                sum_proba = np.vstack([sum_proba,proba])\n",
    "                sum_y = np.vstack([sum_y,y_test])\n",
    "                sum_train_proba = np.vstack([sum_train_proba, train_proba])\n",
    "                sum_train_y= np.vstack([sum_train_y, y_train])\n",
    "            if verbose == True:\n",
    "                print(\"week{}\".format(w))\n",
    "                print(\"numOftest {} , score {}\".format(X_test.shape[0],mlp.score(X_test,y_test)))\n",
    "                print(np.hstack([stack[errorIndx],proba[errorIndx],y_test[errorIndx]]))\n",
    "                print(\"first2 : {}\",firstNScore(2,proba,y_test))\n",
    "            last = start\n",
    "            thisWeek = weeks[index]\n",
    "            w = w+1\n",
    "        index = index -1\n",
    "        \n",
    "    print(\"summary\")\n",
    "    print(\"score:\")\n",
    "    score = firstNScore(1,sum_proba,sum_y)\n",
    "    print(score)\n",
    "    print(\"2like\")\n",
    "    like2 = firstNScore(2,sum_proba,sum_y)\n",
    "    print(precisionMatrix(sum_proba,sum_y))\n",
    "    y_true= np.argmax(sum_y,axis=1)\n",
    "    y_pred = np.argmax(sum_proba,axis=1)\n",
    "    print(\"sum precision:{}\".format(precision_score(y_true,y_pred,average=None)))\n",
    "    return firstNScore(1,sum_train_proba,sum_train_y), score, like2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/scikit-learn/sklearn/cross_validation.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "class FootballDataHelper:\n",
    "    def __init__ (self, recentNum=5):\n",
    "        self.win_mapping = {'H':0, 'D':1,'A':2}\n",
    "        self.recentNum = recentNum\n",
    "        self.df = None\n",
    "        self.teamsData={}\n",
    "        self.session = 0\n",
    "        #self.hiddensCount = 2\n",
    "       \n",
    "        \n",
    "    def readFootBallData(self,filename): \n",
    "        df = pd.read_csv(filename)\n",
    "        #df = df.drop(df.columns[range(23,df.shape[1])], axis=1)\n",
    "        #df = df.drop(\"Div\",axis=1)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "        df['session'] = pd.Series(np.ones(shape=(df.shape[0],))*self.session, index=df.index)\n",
    "        self.session = self.session +1\n",
    "        #df= df.drop('Referee', 1)\n",
    "        #self.team = df['HomeTeam'].drop_duplicates()\n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = pd.concat([self.df,df])\n",
    "            \n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        teams = self.df['HomeTeam'].drop_duplicates()\n",
    "        teamMap = {}\n",
    "        for index , v in enumerate(teams):\n",
    "            teamMap[v] = index\n",
    "        self.teamsMap = teamMap\n",
    "        referees = self.df['Referee'].drop_duplicates()\n",
    "        refereesMap = {}\n",
    "        for index , v in enumerate(referees):\n",
    "            refereesMap[v] = index+1\n",
    "\n",
    "        self.refereesMap = refereesMap\n",
    " \n",
    "    def getTeam(self,dataFrame, teamName):       \n",
    "        return dataFrame[(dataFrame[\"HomeTeam\"] == teamName) | (dataFrame[\"AwayTeam\"] == teamName)]\n",
    " \n",
    "        \n",
    "    def previousRecords(self,team, date , recentNum):\n",
    "        prev = team[( team[\"Date\"] < date)]\n",
    "        \n",
    "        if prev.shape[0] < recentNum :\n",
    "            #print(\"less than min Num\")\n",
    "            return None\n",
    "        else:\n",
    "            return prev.iloc[-recentNum:]\n",
    "    def readPredict (self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "  \n",
    "    def inverseTeamMapping (self, col):\n",
    "        inverseMap ={}\n",
    "        for name in self.teamsMap.keys():        \n",
    "            inverseMap[self.teamsMap[name]] = name\n",
    "        res =[]\n",
    "        for idex, v in enumerate(col):\n",
    "            res.append(inverseMap[v])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def readTeamMatch(self, teamName):\n",
    "        df = pd.read_csv('teams/'+teamName+'.csv')\n",
    "        df['1'] = pd.to_datetime(df['1'],yearfirst=True)\n",
    "        #df['1']= (pd.to_numeric(df['1'])/1e9/24/60/60)\n",
    "        self.teamsData[teamName]=df.sort(['1'],ascending=[False])\n",
    "        self.teamsById[self.teamsMap[teamName]]=self.teamsData[teamName]\n",
    "    \n",
    "    def commonMapping(self, X):\n",
    "        X['HomeTeam'] = X['HomeTeam'].map(self.teamsMap)\n",
    "        X['AwayTeam'] = X['AwayTeam'].map(self.teamsMap)\n",
    "        X['Referee']=X['Referee'].map(self.refereesMap).fillna(0)\n",
    "        X['HTR'] = X['HTR'].map(self.win_mapping)\n",
    "        X['FTR'] = X['FTR'].map(self.win_mapping)\n",
    "        return X\n",
    "    def initData(self, X, target,encode):\n",
    "        X  = X.sort_values(by=\"Date\")\n",
    "        isInput = False\n",
    "        if target is None:\n",
    "            target =X      \n",
    "        else:\n",
    "            if self.ohe is None:\n",
    "                raise Exception(\"Not yet get train data\")\n",
    "            isInput = True\n",
    "            if encode == True:\n",
    "                target = self.commonMapping(target)\n",
    "        y=None\n",
    "        if encode == True:    \n",
    "            X =self.commonMapping(X)\n",
    "            y = []\n",
    "            for v in target['FTR']:\n",
    "                y.append(range(3)==v)\n",
    "        else:\n",
    "            y = target['FTR'].values\n",
    "        target_date = (pd.to_numeric(target['Date'])/1e9/24/60/60).values\n",
    "        return isInput, X,y, target, target_date\n",
    "   \n",
    "    def aggregate(self,recents,nonExpand,isInput,encode):\n",
    "        res =None\n",
    "        if encode == True:\n",
    "            if isInput==False:\n",
    "                self.ohe = OneHotEncoder(categorical_features='all')\n",
    "                self.ohe.fit(recents)\n",
    "            res = self.ohe.transform(recents).toarray()\n",
    "        else:\n",
    "            res = np.array(recents)\n",
    "        self.dateColumn = res.shape[1]\n",
    "        res = np.hstack([res,nonExpand])\n",
    "        return res\n",
    "    def _extractFromPrevious(self, prev,teamName):\n",
    "        homes =  prev['HomeTeam'].values\n",
    "        ftr = prev['FTR'].values\n",
    "        homeGoals = prev['FTHG'].values\n",
    "        awayGoals = prev['FTAG'].values\n",
    "        homeYellow = prev['HY'].values\n",
    "        awayYellow = prev['AY'].values\n",
    "        homeRed = prev['HR'].values\n",
    "        awayRed =prev['AR'].values\n",
    "        rankdiffs = (prev['HAccP'].values+1) / (prev['AAccP'].values +1)\n",
    "        scored = 0\n",
    "        conceded = 0\n",
    "        wins = 0\n",
    "        draw =0\n",
    "        lose = 0\n",
    "        yellowCards = 0\n",
    "        redCards = 0\n",
    "        moral  = 0\n",
    "        for i in range(ftr.shape[0]):\n",
    "            if homes[i] == teamName:\n",
    "                if ftr[i] == 'H' or ftr[i] == 0:\n",
    "                    wins = wins+1\n",
    "                    moral = moral + 3 * 1/rankdiffs[i]\n",
    "                elif ftr[i] == 'A' or ftr[i] == 2:\n",
    "                    lose = lose+1\n",
    "                    moral = moral - 3 * rankdiffs[i]\n",
    "                else:\n",
    "                    draw = draw +1\n",
    "                    moral = moral + 1 * 1/rankdiffs[i]\n",
    "                scored = scored + homeGoals[i]\n",
    "                conceded = conceded + awayGoals[i]\n",
    "                yellowCards = yellowCards + homeYellow[i]\n",
    "                redCards = redCards + homeRed[i]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                if ftr[i] == 'A' or ftr[i] == 2:\n",
    "                    wins = wins+1\n",
    "                    moral = moral + 3 * rankdiffs[i]\n",
    "                elif ftr[i] == 'H' or ftr[i] == 0:\n",
    "                    lose = lose+1\n",
    "                    moral = moral - 3 * 1/rankdiffs[i]\n",
    "                else:\n",
    "                    draw = draw +1\n",
    "                    moral = moral + 1 * rankdiffs[i]\n",
    "                scored = scored + awayGoals[i]\n",
    "                conceded = conceded + homeGoals[i]\n",
    "                yellowCards = yellowCards + awayYellow[i]\n",
    "                redCards = redCards + awayRed[i]\n",
    "                              \n",
    "        \n",
    "        temp = np.array([wins,draw,lose,scored-conceded])\n",
    "        return temp,moral\n",
    "    def _getH7RecentMatches(self,x, X,teamName,recentNum,isHome):\n",
    "        team = self.getTeam(X,teamName)\n",
    "        homeOrAway = None\n",
    "        if isHome == True:\n",
    "            homeOrAway = X[X['HomeTeam']==teamName]\n",
    "        else:\n",
    "            homeOrAway = X[X['AwayTeam']==teamName]\n",
    "        prev = self.previousRecords(team,x['Date'],recentNum)\n",
    "        prevHomeOrAway = self.previousRecords(homeOrAway,x['Date'],recentNum)\n",
    "        if prev is None or prevHomeOrAway is None:\n",
    "            return None,None\n",
    "        res1,moral= self._extractFromPrevious(prev,teamName)\n",
    "        #res2= self._extractFromPrevious(prevHomeOrAway,teamName)\n",
    "        \n",
    "        return res1,moral\n",
    "    def getH7(self,removeInsufficient=False, target=None,encode = True):\n",
    "        #Simple recent win,draw, lose \n",
    "        df = self.df\n",
    "        if removeInsufficient == True:\n",
    "            df= df[df['Sufficient'] == 1]\n",
    "        \n",
    "        \n",
    "        isInput, X, y,target, target_date = self.initData(df,target,encode)\n",
    "        resy=[]\n",
    "        resx=[]\n",
    "        print(\"start format\")\n",
    "        recents = X[['HomeTeam','AwayTeam','Referee']].values\n",
    "        haccp = X['HAccP'].values.reshape(X.shape[0],1)\n",
    "        aaccp = X['AAccP'].values.reshape(X.shape[0],1)\n",
    "        homeRecent = np.hstack([X[['HWin','HDraw','HLose']].values,\n",
    "                                (X['HScore'].values - X['HConcede'].values).reshape(X.shape[0],1)])\n",
    "        awayRecent = np.hstack([X[['AWin','ADraw','ALose']].values,\n",
    "                                (X['AScore'].values - X['AConcede'].values).reshape(X.shape[0],1)])\n",
    "        homeMoral = X['HMoral'].values.reshape(X.shape[0],1)\n",
    "        awayMoral = X['AMoral'].values.reshape(X.shape[0],1)\n",
    "        target_date = target_date.reshape(X.shape[0],1)\n",
    "        nonExpand =np.hstack([target_date,X[['HRestDay','ARestDay','HAccS','AAccS','HAccST','AAccST']].values,haccp-aaccp,(haccp+1)/(aaccp+1),\n",
    "                                homeRecent,awayRecent, homeMoral - awayMoral + haccp - aaccp])\n",
    "        res = self.aggregate(recents,nonExpand,isInput,encode)\n",
    "        print(\"finish\")\n",
    "        sys.stdout.flush()\n",
    "        return res, np.array(y)\n",
    "    def _getRank(self,x, X,teamName,recentNum):\n",
    "        team = self.getTeam(X,teamName)\n",
    "        prev = team[team['Date'] < x['Date']].values      \n",
    "        for i in range(recentNum):\n",
    "            pass\n",
    "    def initRanking(self, n = 20):\n",
    "        defaultPt = 1\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HPoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"APoints\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AAccP\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        hpoints= df[\"HPoints\"].values\n",
    "        apoints=df[\"APoints\"].values\n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hpoints[i] = 3\n",
    "                apoints[i] = 0\n",
    "            elif ftr[i] == 'D':\n",
    "                hpoints[i] = 1\n",
    "                apoints[i] = 1\n",
    "            else :\n",
    "                hpoints[i] = 0\n",
    "                apoints[i] = 3\n",
    "        df[\"HPoints\"]=hpoints\n",
    "        df[\"APoints\"]=apoints\n",
    "        for teamName in self.teamsMap.keys():\n",
    "            print(teamName)\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hpoints = team['HPoints'].values\n",
    "            apoints = team['APoints'].values\n",
    "            psum = 0\n",
    "            haccp = team['HAccP'].values\n",
    "            aaccp = team['AAccP'].values\n",
    "        \n",
    "            for  i in range(0,n):\n",
    "                if i < hpoints.shape[0]:\n",
    "                    psum = psum + (hpoints[i] if hometeam[i] == teamName else apoints[i] ) \n",
    "                else:\n",
    "                    psum = psum + defaultPt        \n",
    "                    \n",
    "        \n",
    "            for j in range(team.shape[0]):\n",
    "\n",
    "                if j+n < hpoints.shape[0]:                     \n",
    "                    psum = psum + (hpoints[j+n] if hometeam[j+n]==teamName else apoints[j+n])\n",
    "                else:\n",
    "                    psum = psum + defaultPt \n",
    "                \n",
    "                psum = psum - (hpoints[j] if hometeam[j]==teamName else apoints[j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    haccp[j]=psum\n",
    "                else:\n",
    "                    aaccp[j]=psum\n",
    "            team['HAccp']=haccp\n",
    "            team['AAccP']=aaccp\n",
    "            #print(team[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "            df.update(team)\n",
    "            \n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initRecentData(self, n =5):\n",
    "        df = self.df.sort(columns=[\"Date\"],ascending=[False])\n",
    "        df[\"HWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AWin\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HDraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ADraw\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HLose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ALose\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AScore\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AConcede\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AMoral\"] = pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HRestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"ARestDay\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HAccS\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AAccS\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"HAccST\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        df[\"AAccST\"]= pd.Series(np.zeros(shape=(df.shape[0],)))\n",
    "        \n",
    "        df[\"Sufficient\"] = pd.Series(np.ones(shape=(df.shape[0],)))\n",
    "        \n",
    "        hscore = df['FTHG'].values\n",
    "        ascore = df['FTAG'].values\n",
    "        hconcede = df ['FTAG'].values\n",
    "        aconcede = df['FTHG'].values\n",
    "        hwin = df['HWin'].values\n",
    "        awin = df['AWin'].values\n",
    "        hlose = df['HLose'].values\n",
    "        alose = df['ALose'].values\n",
    "        hdraw = df['HDraw'].values\n",
    "        adraw = df['ADraw'].values\n",
    "        hmoral = df['HMoral'].values\n",
    "        amoral = df['AMoral'].values\n",
    "        \n",
    "        rankRatio = (df['HAccP'].values+1) / (df['AAccP'].values +1)\n",
    "        \n",
    "        \n",
    "        ftr = df[\"FTR\"].values\n",
    "        for i in range(df.shape[0]):\n",
    "            sys.stdout.write(\"\\r progress {}\".format(i))\n",
    "            sys.stdout.flush()\n",
    "            if ftr[i] == 'H':\n",
    "                hwin[i] = 1\n",
    "                hmoral[i] = 3 * 1/rankRatio[i]\n",
    "                alose[i]= 1\n",
    "                amoral[i] = -3 * 1/rankRatio[i]\n",
    "            elif ftr[i] == 'D':\n",
    "                hdraw[i] = 1\n",
    "                hmoral[i] = 1 * 1/rankRatio[i]\n",
    "                adraw[i] = 1\n",
    "                amoral[i] = 1 * rankRatio[i]\n",
    "            else :\n",
    "                hlose[i] = 1\n",
    "                hmoral[i] = -3*rankRatio[i]\n",
    "                awin [i] = 1\n",
    "                amoral[i] = 3*rankRatio[i]\n",
    "        \n",
    "        \n",
    "        df[\"HWin\"]=hwin\n",
    "        df[\"AWin\"]=awin\n",
    "        df[\"HDraw\"]=hdraw\n",
    "        df[\"ADraw\"]=adraw\n",
    "        df[\"HLose\"]=hlose\n",
    "        df[\"ALose\"]=alose\n",
    "        df[\"HScore\"]=hscore\n",
    "        df[\"AScore\"]=ascore\n",
    "        df[\"HConcede\"]=hconcede\n",
    "        df[\"AConcede\"]=aconcede\n",
    "        df[\"HMoral\"] = hmoral\n",
    "        df[\"AMoral\"] = amoral\n",
    "        \n",
    "        \n",
    "        \n",
    "        for teamName in self.teamsMap.keys():\n",
    "            print(teamName)\n",
    "            team  = df[(df['HomeTeam']==teamName) | (df['AwayTeam'] == teamName)] \n",
    "            hometeam = team['HomeTeam'].values\n",
    "            hwin = team[\"HWin\"].values\n",
    "            awin = team[\"AWin\"].values\n",
    "            hlose= team[\"HLose\"].values\n",
    "            alose = team[\"ALose\"].values\n",
    "            hdraw = team[\"HDraw\"].values\n",
    "            adraw = team[\"ADraw\"].values\n",
    "            hscore = team[\"HScore\"].values\n",
    "            ascore = team[\"AScore\"].values\n",
    "            hconcede = team[\"HConcede\"].values\n",
    "            aconcede = team[\"AConcede\"].values\n",
    "            hmoral = team[\"HMoral\"].values\n",
    "            amoral = team[\"AMoral\"].values\n",
    "            hrestday = team[\"HRestDay\"].values\n",
    "            arestday = team[\"ARestDay\"].values\n",
    "            \n",
    "            hs= team[\"HS\"].values\n",
    "            as_ = team[\"AS\"].values\n",
    "            hst = team[\"HST\"].values\n",
    "            ast = team[\"AST\"].values\n",
    "            haccs = team[\"HAccS\"].values\n",
    "            aaccs = team[\"AAccS\"].values\n",
    "            haccst =team[\"HAccST\"].values\n",
    "            aaccst = team[\"AAccST\"].values\n",
    "            \n",
    "            matchDate =team['Date'].values\n",
    "            sufficient = team['Sufficient'].values\n",
    "            teamMatchesDate = self.teamsData[teamName].sort('1',ascending=False)['1'].values\n",
    "            \n",
    "            restday = 0\n",
    "            winsum =0 \n",
    "            losesum=0\n",
    "            drawsum=0\n",
    "            scoresum =0\n",
    "            concedesum=0\n",
    "            accs = 0\n",
    "            accst = 0\n",
    "            moralsum = 0\n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            for  i in range(0,n):\n",
    "                if i < team.shape[0]:\n",
    "                    scoresum = scoresum + (hscore[i] if hometeam[i] == teamName else ascore[i])\n",
    "                    winsum = winsum + (hwin[i] if hometeam[i] == teamName else awin[i])\n",
    "                    losesum= losesum + (hlose[i] if hometeam[i] == teamName else alose[i])\n",
    "                    drawsum= drawsum + (hdraw[i] if hometeam[i] == teamName else adraw[i])\n",
    "                    concedesum = concedesum+ (hconcede[i] if hometeam[i] == teamName else aconcede[i])\n",
    "                    moralsum= moralsum+ (hmoral[i] if hometeam[i] == teamName else amoral[i])\n",
    "                    accs = accs + (hs[i] if hometeam[i] == teamName else as_[i])\n",
    "                    accst = accst + (hst[i] if hometeam[i] == teamName else ast[i])\n",
    "                else:\n",
    "                    # + 0\n",
    "                    pass\n",
    "            dateIndx = 0\n",
    "            for j in range(team.shape[0]):\n",
    "                while True:\n",
    "                    if dateIndx >= teamMatchesDate.shape[0]:\n",
    "                        sufficient[j] = False\n",
    "                        break\n",
    "                    if teamMatchesDate[dateIndx] < matchDate[j] :\n",
    "                        restday = (matchDate[j] - teamMatchesDate[dateIndx])/np.timedelta64(1,'D')\n",
    "                        break\n",
    "                    else:\n",
    "                        dateIndx = dateIndx + 1\n",
    "                \n",
    "                if j+n < team.shape[0]:                     \n",
    "                    scoresum = scoresum + (hscore[j+n] if hometeam[j+n] == teamName else ascore[j+n])\n",
    "                    winsum = winsum + (hwin[j+n] if hometeam[j+n] == teamName else awin[j+n])\n",
    "                    losesum= losesum + (hlose[j+n] if hometeam[j+n] == teamName else alose[j+n])\n",
    "                    drawsum= drawsum + (hdraw[j+n] if hometeam[j+n] == teamName else adraw[j+n])\n",
    "                    concedesum = concedesum+ (hconcede[j+n] if hometeam[j+n] == teamName else aconcede[j+n])\n",
    "                    moralsum= moralsum+ (hmoral[j+n] if hometeam[j+n] == teamName else amoral[j+n])\n",
    "                    accs = accs + (hs[j+n] if hometeam[j+n] == teamName else as_[j+n])\n",
    "                    accst = accst + (hst[j+n] if hometeam[j+n] == teamName else ast[j+n])\n",
    "                else:\n",
    "                    sufficient[j] = False\n",
    "                    \n",
    "                \n",
    "                scoresum = scoresum - (hscore[j] if hometeam[j] == teamName else ascore[j])\n",
    "                winsum = winsum - (hwin[j] if hometeam[j] == teamName else awin[j])\n",
    "                losesum= losesum - (hlose[j] if hometeam[j] == teamName else alose[j])\n",
    "                drawsum= drawsum - (hdraw[j] if hometeam[j] == teamName else adraw[j])\n",
    "                concedesum = concedesum - (hconcede[j] if hometeam[j] == teamName else aconcede[j])\n",
    "                moralsum= moralsum - (hmoral[j] if hometeam[j] == teamName else amoral[j])\n",
    "                accs = accs - (hs[j] if hometeam[j] == teamName else as_[j])\n",
    "                accst = accst - (hst[j] if hometeam[j] == teamName else ast[j])\n",
    "                    \n",
    "                if hometeam[j] == teamName:\n",
    "                    hscore[j] = scoresum\n",
    "                    hwin[j] = winsum\n",
    "                    hlose[j] = losesum\n",
    "                    hdraw[j] = drawsum\n",
    "                    hconcede[j] = concedesum\n",
    "                    hmoral[j] = moralsum\n",
    "                    hrestday[j] = restday\n",
    "                    haccs[j]=accs\n",
    "                    haccst[j]=accst\n",
    "                else:\n",
    "                    ascore[j] = scoresum\n",
    "                    awin[j] = winsum\n",
    "                    alose[j] = losesum\n",
    "                    adraw[j] = drawsum\n",
    "                    aconcede[j] = concedesum\n",
    "                    amoral[j] = moralsum\n",
    "                    arestday[j] = restday\n",
    "                    aaccs[j] = accs\n",
    "                    aaccst[j] = accst\n",
    "            team[\"HWin\"]=hwin\n",
    "            team[\"AWin\"]=awin\n",
    "            team[\"HDraw\"]=hdraw\n",
    "            team[\"ADraw\"]=adraw\n",
    "            team[\"HLose\"]=hlose\n",
    "            team[\"ALose\"]=alose\n",
    "            team[\"HScore\"]=hscore\n",
    "            team[\"AScore\"]=ascore\n",
    "            team[\"HConcede\"]=hconcede\n",
    "            team[\"AConcede\"]=aconcede\n",
    "            team[\"HMoral\"] = hmoral\n",
    "            team[\"AMoral\"] = amoral\n",
    "            team['Sufficient'] = sufficient\n",
    "            team['HAccS'] = haccs\n",
    "            team['AAccS'] = aaccs\n",
    "            team['HAccST'] = haccst\n",
    "            team['AAccST'] = aaccst\n",
    "        \n",
    "            #print(team[['HomeTeam','AwayTeam','HWin']])\n",
    "            df.update(team)\n",
    "            #print(df[['HomeTeam','AwayTeam','HAccP','AAccP']])\n",
    "        self.df =df\n",
    "        return df\n",
    "    def initTeamData(self):\n",
    "        self.teamsData={}\n",
    "        self.teamsById={}\n",
    "        for name in self.teamsMap.keys():\n",
    "            self.readTeamMatch(name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:77: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "c = FootballDataHelper()\n",
    "#c.readFootBallData(\"E0_1112.csv\")\n",
    "c.readFootBallData(\"E0_1213.csv\")\n",
    "c.readFootBallData(\"E0_1314.csv\")\n",
    "c.readFootBallData(\"E0_1415.csv\")\n",
    "c.readFootBallData(\"E0 (1).csv\")\n",
    "c.initTeamData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1432"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:223: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hull\n",
      "West Ham\n",
      "Stoke\n",
      "Watford\n",
      "Fulham\n",
      "Burnley\n",
      "Wigan\n",
      "Newcastle\n",
      "Man City\n",
      "Chelsea\n",
      "Leicester\n",
      "Aston Villa\n",
      "Reading\n",
      "West Brom\n",
      "Southampton\n",
      "Cardiff\n",
      "Swansea\n",
      "Norwich\n",
      "Crystal Palace\n",
      "Bournemouth\n",
      "Sunderland\n",
      "Everton\n",
      "Man United\n",
      "Arsenal\n",
      "QPR\n",
      "Liverpool\n",
      "Tottenham\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df = c.initRanking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " progress 1432"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:284: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:388: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:467: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:468: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:469: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:470: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:471: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:472: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:473: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:474: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:475: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:479: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:480: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:481: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hull\n",
      "West Ham\n",
      "Stoke\n",
      "Watford\n",
      "Fulham\n",
      "Burnley\n",
      "Wigan\n",
      "Newcastle\n",
      "Man City\n",
      "Chelsea\n",
      "Leicester\n",
      "Aston Villa\n",
      "Reading\n",
      "West Brom\n",
      "Southampton\n",
      "Cardiff\n",
      "Swansea\n",
      "Norwich\n",
      "Crystal Palace\n",
      "Bournemouth\n",
      "Sunderland\n",
      "Everton\n",
      "Man United\n",
      "Arsenal\n",
      "QPR\n",
      "Liverpool\n",
      "Tottenham\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df=c.initRecentData(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "def plotErrorDate(X_test, X_err, dateCol = 10):\n",
    "    X_test_date =np.sort(X_test[:,c.dateColumn])\n",
    "    X_date=[]\n",
    "    y_date=[]\n",
    "    for v in X_test_date:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_date) ==0  or X_date[-1] != date:\n",
    "            X_date.append(date)\n",
    "            y_date.append(1)\n",
    "        else:\n",
    "            y_date[-1] = y_date[-1] +1\n",
    "    plt.plot_date(X_date,y_date,xdate=True)\n",
    "    X_err_d = np.sort(X_err[:,c.dateColumn])\n",
    "    X_err_date=[]\n",
    "    y_err_date = []\n",
    "    for v in X_err_d:\n",
    "        date = datetime.datetime.fromtimestamp(v*24*60*60)\n",
    "        if len(X_err_date) ==0  or X_err_date[-1] != date:\n",
    "            X_err_date.append(date)\n",
    "            y_err_date.append(1)\n",
    "        else:\n",
    "            y_err_date[-1] = y_err_date[-1] +1\n",
    "    plt.plot_date(X_err_date,y_err_date,xdate=True,color='red')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "g_hiddenNodes = int(45)\n",
    "g_alpha = 0 \n",
    "clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_hiddenNodes], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "              learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=1000,early_stopping = False,verbose = 3)\n",
    "mlp = Pipeline([ ('scl', StandardScaler()),('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', SoftMaxMLPClassifier(activation='logistic', algorithm='l-bfgs', alpha=0,\n",
       "           batch_size='auto', beta_1=0.9, beta_2=0.999,\n",
       "           early_stopping=False, epsilon=1e-08, hidden_layer_sizes=[45],\n",
       "           learn...e=None, shuffle=True, tol=0.0001,\n",
       "           validation_fraction=0.1, verbose=3, warm_start=False))])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start learning\n",
      "[learning_curve] Training set sizes: [ 101  404  707 1011]\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.433234 -   0.8s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.522255 -   0.7s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.531157 -   0.4s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.516320 -   1.2s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.468843 -   0.2s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.572700 -   0.5s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.587537 -   1.1s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.584570 -   0.7s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.456973 -   0.1s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.554896 -   0.4s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.534125 -   0.7s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.560831 -   0.6s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.409496 -   0.1s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.486647 -   0.4s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.498516 -   0.6s\n",
      "[CV] no parameters to be set .........................................\n",
      "[CV] ................ no parameters to be set, score=0.486647 -   0.7s\n",
      "finishing\n",
      "[ 0.4421365   0.53412463  0.53783383  0.53709199]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    9.3s finished\n"
     ]
    }
   ],
   "source": [
    "print (\"start learning\")\n",
    "sys.stdout.flush()\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=mlp, \n",
    "                       X=X, \n",
    "                      y=y, \n",
    "                      train_sizes=np.linspace(0.1, 1.0, 4), \n",
    "                      cv=4,\n",
    "                     n_jobs=1,verbose=3)\n",
    "print(\"finishing\")   \n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "print(test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learningRes = np.vstack([train_sizes,train_mean,train_std,test_mean,test_std]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   size  train_mean  train_std  test_mean  test_std\n",
      "0   101    0.443069   0.012862   0.442136  0.022793\n",
      "1   404    0.552599   0.011394   0.534125  0.032843\n",
      "2   707    0.566124   0.011376   0.537834  0.031916\n",
      "3  1011    0.560584   0.012732   0.537092  0.038059\n"
     ]
    }
   ],
   "source": [
    "learningDf = pd.DataFrame(learningRes,columns=['size','train_mean','train_std','test_mean','test_std'])\n",
    "print(learningDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEKCAYAAADXdbjqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmYWJMcaYKfv/7GxUMSONyeUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efca85ceef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plotCurve(train_mean,train_std,test_mean,test_std,train_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate(mlp, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    firstNScores = []\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "\n",
    "        mlp.fit(X[train], y[train])\n",
    "        score = mlp.score(X[test], y[test])\n",
    "        firstNScores.append(firstNScore(2, mlp.predict_proba(X[test]), y[test]))\n",
    "        train_scores.append(mlp.score(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, firstNScores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.500\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.503\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.497\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.542\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.467\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.485\n",
      "lamda: 0, train: 0.616782390635193, test: 0.504384875207568\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.518\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.500\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.515\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.518\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.589\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.479\n",
      "lamda: 2, train: 0.6026860271530378, test: 0.5125250053073704\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.518\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.482\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.544\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.518\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.571\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.473\n",
      "lamda: 4, train: 0.596222542111688, test: 0.5110367019553455\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.482\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.527\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.524\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.571\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.455\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.443\n",
      "lamda: 6, train: 0.5900770326157878, test: 0.5087868939164086\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.524\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.488\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.506\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.571\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.461\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.461\n",
      "lamda: 8, train: 0.5873228818255021, test: 0.5110280529761421\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.488\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.544\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.544\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.506\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.583\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.455\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.461\n",
      "lamda: 10, train: 0.5864754240462257, test: 0.5139822292787009\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.488\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.550\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.506\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.583\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.461\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.461\n",
      "lamda: 12, train: 0.5830831644090773, test: 0.5147307322727128\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.488\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.550\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.583\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.449\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.467\n",
      "lamda: 14, train: 0.5802211959520704, test: 0.5147262768977485\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.535\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.494\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.556\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.583\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.455\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.461\n",
      "lamda: 16, train: 0.5788444365534083, test: 0.5169365101034568\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.494\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.556\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.500\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.583\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.461\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.461\n",
      "lamda: 18, train: 0.5767247126191852, test: 0.5154616237417264\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.494\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.556\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.506\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.577\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.467\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.473\n",
      "lamda: 20, train: 0.5754533454138459, test: 0.5177071327237623\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.506\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.556\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.544\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.518\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.571\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.473\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.473\n",
      "lamda: 22, train: 0.573440541872798, test: 0.5214099165425303\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.506\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.562\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.550\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.518\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.571\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.485\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.479\n",
      "lamda: 24, train: 0.5712151557404126, test: 0.5251347154653946\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.518\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.568\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.550\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.518\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.571\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.485\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.485\n",
      "lamda: 26, train: 0.5695185319792972, test: 0.528093451665115\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.518\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.568\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.550\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.560\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.479\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.485\n",
      "lamda: 28, train: 0.5685649618409883, test: 0.5251128058139601\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.524\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.524\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.562\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.550\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.560\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.491\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.491\n",
      "lamda: 30, train: 0.5666579115668877, test: 0.5266186698255818\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.518\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.524\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.556\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.550\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.548\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.491\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.503\n",
      "lamda: 32, train: 0.5666589894537123, test: 0.5251526414874492\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.512\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.535\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.556\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.562\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.518\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.554\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.497\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.503\n",
      "lamda: 34, train: 0.5649635329713703, test: 0.5296038237780318\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.512\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.529\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.568\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.518\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.554\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.509\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.503\n",
      "lamda: 36, train: 0.5632667298161764, test: 0.5288862457075805\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.506\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.535\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.562\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.542\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.503\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.503\n",
      "lamda: 38, train: 0.5611461079986138, test: 0.5251659548860113\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.506\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.541\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.568\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.506\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.548\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.497\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.503\n",
      "lamda: 40, train: 0.5570125927908061, test: 0.5258923909800607\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.500\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.541\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.568\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.506\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.548\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.503\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.497\n",
      "lamda: 42, train: 0.5534111645302069, test: 0.5251570968624136\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.494\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.559\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.562\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.500\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.542\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.509\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.485\n",
      "lamda: 44, train: 0.5507615110977107, test: 0.5236514418951863\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.488\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.571\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.538\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.544\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.506\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.524\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.509\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.479\n",
      "lamda: 46, train: 0.5480065531881668, test: 0.5199312028694836\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.494\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.571\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.533\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.544\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.524\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.509\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.485\n",
      "lamda: 48, train: 0.5445089012051505, test: 0.521419402629776\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.488\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.571\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.521\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.544\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.500\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.515\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.479\n",
      "lamda: 50, train: 0.5402686449978586, test: 0.51622862809511\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.482\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.571\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.527\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.544\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.500\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.521\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.479\n",
      "lamda: 52, train: 0.5375133269299621, test: 0.5169814819418892\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.476\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.571\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.533\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.544\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.506\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.530\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.515\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.479\n",
      "lamda: 54, train: 0.533061744496964, test: 0.5192135202768349\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.471\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.565\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.527\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.533\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.509\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.485\n",
      "lamda: 56, train: 0.531579860472581, test: 0.5140359018922028\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.476\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.571\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.521\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.527\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.518\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.515\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.485\n",
      "lamda: 58, train: 0.528295330634496, test: 0.5155197507997282\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.476\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.571\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.509\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.509\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.518\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.506\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.515\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.479\n",
      "lamda: 60, train: 0.5227824500927318, test: 0.5103289753345975\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.482\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.571\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.503\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.491\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.530\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.506\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.497\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.491\n",
      "lamda: 62, train: 0.5154688101158688, test: 0.5088452818146711\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.471\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.559\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.503\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.491\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.512\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.488\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.497\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.491\n",
      "lamda: 64, train: 0.5080525638836871, test: 0.5014398196297971\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.453\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.535\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.497\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.491\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.494\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.488\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.509\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.491\n",
      "lamda: 66, train: 0.4981949311493621, test: 0.49481797896673463\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.453\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.524\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.491\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.485\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.488\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.476\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.485\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.485\n",
      "lamda: 68, train: 0.49088174026710174, test: 0.48589344296340936\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.465\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.488\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.467\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.473\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.482\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.458\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.491\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.473\n",
      "lamda: 70, train: 0.48028482956088825, test: 0.4747897032001335\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.453\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.482\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.462\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.462\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.470\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.458\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.467\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.491\n",
      "lamda: 72, train: 0.4719118666364049, test: 0.4681282877038425\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.453\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.471\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.456\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.450\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.470\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.446\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.449\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.479\n",
      "lamda: 74, train: 0.46343189453542905, test: 0.45920815434915063\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.459\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.456\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.450\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.464\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.485\n",
      "lamda: 76, train: 0.4544247747612728, test: 0.4547788826404672\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.447\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.450\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.452\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.467\n",
      "lamda: 78, train: 0.44785679129547873, test: 0.44809540024421346\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.455\n",
      "lamda: 80, train: 0.4444667781956737, test: 0.443635359930033\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.449\n",
      "lamda: 82, train: 0.4425599960981941, test: 0.44288685693602103\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.443\n",
      "lamda: 84, train: 0.44224237888210316, test: 0.44213835394200907\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.443\n",
      "lamda: 86, train: 0.44213653637575256, test: 0.44213835394200907\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.443\n",
      "lamda: 88, train: 0.44213653637575256, test: 0.44213835394200907\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.443\n",
      "lamda: 90, train: 0.44213653637575256, test: 0.44213835394200907\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.443\n",
      "lamda: 92, train: 0.44213653637575256, test: 0.44213835394200907\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.443\n",
      "lamda: 94, train: 0.44213653637575256, test: 0.44213835394200907\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.443\n",
      "lamda: 96, train: 0.44213653637575256, test: 0.44213835394200907\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.441\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.444\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.440\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.443\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.443\n",
      "lamda: 98, train: 0.44213653637575256, test: 0.44213835394200907\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc7f6ef5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def lamda_test(mlp, X, y, lamdas):\n",
    "    \n",
    "    train_scores=[]\n",
    "    test_scores=[]\n",
    "    for lamda in lamdas:\n",
    "        clf.set_params(alpha= lamda)\n",
    "        train_s, test_s, firstNScores = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_s, test_s, firstNScores =futureTest(mlp,X,y,numOfWeek=20) \n",
    "        train_scores.append(train_s)\n",
    "        test_scores.append(test_s)\n",
    "        print(\"lamda: {}, train: {}, test: {}\".format(lamda, \n",
    "                    np.mean(train_s), np.mean(test_s)) )\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plotCurve(train_mean,train_std,test_mean,test_std,lamdas)\n",
    "    return np.array(train_scores),np.array(test_scores)\n",
    "\n",
    "l_range = []\n",
    "for i in range(0,50):\n",
    "    l_range.append(2*i)\n",
    "train_scores,test_scores = lamda_test(mlp,X,y,np.array(l_range))\n",
    "#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alpha  train_mean  train_std  test_mean  test_std\n",
      "0      0    1.000000   0.000000   0.401954  0.026798\n",
      "1     10    0.672959   0.008925   0.503574  0.042680\n",
      "2     20    0.574076   0.006799   0.514748  0.038951\n",
      "3     30    0.562528   0.004899   0.524351  0.037521\n",
      "4     40    0.558923   0.004739   0.533285  0.028010\n",
      "5     50    0.556592   0.004154   0.534782  0.021022\n",
      "6     60    0.552034   0.003219   0.530322  0.019796\n",
      "7     70    0.547369   0.004033   0.527390  0.018763\n",
      "8     80    0.544083   0.005118   0.528103  0.025373\n",
      "9     90    0.535076   0.005195   0.523638  0.027085\n"
     ]
    }
   ],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "alphaRes = np.vstack([l_range,train_mean,train_std,test_mean,test_std]).T\n",
    "alphaDf = pd.DataFrame(alphaRes,columns=['alpha','train_mean','train_std','test_mean','test_std'])\n",
    "print(alphaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testNodeSize(start ,end):\n",
    "    node_range = range(start,end,5)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    for node in node_range:   \n",
    "        print(\"start node:{}\".format(node))\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[node], activation='logistic', algorithm='l-bfgs', alpha=0, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores,first2 = crossValidate(mlp,X,y,fold=8)\n",
    "      #  train_scores,test_scores , first2= futureTest(mlp, X,y,numOfWeek = 10)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        print(\"Node {}: train_mean {}  v.s. test_mean {}\".format(node,np.mean(train_scores),np.mean(test_scores)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(node_range))\n",
    "    return node_range, train_means,train_std,test_means,test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start node:1\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.500\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.506\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.503\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.497\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.470\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.560\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.467\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.443\n",
      "Node 1: train_mean 0.6094788099756978  v.s. test_mean 0.4932279873027055\n",
      "start node:6\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.400\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.424\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.497\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.467\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.458\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.500\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.431\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.431\n",
      "Node 6: train_mean 0.7877269876379113  v.s. test_mean 0.4510794044524942\n",
      "start node:11\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.412\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.447\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.479\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.450\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.435\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.446\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.359\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.383\n",
      "Node 11: train_mean 0.9066275732478482  v.s. test_mean 0.42641062040803285\n",
      "start node:16\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.359\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.382\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.420\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.402\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.423\n",
      "Fold: 6, Class dist.: [522 289 369], Acc: 0.482\n",
      "Fold: 7, Class dist.: [522 289 370], Acc: 0.413\n",
      "Fold: 8, Class dist.: [522 289 370], Acc: 0.359\n",
      "Node 16: train_mean 0.9684186117852025  v.s. test_mean 0.4051098340338864\n",
      "start node:21\n",
      "Fold: 1, Class dist.: [521 288 369], Acc: 0.412\n",
      "Fold: 2, Class dist.: [521 288 369], Acc: 0.435\n",
      "Fold: 3, Class dist.: [521 289 369], Acc: 0.432\n",
      "Fold: 4, Class dist.: [521 289 369], Acc: 0.379\n",
      "Fold: 5, Class dist.: [522 289 369], Acc: 0.405\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-500b34867c54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnode_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_means\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_std\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_means\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_std\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtestNodeSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-ab398c93aeac>\u001b[0m in \u001b[0;36mtestNodeSize\u001b[1;34m(start, end)\u001b[0m\n\u001b[0;32m     16\u001b[0m                   learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n\u001b[0;32m     17\u001b[0m         \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfirst2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossValidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m       \u001b[1;31m#  train_scores,test_scores , first2= futureTest(mlp, X,y,numOfWeek = 10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtrain_means\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-59d8f3983f29>\u001b[0m in \u001b[0;36mcrossValidate\u001b[1;34m(mlp, X, y, fold)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mfirstNScores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirstNScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/scikit-learn/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \"\"\"\n\u001b[0;32m    165\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/scikit-learn/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/scikit-learn/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m             self._fit_lbfgs(X, y, activations, deltas, coef_grads,\n\u001b[1;32m--> 380\u001b[1;33m                             intercept_grads, layer_units)\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/scikit-learn/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[0miprint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[0mpgtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m             args=(X, y, activations, deltas, coef_grads, intercept_grads))\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimal_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[1;32m--> 193\u001b[1;33m                            **opts)\n\u001b[0m\u001b[0;32m    194\u001b[0m     d = {'grad': res['jac'],\n\u001b[0;32m    195\u001b[0m          \u001b[1;34m'task'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[0;32m    328\u001b[0m                 \u001b[1;31m# minimization routine wants f and g at the current x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                 \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/anaconda3/lib/python3.5/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/anaconda3/lib/python3.5/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/scikit-learn/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_loss_grad_lbfgs\u001b[1;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_coef_inter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         loss, coef_grads, intercept_grads = self._backprop(\n\u001b[1;32m--> 177\u001b[1;33m             X, y, activations, deltas, coef_grads, intercept_grads)\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/scikit-learn/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_backprop\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;31m# Compute gradient for the last layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         coef_grads, intercept_grads = self._compute_loss_grad(\n\u001b[1;32m--> 243\u001b[1;33m             last, n_samples, activations, deltas, coef_grads, intercept_grads)\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;31m# Iterate over the hidden layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/scikit-learn/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_compute_loss_grad\u001b[1;34m(self, layer, n_samples, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \"\"\"\n\u001b[0;32m    123\u001b[0m         coef_grads[layer] = safe_sparse_dot(activations[layer].T,\n\u001b[1;32m--> 124\u001b[1;33m                                             deltas[layer])\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[0mcoef_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mcoef_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/y/scikit-learn/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "node_range, train_means,train_std,test_means,test_std=testNodeSize(1,X.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nodeNum  train_mean  train_std  test_mean  test_std\n",
      "0         4    0.738541   0.018985   0.453907  0.033229\n",
      "1         9    0.874104   0.011133   0.430907  0.024683\n",
      "2        14    0.944788   0.006366   0.433094  0.044951\n",
      "3        19    0.983681   0.005758   0.437647  0.033397\n",
      "4        24    0.997878   0.003260   0.426508  0.035193\n",
      "5        29    0.999576   0.000600   0.432443  0.023766\n",
      "6        34    1.000000   0.000000   0.428568  0.049071\n",
      "7        39    1.000000   0.000000   0.432368  0.043109\n",
      "8        44    1.000000   0.000000   0.433975  0.019339\n",
      "9        49    0.999894   0.000280   0.418455  0.034480\n",
      "10       54    1.000000   0.000000   0.439052  0.041520\n",
      "11       59    1.000000   0.000000   0.430911  0.029448\n",
      "12       64    1.000000   0.000000   0.431761  0.019038\n",
      "13       69    1.000000   0.000000   0.422863  0.033736\n",
      "14       74    1.000000   0.000000   0.433068  0.037790\n",
      "15       79    1.000000   0.000000   0.434675  0.021340\n",
      "16       84    1.000000   0.000000   0.433121  0.027380\n",
      "17       89    1.000000   0.000000   0.422004  0.033515\n",
      "18       94    1.000000   0.000000   0.435314  0.039508\n"
     ]
    }
   ],
   "source": [
    "nodeRes = np.vstack([node_range,train_means,train_std,test_means,test_std]).T\n",
    "nodeDf = pd.DataFrame(nodeRes,columns=['nodeNum','train_mean','train_std','test_mean','test_std'])\n",
    "print(nodeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMaxMLPClassifier(activation='logistic', algorithm='l-bfgs', alpha=35,\n",
      "           batch_size='auto', beta_1=0.9, beta_2=0.999,\n",
      "           early_stopping=True, epsilon=1e-08, hidden_layer_sizes=[40],\n",
      "           learning_rate='adaptive', learning_rate_init=0.01,\n",
      "           max_iter=1000, momentum=0.9, nesterovs_momentum=True,\n",
      "           power_t=0.5, random_state=None, shuffle=True, tol=0.0001,\n",
      "           validation_fraction=0.1, verbose=3, warm_start=False)\n",
      "week0\n",
      "numOftest 5 , score 0.6\n",
      "[['Norwich' 'Man City' '0.2414653770354599' '0.24387811092735376'\n",
      "  '0.5146565120371863' 'False' 'True' 'False']\n",
      " ['Stoke' 'Southampton' '0.6596315044458729' '0.16004793758742336'\n",
      "  '0.18032055796670357' 'False' 'False' 'True']]\n",
      "first2 : {} 1.0\n",
      "week1\n",
      "numOftest 10 , score 0.4\n",
      "[['Everton' 'West Ham' '0.5270087906910846' '0.2187384515293811'\n",
      "  '0.2542527577795343' 'False' 'False' 'True']\n",
      " ['Newcastle' 'Bournemouth' '0.29281329427594227' '0.4136981061070198'\n",
      "  '0.2934885996170378' 'False' 'False' 'True']\n",
      " ['Tottenham' 'Arsenal' '0.6057263419652076' '0.17348095104132324'\n",
      "  '0.22079270699346917' 'False' 'True' 'False']\n",
      " ['Chelsea' 'Stoke' '0.7301653043818526' '0.1530169880526253'\n",
      "  '0.11681770756552223' 'False' 'True' 'False']\n",
      " ['Southampton' 'Sunderland' '0.46823009523351866' '0.24333900948470052'\n",
      "  '0.2884308952817806' 'False' 'True' 'False']\n",
      " ['West Brom' 'Man United' '0.2096269427479414' '0.2754017359113974'\n",
      "  '0.5149713213406611' 'True' 'False' 'False']]\n",
      "first2 : {} 0.7\n",
      "week2\n",
      "numOftest 18 , score 0.5555555555555556\n",
      "[['West Brom' 'Crystal Palace' '0.3314799241560408' '0.22917854498371698'\n",
      "  '0.43934153086024225' 'True' 'False' 'False']\n",
      " ['Watford' 'Bournemouth' '0.290871155280242' '0.19240115189361537'\n",
      "  '0.5167276928261427' 'False' 'True' 'False']\n",
      " ['Southampton' 'Chelsea' '0.40833611703098177' '0.2018503994183508'\n",
      "  '0.38981348355066736' 'False' 'False' 'True']\n",
      " ['Bournemouth' 'Southampton' '0.31464114303164803' '0.28434874782251396'\n",
      "  '0.40101010914583807' 'True' 'False' 'False']\n",
      " ['Leicester' 'West Brom' '0.7390176631834846' '0.1367188716157603'\n",
      "  '0.1242634652007551' 'False' 'True' 'False']\n",
      " ['Sunderland' 'Crystal Palace' '0.29012521539448793' '0.2210955929814466'\n",
      "  '0.4887791916240654' 'False' 'True' 'False']\n",
      " ['West Ham' 'Tottenham' '0.17824562653545806' '0.20926727934048195'\n",
      "  '0.61248709412406' 'True' 'False' 'False']\n",
      " ['Arsenal' 'Swansea' '0.7760376610652654' '0.13024582372530927'\n",
      "  '0.09371651520942527' 'False' 'False' 'True']]\n",
      "first2 : {} 0.777777777778\n",
      "week3\n",
      "numOftest 10 , score 0.7\n",
      "[['Sunderland' 'Man United' '0.13344625314166722' '0.23566319957915693'\n",
      "  '0.6308905472791757' 'True' 'False' 'False']\n",
      " ['Norwich' 'West Ham' '0.2603741701589243' '0.3121508644466779'\n",
      "  '0.4274749653943979' 'False' 'True' 'False']\n",
      " ['Everton' 'West Brom' '0.7258434332842729' '0.16354340167398215'\n",
      "  '0.11061316504174495' 'False' 'False' 'True']]\n",
      "first2 : {} 0.8\n",
      "week4\n",
      "numOftest 10 , score 0.5\n",
      "[['Swansea' 'Crystal Palace' '0.42425860826547496' '0.264387026959285'\n",
      "  '0.31135436477524003' 'False' 'True' 'False']\n",
      " ['Stoke' 'Everton' '0.4736975369032062' '0.25868219447445917'\n",
      "  '0.2676202686223347' 'False' 'False' 'True']\n",
      " ['Liverpool' 'Sunderland' '0.6949790755370437' '0.1691410600877744'\n",
      "  '0.1358798643751818' 'False' 'True' 'False']\n",
      " ['Man City' 'Leicester' '0.5564581965925973' '0.15912313130587152'\n",
      "  '0.28441867210153127' 'False' 'False' 'True']\n",
      " ['Chelsea' 'Man United' '0.5210481188505509' '0.2373487684482927'\n",
      "  '0.24160311270115642' 'False' 'True' 'False']]\n",
      "first2 : {} 0.8\n",
      "week5\n",
      "numOftest 10 , score 0.7\n",
      "[['West Brom' 'Swansea' '0.5511102283607991' '0.2122490055870252'\n",
      "  '0.23664076605217568' 'False' 'True' 'False']\n",
      " ['Arsenal' 'Southampton' '0.7047354072355615' '0.15217278126486725'\n",
      "  '0.14309181149957115' 'False' 'True' 'False']\n",
      " ['Watford' 'Chelsea' '0.23517171681121207' '0.20827456150672627'\n",
      "  '0.5565537216820616' 'False' 'True' 'False']]\n",
      "first2 : {} 0.8\n",
      "week6\n",
      "numOftest 10 , score 0.4\n",
      "[['West Ham' 'Man City' '0.3439878204871059' '0.22437971797860526'\n",
      "  '0.43163246153428875' 'False' 'True' 'False']\n",
      " ['West Brom' 'Aston Villa' '0.6541887125881098' '0.16759217198192308'\n",
      "  '0.17821911542996702' 'False' 'True' 'False']\n",
      " ['Man United' 'Southampton' '0.57435080841755' '0.19132325939401854'\n",
      "  '0.23432593218843148' 'False' 'False' 'True']\n",
      " ['Sunderland' 'Bournemouth' '0.3261065539484946' '0.2709930839089244'\n",
      "  '0.4029003621425811' 'False' 'True' 'False']\n",
      " ['Arsenal' 'Chelsea' '0.7033346301379134' '0.1374418635886164'\n",
      "  '0.15922350627347034' 'False' 'False' 'True']\n",
      " ['Everton' 'Swansea' '0.6641923530306354' '0.16644838474243784'\n",
      "  '0.16935926222692674' 'False' 'False' 'True']]\n",
      "first2 : {} 0.7\n",
      "week7\n",
      "numOftest 10 , score 0.4\n",
      "[['Aston Villa' 'Leicester' '0.20296838395815983' '0.22599272249643548'\n",
      "  '0.5710388935454046' 'False' 'True' 'False']\n",
      " ['Newcastle' 'West Ham' '0.2505755132096801' '0.2573596930347198'\n",
      "  '0.4920647937556001' 'True' 'False' 'False']\n",
      " ['Chelsea' 'Everton' '0.5904924103275114' '0.2033631514332963'\n",
      "  '0.20614443823919223' 'False' 'True' 'False']\n",
      " ['Liverpool' 'Man United' '0.5728953526813209' '0.190541798350153'\n",
      "  '0.23656284896852614' 'False' 'False' 'True']\n",
      " ['Stoke' 'Arsenal' '0.487706691706777' '0.20898265520549925'\n",
      "  '0.30331065308772376' 'False' 'True' 'False']\n",
      " ['Swansea' 'Watford' '0.30680317332020696' '0.27989553794812644'\n",
      "  '0.4133012887316666' 'True' 'False' 'False']]\n",
      "first2 : {} 0.7\n",
      "week8\n",
      "numOftest 10 , score 0.1\n",
      "[['Newcastle' 'Man United' '0.18268259489184996' '0.24459150140976174'\n",
      "  '0.5727259036983884' 'False' 'True' 'False']\n",
      " ['Aston Villa' 'Crystal Palace' '0.16529940092917741'\n",
      "  '0.22180400776297104' '0.6128965913078516' 'True' 'False' 'False']\n",
      " ['Bournemouth' 'West Ham' '0.41004332230283214' '0.2706391493253951'\n",
      "  '0.3193175283717726' 'False' 'False' 'True']\n",
      " ['Chelsea' 'West Brom' '0.6446879565989463' '0.17120717662763585'\n",
      "  '0.18410486677341784' 'False' 'True' 'False']\n",
      " ['Man City' 'Everton' '0.8496146302885587' '0.08852619500816233'\n",
      "  '0.06185917470327903' 'False' 'True' 'False']\n",
      " ['Southampton' 'Watford' '0.24893743482341477' '0.28715421619534176'\n",
      "  '0.4639083489812436' 'True' 'False' 'False']\n",
      " ['Swansea' 'Sunderland' '0.5727519907052087' '0.22661171130999275'\n",
      "  '0.20063629798479854' 'False' 'False' 'True']\n",
      " ['Tottenham' 'Leicester' '0.6225228491148629' '0.15409355245615936'\n",
      "  '0.2233835984289778' 'False' 'False' 'True']\n",
      " ['Liverpool' 'Arsenal' '0.5117451350408921' '0.19658603437388825'\n",
      "  '0.29166883058521953' 'False' 'True' 'False']]\n",
      "first2 : {} 0.5\n",
      "week9\n",
      "numOftest 10 , score 0.6\n",
      "[['West Ham' 'Liverpool' '0.35432624086988745' '0.20271395832054206'\n",
      "  '0.4429598008095704' 'True' 'False' 'False']\n",
      " ['West Brom' 'Stoke' '0.30608821915448614' '0.2563808880930201'\n",
      "  '0.4375308927524938' 'True' 'False' 'False']\n",
      " ['Leicester' 'Bournemouth' '0.6256591675331855' '0.1638121493630969'\n",
      "  '0.21052868310371753' 'False' 'True' 'False']\n",
      " ['Everton' 'Tottenham' '0.40426614543312944' '0.25269759393224783'\n",
      "  '0.34303626063462284' 'False' 'True' 'False']]\n",
      "first2 : {} 0.8\n",
      "week10\n",
      "numOftest 20 , score 0.45\n",
      "[['Stoke' 'Man United' '0.36683885516721004' '0.2204033244828516'\n",
      "  '0.4127578203499383' 'True' 'False' 'False']\n",
      " ['Southampton' 'Arsenal' '0.2514895133502345' '0.2347860777703539'\n",
      "  '0.5137244088794116' 'True' 'False' 'False']\n",
      " ['Chelsea' 'Watford' '0.2949963515241796' '0.21755219142085414'\n",
      "  '0.4874514570549663' 'False' 'True' 'False']\n",
      " ['Bournemouth' 'Crystal Palace' '0.44154341285051024' '0.2294363638539514'\n",
      "  '0.3290202232955384' 'False' 'True' 'False']\n",
      " ['Aston Villa' 'West Ham' '0.241382051940047' '0.25067094407902996'\n",
      "  '0.507947003980923' 'False' 'True' 'False']\n",
      " ['Liverpool' 'Leicester' '0.3792369434038846' '0.2371871927786881'\n",
      "  '0.3835758638174274' 'True' 'False' 'False']\n",
      " ['Norwich' 'Aston Villa' '0.33136215409013087' '0.295807233559454'\n",
      "  '0.37283061235041515' 'True' 'False' 'False']\n",
      " ['Everton' 'Stoke' '0.6685704912967122' '0.17308472287906765'\n",
      "  '0.15834478582422015' 'False' 'False' 'True']\n",
      " ['Crystal Palace' 'Swansea' '0.6707814383006035' '0.13900583904113492'\n",
      "  '0.19021272265826147' 'False' 'True' 'False']\n",
      " ['Man United' 'Chelsea' '0.5572293561119752' '0.18463442965507387'\n",
      "  '0.2581362142329508' 'False' 'True' 'False']\n",
      " ['Leicester' 'Man City' '0.36382462760196116' '0.2216310904322187'\n",
      "  '0.41454428196582005' 'False' 'True' 'False']]\n",
      "first2 : {} 0.7\n",
      "week11\n",
      "numOftest 10 , score 0.5\n",
      "[['Newcastle' 'Aston Villa' '0.5737894334830334' '0.18319245532743414'\n",
      "  '0.2430181111895325' 'False' 'True' 'False']\n",
      " ['Man United' 'Norwich' '0.8881879844659396' '0.074166441027549'\n",
      "  '0.03764557450651137' 'False' 'False' 'True']\n",
      " ['Everton' 'Leicester' '0.44728217923588776' '0.20489330765369954'\n",
      "  '0.3478245131104127' 'False' 'False' 'True']\n",
      " ['Swansea' 'West Ham' '0.34838202613354197' '0.2404110482419321'\n",
      "  '0.41120692562452593' 'False' 'True' 'False']\n",
      " ['Watford' 'Liverpool' '0.31654153412579655' '0.1764201432766034'\n",
      "  '0.5070383225976001' 'True' 'False' 'False']]\n",
      "first2 : {} 0.7\n",
      "week12\n",
      "numOftest 10 , score 0.5\n",
      "[['Norwich' 'Everton' '0.20805462286096096' '0.2761510082283178'\n",
      "  '0.5157943689107213' 'False' 'True' 'False']\n",
      " ['West Ham' 'Stoke' '0.5096615277957777' '0.22565376466953174'\n",
      "  '0.2646847075346906' 'False' 'True' 'False']\n",
      " ['Bournemouth' 'Man United' '0.1805396670071243' '0.23457159125527838'\n",
      "  '0.5848887417375973' 'True' 'False' 'False']\n",
      " ['Tottenham' 'Newcastle' '0.9006261751543769' '0.06288826573948328'\n",
      "  '0.036485559106139674' 'False' 'False' 'True']\n",
      " ['Liverpool' 'West Brom' '0.7308787559710344' '0.14926788220587564'\n",
      "  '0.11985336182309009' 'False' 'True' 'False']]\n",
      "first2 : {} 0.7\n",
      "week13\n",
      "numOftest 10 , score 0.3\n",
      "[['Southampton' 'Aston Villa' '0.6570585579047978' '0.16303202632427938'\n",
      "  '0.17990941577092287' 'False' 'True' 'False']\n",
      " ['Chelsea' 'Bournemouth' '0.7942836704773929' '0.12557079484713757'\n",
      "  '0.08014553467546949' 'False' 'False' 'True']\n",
      " ['Stoke' 'Man City' '0.28470953926042925' '0.23313447296107487'\n",
      "  '0.4821559877784959' 'True' 'False' 'False']\n",
      " ['Man United' 'West Ham' '0.7501625043960142' '0.11062247856653357'\n",
      "  '0.13921501703745215' 'False' 'True' 'False']\n",
      " ['West Brom' 'Tottenham' '0.14525455338046644' '0.20693871543849118'\n",
      "  '0.6478067311810424' 'False' 'True' 'False']\n",
      " ['Newcastle' 'Liverpool' '0.1540116947189148' '0.21756472310598093'\n",
      "  '0.6284235821751043' 'True' 'False' 'False']\n",
      " ['Everton' 'Crystal Palace' '0.6412964493946259' '0.16633837973742827'\n",
      "  '0.1923651708679458' 'False' 'True' 'False']]\n",
      "first2 : {} 0.5\n",
      "week14\n",
      "numOftest 10 , score 0.3\n",
      "[['Crystal Palace' 'Newcastle' '0.31472660774870875' '0.2836425079385674'\n",
      "  '0.4016308843127238' 'True' 'False' 'False']\n",
      " ['Bournemouth' 'Everton' '0.15321093081957204' '0.24794306883566214'\n",
      "  '0.5988460003447658' 'False' 'True' 'False']\n",
      " ['Leicester' 'Man United' '0.5701339279225496' '0.1646738310656556'\n",
      "  '0.2651922410117948' 'False' 'True' 'False']\n",
      " ['Sunderland' 'Stoke' '0.2343938053865604' '0.30615097070374725'\n",
      "  '0.45945522390969235' 'True' 'False' 'False']\n",
      " ['Tottenham' 'Chelsea' '0.7456546471266946' '0.12802384059518526'\n",
      "  '0.12632151227812005' 'False' 'True' 'False']\n",
      " ['West Ham' 'West Brom' '0.6304942799853258' '0.18162212227754107'\n",
      "  '0.18788359773713312' 'False' 'True' 'False']\n",
      " ['Norwich' 'Arsenal' '0.14117355173162774' '0.20473041930530342'\n",
      "  '0.6540960289630687' 'False' 'True' 'False']]\n",
      "first2 : {} 0.7\n",
      "week15\n",
      "numOftest 10 , score 0.5\n",
      "[['Swansea' 'Bournemouth' '0.8334592786646949' '0.091916494586056'\n",
      "  '0.07462422674924898' 'False' 'True' 'False']\n",
      " ['West Brom' 'Arsenal' '0.1270465873265253' '0.19318631338007544'\n",
      "  '0.6797670992933994' 'True' 'False' 'False']\n",
      " ['Southampton' 'Stoke' '0.6656145021098688' '0.16499120787389746'\n",
      "  '0.16939429001623368' 'False' 'False' 'True']\n",
      " ['Man City' 'Liverpool' '0.8361800767215576' '0.07919426703333224'\n",
      "  '0.08462565624511022' 'False' 'False' 'True']\n",
      " ['Crystal Palace' 'Sunderland' '0.6805686165748083' '0.146565518084437'\n",
      "  '0.1728658653407548' 'False' 'False' 'True']]\n",
      "first2 : {} 0.9\n",
      "week16\n",
      "numOftest 10 , score 0.4\n",
      "[['West Ham' 'Everton' '0.6127853128416136' '0.21529280322979896'\n",
      "  '0.1719218839285874' 'False' 'True' 'False']\n",
      " ['Stoke' 'Chelsea' '0.3501244990094231' '0.2087569195943698'\n",
      "  '0.44111858139620713' 'True' 'False' 'False']\n",
      " ['Bournemouth' 'Newcastle' '0.6564771588791911' '0.17386711005442804'\n",
      "  '0.16965573106638088' 'False' 'False' 'True']\n",
      " ['Arsenal' 'Tottenham' '0.6454882950137816' '0.15244885218849208'\n",
      "  '0.2020628527977263' 'False' 'True' 'False']\n",
      " ['Aston Villa' 'Man City' '0.06190021555866233' '0.18216045063896188'\n",
      "  '0.7559393338023757' 'False' 'True' 'False']\n",
      " ['Liverpool' 'Crystal Palace' '0.5766073544173455' '0.18160991222672443'\n",
      "  '0.2417827333559301' 'False' 'False' 'True']]\n",
      "first2 : {} 0.8\n",
      "week17\n",
      "numOftest 10 , score 0.7\n",
      "[['Chelsea' 'Liverpool' '0.5831598548865173' '0.1798672751212603'\n",
      "  '0.23697286999222245' 'False' 'False' 'True']\n",
      " ['Crystal Palace' 'Man United' '0.20146195219951532' '0.22152662274395482'\n",
      "  '0.5770114250565298' 'False' 'True' 'False']\n",
      " ['Newcastle' 'Stoke' '0.45334242099055305' '0.2335494663851795'\n",
      "  '0.3131081126242674' 'False' 'True' 'False']]\n",
      "first2 : {} 0.9\n",
      "week18\n",
      "numOftest 10 , score 0.5\n",
      "[['Norwich' 'West Brom' '0.477434387140622' '0.22523295006257107'\n",
      "  '0.2973326627968069' 'False' 'False' 'True']\n",
      " ['Sunderland' 'Newcastle' '0.3367241627074278' '0.26176951298318807'\n",
      "  '0.4015063243093842' 'True' 'False' 'False']\n",
      " ['Man United' 'Man City' '0.3026860254592935' '0.17967052744283873'\n",
      "  '0.5176434470978677' 'False' 'True' 'False']\n",
      " ['Bournemouth' 'Tottenham' '0.40783598472982335' '0.2717087577248965'\n",
      "  '0.3204552575452802' 'False' 'False' 'True']\n",
      " ['Liverpool' 'Southampton' '0.6234907383263366' '0.1677545760233062'\n",
      "  '0.20875468565035718' 'False' 'True' 'False']]\n",
      "first2 : {} 0.8\n",
      "week19\n",
      "numOftest 10 , score 0.5\n",
      "[['Crystal Palace' 'West Ham' '0.6403959116381643' '0.14619935461961173'\n",
      "  '0.21340473374222402' 'False' 'False' 'True']\n",
      " ['Tottenham' 'Liverpool' '0.7734455732032907' '0.10433920744996932'\n",
      "  '0.12221521934673991' 'False' 'True' 'False']\n",
      " ['Southampton' 'Leicester' '0.8846929895656038' '0.0647544441077604'\n",
      "  '0.05055256632663572' 'False' 'True' 'False']\n",
      " ['Everton' 'Man United' '0.5272582249466219' '0.19426012603763673'\n",
      "  '0.27848164901574135' 'False' 'False' 'True']\n",
      " ['Swansea' 'Stoke' '0.46981702801323066' '0.25247905093116796'\n",
      "  '0.2777039210556013' 'False' 'False' 'True']]\n",
      "first2 : {} 0.9\n",
      "week20\n",
      "numOftest 10 , score 0.3\n",
      "[['Aston Villa' 'Stoke' '0.3897477901808693' '0.26181412452625696'\n",
      "  '0.3484380852928736' 'False' 'False' 'True']\n",
      " ['Bournemouth' 'Watford' '0.5856534814780068' '0.1478946612570425'\n",
      "  '0.26645185726495085' 'False' 'True' 'False']\n",
      " ['Chelsea' 'Southampton' '0.71706702824917' '0.1526415641573102'\n",
      "  '0.13029140759351981' 'False' 'False' 'True']\n",
      " ['Norwich' 'Leicester' '0.5568805663401492' '0.18074438854802144'\n",
      "  '0.2623750451118294' 'False' 'False' 'True']\n",
      " ['Sunderland' 'West Ham' '0.2325518394753812' '0.26710321697513195'\n",
      "  '0.5003449435494869' 'False' 'True' 'False']\n",
      " ['Everton' 'Liverpool' '0.5355196204363487' '0.19149434562700418'\n",
      "  '0.27298603393664717' 'False' 'True' 'False']\n",
      " ['Swansea' 'Tottenham' '0.2035440576780306' '0.23378804340380194'\n",
      "  '0.5626678989181675' 'False' 'True' 'False']]\n",
      "first2 : {} 0.7\n",
      "week21\n",
      "numOftest 10 , score 0.6\n",
      "[['West Ham' 'Norwich' '0.5438079905286296' '0.2072913486900292'\n",
      "  '0.24890066078134118' 'False' 'True' 'False']\n",
      " ['Newcastle' 'Chelsea' '0.11329415794835691' '0.19281262030805024'\n",
      "  '0.6938932217435929' 'False' 'True' 'False']\n",
      " ['Tottenham' 'Man City' '0.35416156354404604' '0.18684798857218424'\n",
      "  '0.45899044788376964' 'True' 'False' 'False']\n",
      " ['West Brom' 'Everton' '0.3869871025327655' '0.2532816810036383'\n",
      "  '0.3597312164635961' 'False' 'False' 'True']]\n",
      "first2 : {} 0.9\n",
      "week22\n",
      "numOftest 10 , score 0.3\n",
      "[['Stoke' 'Leicester' '0.4426734399491675' '0.21364760508453284'\n",
      "  '0.34367895496629963' 'False' 'True' 'False']\n",
      " ['Swansea' 'Everton' '0.31718295124608265' '0.26921925948732706'\n",
      "  '0.41359778926659024' 'False' 'True' 'False']\n",
      " ['Chelsea' 'Arsenal' '0.2809215877351943' '0.23031287596055366'\n",
      "  '0.48876553630425196' 'True' 'False' 'False']\n",
      " ['Aston Villa' 'West Brom' '0.4169333166011775' '0.22993006041426003'\n",
      "  '0.35313662298456244' 'False' 'False' 'True']\n",
      " ['Man City' 'West Ham' '0.9079540602726311' '0.06058891545726985'\n",
      "  '0.03145702427009901' 'False' 'False' 'True']\n",
      " ['Liverpool' 'Norwich' '0.6941317293982114' '0.15319389556572713'\n",
      "  '0.15267437503606132' 'False' 'True' 'False']\n",
      " ['Southampton' 'Man United' '0.4040677829614437' '0.22233602377624911'\n",
      "  '0.37359619326230714' 'False' 'False' 'True']]\n",
      "first2 : {} 0.7\n",
      "week23\n",
      "numOftest 8 , score 0.75\n",
      "[['Everton' 'Chelsea' '0.3209144030408954' '0.21847798164743726'\n",
      "  '0.4606076153116673' 'True' 'False' 'False']\n",
      " ['West Brom' 'Southampton' '0.11283237492768336' '0.17330454385897281'\n",
      "  '0.7138630812133439' 'False' 'True' 'False']]\n",
      "first2 : {} 1.0\n",
      "week24\n",
      "numOftest 8 , score 0.25\n",
      "[['Liverpool' 'West Ham' '0.7980676170653893' '0.1194419062953243'\n",
      "  '0.08249047663928637' 'False' 'False' 'True']\n",
      " ['Tottenham' 'Everton' '0.6044988391913967' '0.1846502267217506'\n",
      "  '0.21085093408685265' 'False' 'True' 'False']\n",
      " ['Chelsea' 'Crystal Palace' '0.6443723515308636' '0.18027105260535473'\n",
      "  '0.17535659586378166' 'False' 'False' 'True']\n",
      " ['Stoke' 'West Brom' '0.7435963236281615' '0.14708289723987833'\n",
      "  '0.10932077913196031' 'False' 'False' 'True']\n",
      " ['Aston Villa' 'Sunderland' '0.3308096877947433' '0.254991489923674'\n",
      "  '0.4141988222815828' 'False' 'True' 'False']\n",
      " ['Swansea' 'Man United' '0.36881931519257966' '0.23347308175727832'\n",
      "  '0.39770760305014197' 'True' 'False' 'False']]\n",
      "first2 : {} 0.375\n",
      "week25\n",
      "numOftest 8 , score 0.375\n",
      "[['Leicester' 'Tottenham' '0.6062607045787434' '0.15652480028997612'\n",
      "  '0.23721449513128054' 'False' 'True' 'False']\n",
      " ['Man United' 'Newcastle' '0.9009463216642334' '0.05873820347149182'\n",
      "  '0.04031547486427483' 'False' 'True' 'False']\n",
      " ['Sunderland' 'Swansea' '0.220935473846393' '0.24656257072495577'\n",
      "  '0.5325019554286513' 'False' 'True' 'False']\n",
      " ['Norwich' 'Stoke' '0.33366171146712775' '0.26116417570165984'\n",
      "  '0.4051741128312124' 'False' 'True' 'False']\n",
      " ['Arsenal' 'Liverpool' '0.7721390623285717' '0.12647667240065263'\n",
      "  '0.10138426527077572' 'False' 'True' 'False']]\n",
      "first2 : {} 0.75\n",
      "week26\n",
      "numOftest 8 , score 0.625\n",
      "[['Tottenham' 'Stoke' '0.5873616750636489' '0.19588106128335198'\n",
      "  '0.21675726365299916' 'False' 'True' 'False']\n",
      " ['Sunderland' 'Norwich' '0.5682176259241678' '0.20124854764565805'\n",
      "  '0.23053382643017414' 'False' 'False' 'True']\n",
      " ['Southampton' 'Everton' '0.6030359946556412' '0.20658892254452246'\n",
      "  '0.1903750827998364' 'False' 'False' 'True']]\n",
      "first2 : {} 0.75\n",
      "week27\n",
      "numOftest 8 , score 0.5\n",
      "[['Chelsea' 'Swansea' '0.8338318732705389' '0.10229239710856305'\n",
      "  '0.06387572962089803' 'False' 'True' 'False']\n",
      " ['Stoke' 'Liverpool' '0.5574659811520635' '0.1823658044039756'\n",
      "  '0.2601682144439609' 'False' 'False' 'True']\n",
      " ['Arsenal' 'West Ham' '0.9335027568200696' '0.046154884060346575'\n",
      "  '0.020342359119583885' 'False' 'False' 'True']\n",
      " ['Newcastle' 'Southampton' '0.5191629421875946' '0.19085255164373527'\n",
      "  '0.28998450616867005' 'False' 'True' 'False']]\n",
      "first2 : {} 0.75\n",
      "week28\n",
      "numOftest 10 , score 0.7\n",
      "[['Everton' 'Tottenham' '0.5281576639879249' '0.19881613390952488'\n",
      "  '0.27302620210255024' 'False' 'False' 'True']\n",
      " ['Hull' 'Man United' '0.19073085361761807' '0.2123463030442688'\n",
      "  '0.5969228433381131' 'False' 'True' 'False']\n",
      " ['Aston Villa' 'Burnley' '0.6164364784779799' '0.187712926012208'\n",
      "  '0.19585059550981213' 'False' 'False' 'True']]\n",
      "first2 : {} 1.0\n",
      "week29\n",
      "numOftest 11 , score 0.36363636363636365\n",
      "[['Burnley' 'Stoke' '0.2424712056174294' '0.25540808791752767'\n",
      "  '0.5021207064650429' 'False' 'True' 'False']\n",
      " ['Sunderland' 'Leicester' '0.4945730893617539' '0.203056935905337'\n",
      "  '0.30236997473290905' 'False' 'True' 'False']\n",
      " ['Liverpool' 'Crystal Palace' '0.7922542831972775' '0.1152392743855766'\n",
      "  '0.09250644241714587' 'False' 'False' 'True']\n",
      " ['West Ham' 'Everton' '0.4048299216537181' '0.2294163296652648'\n",
      "  '0.36575374868101707' 'False' 'False' 'True']\n",
      " ['Man United' 'Arsenal' '0.3027030024512451' '0.20780527937568866'\n",
      "  '0.4894917181730662' 'False' 'True' 'False']\n",
      " ['West Brom' 'Chelsea' '0.19974308125150342' '0.22708824350861279'\n",
      "  '0.5731686752398838' 'True' 'False' 'False']\n",
      " ['Arsenal' 'Sunderland' '0.7754659450637541' '0.12166391560513999'\n",
      "  '0.1028701393311059' 'False' 'True' 'False']]\n",
      "first2 : {} 0.636363636364\n",
      "summary\n",
      "score:\n",
      "0.476973684211\n",
      "2like\n",
      "       h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  d_Precent  \\\n",
      ">80           24       10   0.705882          0        0        NaN   \n",
      "60-80         34       43   0.441558          0        0        NaN   \n",
      "50-60         14       25   0.358974          0        0        NaN   \n",
      "40-50         22       18   0.550000          0        1   0.000000   \n",
      "30-40         16       23   0.410256          1        2   0.333333   \n",
      "20-30          7       34   0.170732         39       97   0.286765   \n",
      "<20            7       27   0.205882         41      123   0.250000   \n",
      "\n",
      "       a_Correct  a_Wrong  a_Precent  \n",
      ">80            2        0   1.000000  \n",
      "60-80         13       10   0.565217  \n",
      "50-60         17       20   0.459459  \n",
      "40-50         14       27   0.341463  \n",
      "30-40         13       29   0.309524  \n",
      "20-30         18       43   0.295082  \n",
      "<20           22       76   0.224490  \n",
      "sum precision:[ 0.49222798  0.          0.45454545]\n"
     ]
    }
   ],
   "source": [
    "clf.set_params(alpha=g_alpha)\n",
    "print(clf)\n",
    "train_score, test_score, first2 = futureTest(mlp,X,y,numOfWeek = 30, verbose=True)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y/scikit-learn/sklearn/metrics/classification.py:1098: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.56491228,  0.        ,  0.54862843])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from custom import SoftMaxMLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def testRecentNum(start, end):\n",
    "    recent_range = range(start,end)\n",
    "    train_means = []\n",
    "    train_std = []\n",
    "    test_means =[]\n",
    "    test_std=[]\n",
    "    first2_mean=[]\n",
    "    for recent in recent_range:\n",
    "        print(\"start recent:{}\".format(recent))\n",
    "        X,y = c.getH7(recent)\n",
    "        clf = SoftMaxMLPClassifier(hidden_layer_sizes=[g_alpha], activation='logistic', algorithm='l-bfgs', alpha=g_alpha, \n",
    "                  learning_rate_init=0.01,learning_rate='adaptive' ,max_iter=500,early_stopping = True,verbose = 3)\n",
    "        mlp = Pipeline([('scl', StandardScaler()),('clf', clf)])\n",
    "        train_scores,test_scores, first2 = crossValidate(mlp,X,y,fold=10)\n",
    "        #train_scores,test_scores, first2 = futureTest(mlp, X,y,numOfWeek = 15)\n",
    "        train_means.append(np.mean(train_scores))\n",
    "        train_std.append(np.std(train_scores))\n",
    "        test_means.append(np.mean(test_scores))\n",
    "        test_std.append(np.std(test_scores))\n",
    "        first2_mean.append(np.mean(first2))\n",
    "        print(\"recent {}: train_mean {}  v.s. test_mean {} , first2_mean {}\".format(\n",
    "                recent,np.mean(train_scores),np.mean(test_scores),np.mean(first2)))\n",
    "    plotCurve(np.array(train_means),np.array(train_std),np.array(test_means),np.array(test_std),np.array(recent_range))\n",
    "    return train_means,train_std,test_means,test_std,first2_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start recent:1\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [554 314 389], Acc: 0.532\n",
      "Fold: 2, Class dist.: [554 314 389], Acc: 0.532\n",
      "Fold: 3, Class dist.: [554 314 389], Acc: 0.546\n",
      "Fold: 4, Class dist.: [554 314 390], Acc: 0.557\n",
      "Fold: 5, Class dist.: [554 314 390], Acc: 0.543\n",
      "Fold: 6, Class dist.: [554 314 390], Acc: 0.514\n",
      "Fold: 7, Class dist.: [555 314 390], Acc: 0.511\n",
      "Fold: 8, Class dist.: [555 314 390], Acc: 0.525\n",
      "Fold: 9, Class dist.: [555 314 390], Acc: 0.475\n",
      "Fold: 10, Class dist.: [555 315 390], Acc: 0.486\n",
      "recent 1: train_mean 0.5517397415529908  v.s. test_mean 0.5220513405583185 , first2_mean 0.7603244144475618\n",
      "start recent:2\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [542 300 385], Acc: 0.558\n",
      "Fold: 2, Class dist.: [542 300 385], Acc: 0.514\n",
      "Fold: 3, Class dist.: [542 300 385], Acc: 0.536\n",
      "Fold: 4, Class dist.: [543 300 385], Acc: 0.540\n",
      "Fold: 5, Class dist.: [543 301 385], Acc: 0.500\n",
      "Fold: 6, Class dist.: [543 301 385], Acc: 0.522\n",
      "Fold: 7, Class dist.: [543 301 385], Acc: 0.544\n",
      "Fold: 8, Class dist.: [543 301 385], Acc: 0.500\n",
      "Fold: 9, Class dist.: [543 301 386], Acc: 0.452\n",
      "Fold: 10, Class dist.: [543 301 386], Acc: 0.474\n",
      "recent 2: train_mean 0.5572640942516806  v.s. test_mean 0.5140944034089534 , first2_mean 0.7479846492001364\n",
      "start recent:3\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [530 294 373], Acc: 0.545\n",
      "Fold: 2, Class dist.: [530 294 373], Acc: 0.507\n",
      "Fold: 3, Class dist.: [530 294 373], Acc: 0.552\n",
      "Fold: 4, Class dist.: [530 294 373], Acc: 0.530\n",
      "Fold: 5, Class dist.: [530 294 373], Acc: 0.560\n",
      "Fold: 6, Class dist.: [530 294 374], Acc: 0.511\n",
      "Fold: 7, Class dist.: [530 294 374], Acc: 0.541\n",
      "Fold: 8, Class dist.: [530 295 374], Acc: 0.515\n",
      "Fold: 9, Class dist.: [530 295 374], Acc: 0.470\n",
      "Fold: 10, Class dist.: [531 295 374], Acc: 0.504\n",
      "recent 3: train_mean 0.5616470677406641  v.s. test_mean 0.5235326708435252 , first2_mean 0.7617636583976539\n",
      "start recent:4\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [513 284 364], Acc: 0.546\n",
      "Fold: 2, Class dist.: [513 284 364], Acc: 0.508\n",
      "Fold: 3, Class dist.: [513 284 364], Acc: 0.569\n",
      "Fold: 4, Class dist.: [513 284 364], Acc: 0.554\n",
      "Fold: 5, Class dist.: [513 284 364], Acc: 0.538\n",
      "Fold: 6, Class dist.: [513 284 365], Acc: 0.512\n",
      "Fold: 7, Class dist.: [513 285 365], Acc: 0.547\n",
      "Fold: 8, Class dist.: [513 285 365], Acc: 0.523\n",
      "Fold: 9, Class dist.: [513 285 365], Acc: 0.492\n",
      "Fold: 10, Class dist.: [513 285 365], Acc: 0.508\n",
      "recent 4: train_mean 0.5649361210857612  v.s. test_mean 0.529732502236136 , first2_mean 0.7590878428741801\n",
      "start recent:5\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [501 275 353], Acc: 0.551\n",
      "Fold: 2, Class dist.: [501 275 353], Acc: 0.488\n",
      "Fold: 3, Class dist.: [501 275 353], Acc: 0.591\n",
      "Fold: 4, Class dist.: [501 275 354], Acc: 0.571\n",
      "Fold: 5, Class dist.: [501 275 354], Acc: 0.524\n",
      "Fold: 6, Class dist.: [501 275 354], Acc: 0.524\n",
      "Fold: 7, Class dist.: [501 276 354], Acc: 0.528\n",
      "Fold: 8, Class dist.: [502 276 354], Acc: 0.532\n",
      "Fold: 9, Class dist.: [502 276 354], Acc: 0.452\n",
      "Fold: 10, Class dist.: [502 276 354], Acc: 0.508\n",
      "recent 5: train_mean 0.5637815824386981  v.s. test_mean 0.5268904362761105 , first2_mean 0.7658925932645516\n",
      "start recent:6\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [486 268 348], Acc: 0.561\n",
      "Fold: 2, Class dist.: [486 268 348], Acc: 0.504\n",
      "Fold: 3, Class dist.: [486 268 348], Acc: 0.561\n",
      "Fold: 4, Class dist.: [486 268 348], Acc: 0.577\n",
      "Fold: 5, Class dist.: [486 268 348], Acc: 0.537\n",
      "Fold: 6, Class dist.: [486 268 348], Acc: 0.512\n",
      "Fold: 7, Class dist.: [486 268 348], Acc: 0.537\n",
      "Fold: 8, Class dist.: [486 268 349], Acc: 0.525\n",
      "Fold: 9, Class dist.: [486 269 349], Acc: 0.463\n",
      "Fold: 10, Class dist.: [486 269 349], Acc: 0.446\n",
      "recent 6: train_mean 0.561722875082476  v.s. test_mean 0.5222298959204197 , first2_mean 0.7566707201416955\n",
      "start recent:7\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [471 261 340], Acc: 0.521\n",
      "Fold: 2, Class dist.: [471 262 340], Acc: 0.575\n",
      "Fold: 3, Class dist.: [471 262 340], Acc: 0.558\n",
      "Fold: 4, Class dist.: [471 262 340], Acc: 0.558\n",
      "Fold: 5, Class dist.: [472 262 340], Acc: 0.521\n",
      "Fold: 6, Class dist.: [472 262 340], Acc: 0.538\n",
      "Fold: 7, Class dist.: [472 262 340], Acc: 0.546\n",
      "Fold: 8, Class dist.: [472 262 340], Acc: 0.529\n",
      "Fold: 9, Class dist.: [472 262 341], Acc: 0.466\n",
      "Fold: 10, Class dist.: [472 262 341], Acc: 0.492\n",
      "recent 7: train_mean 0.561516238852003  v.s. test_mean 0.5304408723848132 , first2_mean 0.7585897835811731\n",
      "start recent:8\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [457 252 333], Acc: 0.508\n",
      "Fold: 2, Class dist.: [457 253 334], Acc: 0.552\n",
      "Fold: 3, Class dist.: [457 253 334], Acc: 0.560\n",
      "Fold: 4, Class dist.: [457 253 334], Acc: 0.543\n",
      "Fold: 5, Class dist.: [457 253 334], Acc: 0.526\n",
      "Fold: 6, Class dist.: [457 253 334], Acc: 0.509\n",
      "Fold: 7, Class dist.: [457 253 334], Acc: 0.543\n",
      "Fold: 8, Class dist.: [457 253 334], Acc: 0.534\n",
      "Fold: 9, Class dist.: [458 253 334], Acc: 0.461\n",
      "Fold: 10, Class dist.: [458 253 334], Acc: 0.487\n",
      "recent 8: train_mean 0.5620683560878843  v.s. test_mean 0.5223542042538052 , first2_mean 0.7585725357660152\n",
      "start recent:9\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [447 246 323], Acc: 0.518\n",
      "Fold: 2, Class dist.: [447 246 323], Acc: 0.553\n",
      "Fold: 3, Class dist.: [447 246 323], Acc: 0.561\n",
      "Fold: 4, Class dist.: [447 246 323], Acc: 0.526\n",
      "Fold: 5, Class dist.: [447 247 323], Acc: 0.531\n",
      "Fold: 6, Class dist.: [447 247 323], Acc: 0.549\n",
      "Fold: 7, Class dist.: [447 247 323], Acc: 0.540\n",
      "Fold: 8, Class dist.: [448 247 323], Acc: 0.527\n",
      "Fold: 9, Class dist.: [448 247 323], Acc: 0.473\n",
      "Fold: 10, Class dist.: [448 247 324], Acc: 0.505\n",
      "recent 9: train_mean 0.5573252309738873  v.s. test_mean 0.5281868267895282 , first2_mean 0.7619358504488919\n",
      "start recent:10\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [435 242 308], Acc: 0.514\n",
      "Fold: 2, Class dist.: [435 242 308], Acc: 0.568\n",
      "Fold: 3, Class dist.: [435 242 308], Acc: 0.568\n",
      "Fold: 4, Class dist.: [435 242 309], Acc: 0.500\n",
      "Fold: 5, Class dist.: [436 242 309], Acc: 0.523\n",
      "Fold: 6, Class dist.: [436 242 309], Acc: 0.532\n",
      "Fold: 7, Class dist.: [436 242 309], Acc: 0.505\n",
      "Fold: 8, Class dist.: [436 242 309], Acc: 0.541\n",
      "Fold: 9, Class dist.: [436 242 309], Acc: 0.459\n",
      "Fold: 10, Class dist.: [436 243 309], Acc: 0.491\n",
      "recent 10: train_mean 0.5549468067812195  v.s. test_mean 0.5199022416912326 , first2_mean 0.761000879945834\n",
      "start recent:11\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [422 234 301], Acc: 0.495\n",
      "Fold: 2, Class dist.: [422 234 301], Acc: 0.551\n",
      "Fold: 3, Class dist.: [422 234 301], Acc: 0.551\n",
      "Fold: 4, Class dist.: [422 234 301], Acc: 0.533\n",
      "Fold: 5, Class dist.: [422 234 301], Acc: 0.495\n",
      "Fold: 6, Class dist.: [422 234 302], Acc: 0.519\n",
      "Fold: 7, Class dist.: [422 234 302], Acc: 0.519\n",
      "Fold: 8, Class dist.: [422 234 302], Acc: 0.519\n",
      "Fold: 9, Class dist.: [422 234 302], Acc: 0.500\n",
      "Fold: 10, Class dist.: [423 234 302], Acc: 0.514\n",
      "recent 11: train_mean 0.5550335729763464  v.s. test_mean 0.5197057712169685 , first2_mean 0.7593856798582597\n",
      "start recent:12\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [408 223 296], Acc: 0.529\n",
      "Fold: 2, Class dist.: [408 223 296], Acc: 0.577\n",
      "Fold: 3, Class dist.: [408 223 296], Acc: 0.558\n",
      "Fold: 4, Class dist.: [408 223 296], Acc: 0.558\n",
      "Fold: 5, Class dist.: [409 223 296], Acc: 0.476\n",
      "Fold: 6, Class dist.: [409 223 296], Acc: 0.534\n",
      "Fold: 7, Class dist.: [409 223 296], Acc: 0.553\n",
      "Fold: 8, Class dist.: [409 223 296], Acc: 0.524\n",
      "Fold: 9, Class dist.: [409 224 296], Acc: 0.490\n",
      "Fold: 10, Class dist.: [409 224 297], Acc: 0.495\n",
      "recent 12: train_mean 0.5590036545665162  v.s. test_mean 0.5293778070312414 , first2_mean 0.7672154953271543\n",
      "start recent:13\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [396 213 288], Acc: 0.540\n",
      "Fold: 2, Class dist.: [396 213 288], Acc: 0.530\n",
      "Fold: 3, Class dist.: [396 213 288], Acc: 0.610\n",
      "Fold: 4, Class dist.: [396 213 288], Acc: 0.560\n",
      "Fold: 5, Class dist.: [396 213 288], Acc: 0.510\n",
      "Fold: 6, Class dist.: [396 213 288], Acc: 0.520\n",
      "Fold: 7, Class dist.: [396 213 288], Acc: 0.580\n",
      "Fold: 8, Class dist.: [396 214 288], Acc: 0.535\n",
      "Fold: 9, Class dist.: [396 214 288], Acc: 0.485\n",
      "Fold: 10, Class dist.: [396 214 288], Acc: 0.465\n",
      "recent 13: train_mean 0.5626876770626165  v.s. test_mean 0.5334848484848485 , first2_mean 0.7713030303030303\n",
      "start recent:14\n",
      "start format\n",
      " progress 1432finish\n",
      "Fold: 1, Class dist.: [380 209 278], Acc: 0.541\n",
      "Fold: 2, Class dist.: [380 209 278], Acc: 0.561\n",
      "Fold: 3, Class dist.: [380 209 278], Acc: 0.541\n",
      "Fold: 4, Class dist.: [381 210 278], Acc: 0.562\n",
      "Fold: 5, Class dist.: [381 210 278], Acc: 0.531\n",
      "Fold: 6, Class dist.: [381 210 278], Acc: 0.552\n",
      "Fold: 7, Class dist.: [381 210 278], Acc: 0.531\n",
      "Fold: 8, Class dist.: [381 210 278], Acc: 0.521\n",
      "Fold: 9, Class dist.: [381 210 278], Acc: 0.479\n",
      "Fold: 10, Class dist.: [381 210 279], Acc: 0.463\n",
      "recent 14: train_mean 0.5557846883375996  v.s. test_mean 0.5283098370927319 , first2_mean 0.7565511994271392\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEPCAYAAABRHfM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCcM551wk/wcopASBa9m/RgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f439f495cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_means,train_std,test_means,test_std,first2_mean=testRecentNum(1 ,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "proba = mlp.predict_proba(X_test)\n",
    "precisionMatrix(proba,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#future\n",
    "mlp.fit(X,y)\n",
    "inData = c.readPredict(\"future.csv\")\n",
    "print(inData)\n",
    "X_in, y_in = c.getH6(5,target=inData)\n",
    "res = mlp.predict(X_in)\n",
    "proba= mlp.predict_proba(X_in)\n",
    "print(mlp.score(X_in,y_in))\n",
    "print (np.hstack([proba,y_in]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "return\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient=True, encode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1348, 21)\n",
      "                home            away        Referee   time HRestTime  \\\n",
      "0             Fulham        Man City       M Halsey  15612         7   \n",
      "1         Man United       Tottenham          C Foy  15612         2   \n",
      "2              Stoke         Swansea         J Moss  15612         7   \n",
      "3            Norwich       Liverpool        M Jones  15612         2   \n",
      "4            Everton     Southampton      L Probert  15612         3   \n",
      "5            Arsenal         Chelsea     M Atkinson  15612         2   \n",
      "6        Aston Villa       West Brom       A Taylor  15613         4   \n",
      "7                QPR        West Ham  M Clattenburg  15614         4   \n",
      "8              Wigan         Everton       K Friend  15619         7   \n",
      "9           West Ham         Arsenal         P Dowd  15619         4   \n",
      "10         West Brom             QPR        M Jones  15619         6   \n",
      "11           Swansea         Reading         M Dean  15619         7   \n",
      "12          Man City      Sunderland      L Probert  15619         2   \n",
      "13           Chelsea         Norwich       A Taylor  15619         3   \n",
      "14         Liverpool           Stoke        L Mason  15620         2   \n",
      "15         Newcastle      Man United         H Webb  15620         2   \n",
      "16       Southampton          Fulham  M Clattenburg  15620         8   \n",
      "17         Tottenham     Aston Villa    N Swarbrick  15620         2   \n",
      "18           Norwich         Arsenal      L Probert  15633        14   \n",
      "19         Tottenham         Chelsea         M Dean  15633        13   \n",
      "20          West Ham     Southampton    N Swarbrick  15633        13   \n",
      "21           Swansea           Wigan        M Jones  15633        14   \n",
      "22        Man United           Stoke       A Taylor  15633        13   \n",
      "23         Liverpool         Reading         R East  15633        13   \n",
      "24            Fulham     Aston Villa          C Foy  15633        13   \n",
      "25         West Brom        Man City  M Clattenburg  15633        14   \n",
      "26               QPR         Everton         J Moss  15634        15   \n",
      "27        Sunderland       Newcastle     M Atkinson  15634        15   \n",
      "28           Reading          Fulham        M Jones  15640         7   \n",
      "29             Wigan        West Ham         J Moss  15640         7   \n",
      "...              ...             ...            ...    ...       ...   \n",
      "1318     Southampton         Chelsea     M Atkinson  16858        14   \n",
      "1319       Leicester         Norwich    N Swarbrick  16858        13   \n",
      "1320           Stoke     Aston Villa       K Friend  16858        14   \n",
      "1321       Tottenham         Swansea        M Jones  16859         2   \n",
      "1322      Man United         Arsenal       C Pawson  16859         2   \n",
      "1323     Aston Villa         Everton         R East  16861         3   \n",
      "1324     Bournemouth     Southampton         M Dean  16861         3   \n",
      "1325       Leicester       West Brom  M Clattenburg  16861         3   \n",
      "1326         Norwich         Chelsea        L Mason  16861         3   \n",
      "1327      Sunderland  Crystal Palace       A Taylor  16861         3   \n",
      "1328       Liverpool        Man City     M Atkinson  16862         2   \n",
      "1329        West Ham       Tottenham     A Marriner  16862         4   \n",
      "1330         Arsenal         Swansea       R Madley  16862         3   \n",
      "1331      Man United         Watford        M Jones  16862         3   \n",
      "1332           Stoke       Newcastle    N Swarbrick  16862         4   \n",
      "1333         Everton        West Ham       A Taylor  16865         3   \n",
      "1334        Man City     Aston Villa        L Mason  16865         2   \n",
      "1335         Watford       Leicester         J Moss  16865         2   \n",
      "1336       Newcastle     Bournemouth      P Tierney  16865         2   \n",
      "1337         Swansea         Norwich       C Pawson  16865         2   \n",
      "1338       Tottenham         Arsenal       M Oliver  16865         2   \n",
      "1339         Chelsea           Stoke  M Clattenburg  16865         3   \n",
      "1340     Southampton      Sunderland    N Swarbrick  16865         3   \n",
      "1341  Crystal Palace       Liverpool     A Marriner  16866         4   \n",
      "1342       West Brom      Man United         M Dean  16866         4   \n",
      "1343         Norwich        Man City         J Moss  16872         7   \n",
      "1344     Bournemouth         Swansea         R East  16872         7   \n",
      "1345           Stoke     Southampton        L Mason  16872         7   \n",
      "1346     Aston Villa       Tottenham       A Taylor  16873         8   \n",
      "1347       Leicester       Newcastle       C Pawson  16874         8   \n",
      "\n",
      "     ARestTime HAccS AAccS HAccST AAccST ... HWin HDraw HLose H goal Diff  \\\n",
      "0            3    74    79     56     53 ...    3     0     2           5   \n",
      "1            2    75    85     41     50 ...    4     0     1           6   \n",
      "2            3    46    64     24     41 ...    0     4     1          -1   \n",
      "3            2    55    79     27     40 ...    0     3     2          -6   \n",
      "4            3    94    59     52     31 ...    3     1     1           4   \n",
      "5            3    71    69     33     38 ...    2     3     0           7   \n",
      "6            3    55    58     33     35 ...    1     1     3          -4   \n",
      "7            5    54    61     32     37 ...    0     2     3          -8   \n",
      "8            7    55    95     29     54 ...    1     1     3          -4   \n",
      "9            2    70    71     43     34 ...    2     2     1           1   \n",
      "10           4    57    47     32     30 ...    2     2     1           0   \n",
      "11           7    60    42     33     18 ...    1     1     3          -4   \n",
      "12           7    80    28     50     17 ...    2     3     0           3   \n",
      "13           7    74    61     40     32 ...    4     1     0           6   \n",
      "14           8    75    50     39     26 ...    1     2     2           0   \n",
      "15           4    62    78     29     42 ...    1     3     1          -1   \n",
      "16           8    58    70     27     50 ...    1     0     4          -7   \n",
      "17           7    83    64     50     35 ...    3     2     0           4   \n",
      "18          13    55    75     32     42 ...    0     2     3          -7   \n",
      "19          14    88    71     50     42 ...    4     1     0           6   \n",
      "20          13    70    57     40     24 ...    2     2     1           2   \n",
      "21          14    75    53     42     27 ...    0     2     3          -7   \n",
      "22          13    70    48     40     25 ...    4     0     1           8   \n",
      "23          14    76    41     36     21 ...    1     2     2           0   \n",
      "24          13    70    65     44     37 ...    2     1     2           0   \n",
      "25          14    60    95     35     61 ...    3     1     1           1   \n",
      "26          15    54    92     32     54 ...    0     1     4          -5   \n",
      "27          14    33    64     21     27 ...    1     3     1          -2   \n",
      "28           7    42    73     21     39 ...    0     2     3          -4   \n",
      "29           7    54    62     29     36 ...    0     1     4          -7   \n",
      "...        ...   ...   ...    ...    ... ...  ...   ...   ...         ...   \n",
      "1318         5    51    69     17     27 ...    4     1     0           6   \n",
      "1319        14    61    51     26     17 ...    3     1     1           6   \n",
      "1320        13    49    51     17     15 ...    1     1     3          -7   \n",
      "1321        15    99    67     40     16 ...    5     0     0          10   \n",
      "1322         4    59    72     20     26 ...    2     1     2           2   \n",
      "1323         9    42    95     11     25 ...    1     1     3          -7   \n",
      "1324         3    62    49     18     14 ...    1     2     2          -3   \n",
      "1325         2    62    44     24     13 ...    4     0     1           7   \n",
      "1326         3    50    71     15     24 ...    0     1     4          -7   \n",
      "1327         2    62    66     18     17 ...    1     2     2          -1   \n",
      "1328         2    73    80     27     17 ...    2     1     2           4   \n",
      "1329         3    73   107     20     43 ...    2     2     1           2   \n",
      "1330         3    77    67     28     18 ...    2     1     2           1   \n",
      "1331         4    59    52     24     12 ...    2     1     2           2   \n",
      "1332        17    46    70     14     26 ...    2     0     3          -6   \n",
      "1333         2    96    77     28     20 ...    3     0     2           6   \n",
      "1334         3    67    41     15     12 ...    1     1     3          -5   \n",
      "1335         3    52    69     10     24 ...    1     2     2          -1   \n",
      "1336         3    52    56     19     17 ...    1     0     4          -8   \n",
      "1337         3    71    57     15     12 ...    1     2     2          -1   \n",
      "1338         2    92    86     34     31 ...    4     0     1           5   \n",
      "1339         2    69    52     20     14 ...    3     2     0           6   \n",
      "1340         3    48    69     13     19 ...    2     1     2          -1   \n",
      "1341         3    68    65     19     30 ...    0     2     3          -3   \n",
      "1342         3    51    65     16     26 ...    2     2     1           1   \n",
      "1343         7    55    73     13     21 ...    0     1     4          -5   \n",
      "1344         7    64    63     18     14 ...    2     1     2           0   \n",
      "1345         7    58    50     19     15 ...    3     1     1           1   \n",
      "1346         2    37   101     11     39 ...    1     0     4         -11   \n",
      "1347         9    70    55     25     20 ...    3     1     1           3   \n",
      "\n",
      "     AWin ADraw ALose A goal diff moraldiff + h-a  y  \n",
      "0       2     3     0           3         -5.4577  A  \n",
      "1       2     2     1           2         7.24231  A  \n",
      "2       2     1     2           3        -0.98787  H  \n",
      "3       0     2     3          -6         4.26691  A  \n",
      "4       1     0     4          -6         20.4973  H  \n",
      "5       4     1     0           7         -7.3312  A  \n",
      "6       3     1     1           3        -16.1433  D  \n",
      "7       2     2     1           1        -16.4869  A  \n",
      "8       3     1     1           5        -18.8485  D  \n",
      "9       2     2     1           6         1.33957  A  \n",
      "10      0     2     3          -4         17.7318  H  \n",
      "11      0     2     3          -5         3.22343  D  \n",
      "12      1     4     0           1         5.22302  H  \n",
      "13      0     3     2          -4         26.8163  H  \n",
      "14      1     3     1           1        -5.98228  D  \n",
      "15      4     0     1           6         -7.8114  A  \n",
      "16      2     0     3          -1        -9.28572  D  \n",
      "17      1     2     2          -3         17.1502  H  \n",
      "18      3     1     1           8        -21.3046  H  \n",
      "19      4     1     0           7        -2.72658  A  \n",
      "20      1     1     3          -5         12.8002  H  \n",
      "21      0     2     3          -6        0.178857  H  \n",
      "22      1     3     1           1         9.43603  H  \n",
      "23      0     2     3          -5         7.88105  H  \n",
      "24      1     2     2          -3         5.66887  H  \n",
      "25      3     2     0           6        -6.20904  A  \n",
      "26      2     2     1           3        -23.5168  D  \n",
      "27      1     3     1          -2       -0.225469  D  \n",
      "28      3     1     1           4         -20.955  D  \n",
      "29      2     2     1           2        -21.3318  H  \n",
      "...   ...   ...   ...         ...             ... ..  \n",
      "1318    2     3     0           5         9.59745  A  \n",
      "1319    0     1     4          -9         40.3464  H  \n",
      "1320    1     2     2          -6         4.67314  H  \n",
      "1321    2     2     1           1         24.3468  H  \n",
      "1322    2     2     1           2        -16.3937  H  \n",
      "1323    2     1     2           4        -7.89062  A  \n",
      "1324    3     1     1           2        -13.7102  H  \n",
      "1325    2     2     1           1         18.5451  D  \n",
      "1326    3     2     0           6        -36.2694  A  \n",
      "1327    0     1     4          -5         15.3122  D  \n",
      "1328    2     1     2           2        -2.28404  H  \n",
      "1329    5     0     0           8        -19.9385  H  \n",
      "1330    1     2     2          -1         13.1779  A  \n",
      "1331    2     2     1           1        -4.32401  H  \n",
      "1332    2     0     3          -6         3.14025  H  \n",
      "1333    3     1     1           3        -10.8407  A  \n",
      "1334    1     0     4          -9         13.3512  H  \n",
      "1335    3     1     1           4        -19.0615  A  \n",
      "1336    2     1     2          -1        -12.8138  A  \n",
      "1337    0     1     4          -7         19.0105  H  \n",
      "1338    2     1     2           1         13.5843  D  \n",
      "1339    3     0     2          -2         9.61243  D  \n",
      "1340    1     2     2          -1         1.78991  D  \n",
      "1341    3     1     1           8        -24.5246  A  \n",
      "1342    3     1     1           4        -4.45653  H  \n",
      "1343    2     0     3          -1        -21.0893  D  \n",
      "1344    2     1     2           0         1.36626  H  \n",
      "1345    2     1     2          -1         8.06201  A  \n",
      "1346    3     1     1           2        -30.6047  A  \n",
      "1347    1     0     4          -9         31.6266  H  \n",
      "\n",
      "[1348 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "df = pd.DataFrame(np.hstack([X,y.reshape(y.shape[0],1)]))\n",
    "\n",
    "df.columns = ['home','away','Referee','time','HRestTime','ARestTime','HAccS','AAccS','HAccST','AAccST'\n",
    "              ,'HAccP - AAccP','H/A','HWin','HDraw','HLose','H goal Diff',\n",
    "'AWin','ADraw','ALose','A goal diff','moraldiff + h-a',\n",
    "              'y']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('dataSet/moral.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start format\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "X,y = c.getH7(removeInsufficient = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "def splitData(X,y):\n",
    "    X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_test, X_val, y_test,y_val = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "def createModel(hidSize):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidSize[0], input_dim=X.shape[1], init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(hidSize[1], init='glorot_normal'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(3, init='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd = SGD(lr=1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "    return model\n",
    "model=createModel([64,64])\n",
    "earlyCallback = EarlyStopping(patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.2625 - acc: 0.3685 - val_loss: 1.0888 - val_acc: 0.4296\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1990 - acc: 0.3825 - val_loss: 1.0669 - val_acc: 0.4296\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1644 - acc: 0.3792 - val_loss: 1.0598 - val_acc: 0.4519\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1022 - acc: 0.4328 - val_loss: 1.0544 - val_acc: 0.4519\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1111 - acc: 0.4279 - val_loss: 1.0481 - val_acc: 0.4593\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0747 - acc: 0.4345 - val_loss: 1.0506 - val_acc: 0.4963\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0629 - acc: 0.4534 - val_loss: 1.0405 - val_acc: 0.4815\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0609 - acc: 0.4658 - val_loss: 1.0391 - val_acc: 0.4519\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0551 - acc: 0.4757 - val_loss: 1.0326 - val_acc: 0.4444\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0612 - acc: 0.4724 - val_loss: 1.0311 - val_acc: 0.5111\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0077 - acc: 0.5045 - val_loss: 1.0282 - val_acc: 0.5407\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0314 - acc: 0.4864 - val_loss: 1.0258 - val_acc: 0.5333\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0201 - acc: 0.5062 - val_loss: 1.0240 - val_acc: 0.5333\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0218 - acc: 0.4971 - val_loss: 1.0235 - val_acc: 0.5333\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0249 - acc: 0.5087 - val_loss: 1.0258 - val_acc: 0.4815\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0057 - acc: 0.5185 - val_loss: 1.0240 - val_acc: 0.4815\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0205 - acc: 0.4979 - val_loss: 1.0211 - val_acc: 0.5407\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0117 - acc: 0.5153 - val_loss: 1.0186 - val_acc: 0.5407\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0015 - acc: 0.5136 - val_loss: 1.0193 - val_acc: 0.5407\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9932 - acc: 0.5111 - val_loss: 1.0192 - val_acc: 0.5259\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0004 - acc: 0.5169 - val_loss: 1.0260 - val_acc: 0.4741\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0006 - acc: 0.5153 - val_loss: 1.0173 - val_acc: 0.5185\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9972 - acc: 0.5218 - val_loss: 1.0158 - val_acc: 0.5185\n",
      "Epoch 24/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9847 - acc: 0.5293 - val_loss: 1.0159 - val_acc: 0.5111\n",
      "Epoch 25/500\n",
      "1213/1213 [==============================] - 1s - loss: 0.9895 - acc: 0.5309 - val_loss: 1.0204 - val_acc: 0.5111\n",
      "Epoch 26/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9809 - acc: 0.5375 - val_loss: 1.0152 - val_acc: 0.5111\n",
      "Epoch 27/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9797 - acc: 0.5458 - val_loss: 1.0149 - val_acc: 0.5407\n",
      "Epoch 28/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9698 - acc: 0.5499 - val_loss: 1.0143 - val_acc: 0.5037\n",
      "Epoch 29/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9771 - acc: 0.5392 - val_loss: 1.0169 - val_acc: 0.5185\n",
      "Epoch 30/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9872 - acc: 0.5243 - val_loss: 1.0142 - val_acc: 0.4963\n",
      "Epoch 31/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9855 - acc: 0.5350 - val_loss: 1.0208 - val_acc: 0.5111\n",
      "Epoch 32/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9758 - acc: 0.5400 - val_loss: 1.0115 - val_acc: 0.5185\n",
      "Epoch 33/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9897 - acc: 0.5334 - val_loss: 1.0118 - val_acc: 0.4963\n",
      "Epoch 34/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9741 - acc: 0.5367 - val_loss: 1.0119 - val_acc: 0.4963\n",
      "Epoch 35/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9670 - acc: 0.5507 - val_loss: 1.0135 - val_acc: 0.5037\n",
      "Epoch 36/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9684 - acc: 0.5392 - val_loss: 1.0158 - val_acc: 0.5111\n",
      "Epoch 37/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9776 - acc: 0.5433 - val_loss: 1.0122 - val_acc: 0.4963\n",
      "Epoch 38/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9701 - acc: 0.5499 - val_loss: 1.0122 - val_acc: 0.4963\n",
      "Epoch 39/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9711 - acc: 0.5400 - val_loss: 1.0140 - val_acc: 0.5037\n",
      "Epoch 40/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9677 - acc: 0.5523 - val_loss: 1.0106 - val_acc: 0.5037\n",
      "Epoch 41/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9664 - acc: 0.5449 - val_loss: 1.0092 - val_acc: 0.5037\n",
      "Epoch 42/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9714 - acc: 0.5491 - val_loss: 1.0090 - val_acc: 0.5037\n",
      "Epoch 43/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9550 - acc: 0.5540 - val_loss: 1.0102 - val_acc: 0.5037\n",
      "Epoch 44/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9661 - acc: 0.5491 - val_loss: 1.0085 - val_acc: 0.5037\n",
      "Epoch 45/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9740 - acc: 0.5466 - val_loss: 1.0089 - val_acc: 0.4889\n",
      "Epoch 46/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9646 - acc: 0.5482 - val_loss: 1.0081 - val_acc: 0.4963\n",
      "Epoch 47/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9636 - acc: 0.5573 - val_loss: 1.0076 - val_acc: 0.4889\n",
      "Epoch 48/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9605 - acc: 0.5474 - val_loss: 1.0121 - val_acc: 0.4889\n",
      "Epoch 49/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9523 - acc: 0.5598 - val_loss: 1.0066 - val_acc: 0.4815\n",
      "Epoch 50/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9511 - acc: 0.5523 - val_loss: 1.0060 - val_acc: 0.4889\n",
      "Epoch 51/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9650 - acc: 0.5466 - val_loss: 1.0054 - val_acc: 0.4889\n",
      "Epoch 52/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9516 - acc: 0.5614 - val_loss: 1.0096 - val_acc: 0.4889\n",
      "Epoch 53/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9564 - acc: 0.5532 - val_loss: 1.0065 - val_acc: 0.4889\n",
      "Epoch 54/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9526 - acc: 0.5515 - val_loss: 1.0068 - val_acc: 0.4889\n",
      "Epoch 55/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9558 - acc: 0.5581 - val_loss: 1.0057 - val_acc: 0.4889\n",
      "Epoch 56/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9609 - acc: 0.5515 - val_loss: 1.0065 - val_acc: 0.4815\n",
      "Epoch 57/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9449 - acc: 0.5507 - val_loss: 1.0054 - val_acc: 0.4815\n",
      "Epoch 58/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9536 - acc: 0.5540 - val_loss: 1.0053 - val_acc: 0.4815\n",
      "Epoch 59/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9387 - acc: 0.5705 - val_loss: 1.0054 - val_acc: 0.4889\n",
      "Epoch 60/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9451 - acc: 0.5680 - val_loss: 1.0052 - val_acc: 0.4889\n",
      "Epoch 61/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9549 - acc: 0.5598 - val_loss: 1.0044 - val_acc: 0.4741\n",
      "Epoch 62/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9351 - acc: 0.5655 - val_loss: 1.0052 - val_acc: 0.4889\n",
      "Epoch 63/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9523 - acc: 0.5532 - val_loss: 1.0040 - val_acc: 0.4741\n",
      "Epoch 64/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9424 - acc: 0.5466 - val_loss: 1.0041 - val_acc: 0.4741\n",
      "Epoch 65/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9365 - acc: 0.5581 - val_loss: 1.0038 - val_acc: 0.4815\n",
      "Epoch 66/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9452 - acc: 0.5639 - val_loss: 1.0043 - val_acc: 0.4741\n",
      "Epoch 67/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9359 - acc: 0.5639 - val_loss: 1.0026 - val_acc: 0.4889\n",
      "Epoch 68/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9410 - acc: 0.5705 - val_loss: 1.0039 - val_acc: 0.5037\n",
      "Epoch 69/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9501 - acc: 0.5565 - val_loss: 1.0042 - val_acc: 0.4815\n",
      "Epoch 70/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9351 - acc: 0.5672 - val_loss: 1.0053 - val_acc: 0.4889\n",
      "Epoch 71/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9353 - acc: 0.5771 - val_loss: 1.0039 - val_acc: 0.4889\n",
      "Epoch 72/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9352 - acc: 0.5664 - val_loss: 1.0031 - val_acc: 0.4889\n",
      "Epoch 73/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9308 - acc: 0.5507 - val_loss: 1.0028 - val_acc: 0.4963\n",
      "Epoch 74/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9334 - acc: 0.5721 - val_loss: 1.0074 - val_acc: 0.4889\n",
      "Epoch 75/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9349 - acc: 0.5639 - val_loss: 1.0030 - val_acc: 0.5037\n",
      "Epoch 76/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9289 - acc: 0.5523 - val_loss: 1.0022 - val_acc: 0.4889\n",
      "Epoch 77/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9352 - acc: 0.5713 - val_loss: 1.0020 - val_acc: 0.4963\n",
      "Epoch 78/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9194 - acc: 0.5622 - val_loss: 1.0034 - val_acc: 0.4963\n",
      "Epoch 79/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9251 - acc: 0.5779 - val_loss: 1.0044 - val_acc: 0.4963\n",
      "Epoch 80/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9369 - acc: 0.5754 - val_loss: 1.0029 - val_acc: 0.5037\n",
      "Epoch 81/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9207 - acc: 0.5730 - val_loss: 1.0024 - val_acc: 0.4963\n",
      "Epoch 82/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9463 - acc: 0.5523 - val_loss: 1.0032 - val_acc: 0.4889\n",
      "Epoch 83/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9215 - acc: 0.5820 - val_loss: 1.0023 - val_acc: 0.5037\n",
      "Epoch 84/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9226 - acc: 0.5763 - val_loss: 1.0060 - val_acc: 0.4889\n",
      "Epoch 85/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9288 - acc: 0.5664 - val_loss: 1.0051 - val_acc: 0.4889\n",
      "Epoch 86/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9179 - acc: 0.5647 - val_loss: 1.0052 - val_acc: 0.4963\n",
      "Epoch 87/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9366 - acc: 0.5614 - val_loss: 1.0073 - val_acc: 0.4889\n",
      "Epoch 88/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9161 - acc: 0.5697 - val_loss: 1.0067 - val_acc: 0.5037\n",
      "Epoch 89/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9213 - acc: 0.5647 - val_loss: 1.0067 - val_acc: 0.4889\n",
      "Epoch 90/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9235 - acc: 0.5664 - val_loss: 1.0070 - val_acc: 0.4963\n",
      "Epoch 91/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9194 - acc: 0.5796 - val_loss: 1.0057 - val_acc: 0.4963\n",
      "Epoch 92/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9216 - acc: 0.5754 - val_loss: 1.0078 - val_acc: 0.4963\n",
      "Epoch 93/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9155 - acc: 0.5680 - val_loss: 1.0049 - val_acc: 0.4889\n",
      "Epoch 94/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9164 - acc: 0.5787 - val_loss: 1.0060 - val_acc: 0.4963\n",
      "Epoch 95/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9262 - acc: 0.5779 - val_loss: 1.0055 - val_acc: 0.4963\n",
      "Epoch 96/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9103 - acc: 0.5878 - val_loss: 1.0048 - val_acc: 0.4889\n",
      "Epoch 97/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9166 - acc: 0.5771 - val_loss: 1.0057 - val_acc: 0.4889\n",
      "Epoch 98/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9192 - acc: 0.5804 - val_loss: 1.0067 - val_acc: 0.5037\n",
      "Epoch 00097: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_scaled,y,verbose=1,nb_epoch=500, validation_split=0.1,show_accuracy=True, callbacks=[earlyCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348/1348 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8903485469365332"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD7CAYAAACMlyg3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbad5acb940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "x_range = range(len(history.history['val_loss']))\n",
    "plt.plot(x_range,history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def crossValidate2(model, X,y, fold = 10):\n",
    "    y_label = np.argmax(y,axis=1)\n",
    "\n",
    "    kfold = StratifiedKFold(y=y_label, \n",
    "                             n_folds=fold,\n",
    "                            random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    train_scores=[]\n",
    "    proba_test = []\n",
    "    proba_y=[]\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "        earlyCallback = EarlyStopping(patience=20,verbose=1)\n",
    "        history = model.fit(X_scaled,y,verbose=1,nb_epoch=500, validation_split=0.1,show_accuracy=True, callbacks=[earlyCallback])\n",
    "      #  firstNScores.append(firstNScore(2, model.predict_proba(X[test]), y[test]))\n",
    "        score = model.evaluate(X[test],y[test])\n",
    "        proba_test.append(model.predict_proba(X[test]))\n",
    "        proba_y.append(y[test])\n",
    "        train_scores.append(model.evaluate(X[train],y[train]))\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, val_loss: %.3f' % (k+1, \n",
    "                    np.bincount(y_label[train]), score))    \n",
    "        \n",
    "        \n",
    "    return train_scores,scores, proba_test,proba_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8406 - acc: 0.5911 - val_loss: 0.8669 - val_acc: 0.5778\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8609 - acc: 0.6010 - val_loss: 0.8678 - val_acc: 0.5852\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8602 - acc: 0.5903 - val_loss: 0.8699 - val_acc: 0.5926\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8322 - acc: 0.6010 - val_loss: 0.8701 - val_acc: 0.5926\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8512 - acc: 0.5969 - val_loss: 0.8710 - val_acc: 0.5778\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8344 - acc: 0.5960 - val_loss: 0.8710 - val_acc: 0.5926\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8433 - acc: 0.6183 - val_loss: 0.8722 - val_acc: 0.5852\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8436 - acc: 0.6051 - val_loss: 0.8748 - val_acc: 0.5778\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8333 - acc: 0.6167 - val_loss: 0.8743 - val_acc: 0.5852\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8425 - acc: 0.6059 - val_loss: 0.8748 - val_acc: 0.5852\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8572 - acc: 0.6043 - val_loss: 0.8762 - val_acc: 0.5852\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8255 - acc: 0.6241 - val_loss: 0.8764 - val_acc: 0.5778\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8411 - acc: 0.6076 - val_loss: 0.8771 - val_acc: 0.5778\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8218 - acc: 0.6224 - val_loss: 0.8767 - val_acc: 0.5778\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8258 - acc: 0.6010 - val_loss: 0.8790 - val_acc: 0.5778\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8293 - acc: 0.6167 - val_loss: 0.8791 - val_acc: 0.5778\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8258 - acc: 0.6183 - val_loss: 0.8789 - val_acc: 0.5704\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8217 - acc: 0.6150 - val_loss: 0.8794 - val_acc: 0.5778\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8258 - acc: 0.6257 - val_loss: 0.8811 - val_acc: 0.5778\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8213 - acc: 0.6117 - val_loss: 0.8819 - val_acc: 0.5704\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8326 - acc: 0.6158 - val_loss: 0.8846 - val_acc: 0.5704\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8361 - acc: 0.6249 - val_loss: 0.8851 - val_acc: 0.5852\n",
      "Epoch 00021: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "1077/1077 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [476 264 337], val_loss: 0.851\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8435 - acc: 0.5944 - val_loss: 0.8879 - val_acc: 0.5852\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8252 - acc: 0.6092 - val_loss: 0.8890 - val_acc: 0.5778\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8307 - acc: 0.6167 - val_loss: 0.8889 - val_acc: 0.5704\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8251 - acc: 0.6224 - val_loss: 0.8899 - val_acc: 0.5704\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8181 - acc: 0.6109 - val_loss: 0.8884 - val_acc: 0.5704\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8137 - acc: 0.6191 - val_loss: 0.8891 - val_acc: 0.5778\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8239 - acc: 0.6035 - val_loss: 0.8893 - val_acc: 0.5704\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8060 - acc: 0.6290 - val_loss: 0.8902 - val_acc: 0.5630\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8215 - acc: 0.6117 - val_loss: 0.8920 - val_acc: 0.5630\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8203 - acc: 0.6216 - val_loss: 0.8942 - val_acc: 0.5630\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8107 - acc: 0.6282 - val_loss: 0.8953 - val_acc: 0.5407\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8020 - acc: 0.6125 - val_loss: 0.8957 - val_acc: 0.5630\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8202 - acc: 0.6183 - val_loss: 0.8971 - val_acc: 0.5481\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8224 - acc: 0.6125 - val_loss: 0.8981 - val_acc: 0.5333\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8030 - acc: 0.6331 - val_loss: 0.8981 - val_acc: 0.5556\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8188 - acc: 0.6224 - val_loss: 0.9012 - val_acc: 0.5333\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7986 - acc: 0.6249 - val_loss: 0.9013 - val_acc: 0.5481\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8137 - acc: 0.6224 - val_loss: 0.9004 - val_acc: 0.5556\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8100 - acc: 0.6208 - val_loss: 0.9004 - val_acc: 0.5556\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8076 - acc: 0.6257 - val_loss: 0.9005 - val_acc: 0.5481\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8053 - acc: 0.6175 - val_loss: 0.9017 - val_acc: 0.5407\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8108 - acc: 0.6158 - val_loss: 0.9028 - val_acc: 0.5407\n",
      "Epoch 00021: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "1078/1078 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [477 264 337], val_loss: 0.693\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8189 - acc: 0.6076 - val_loss: 0.9024 - val_acc: 0.5630\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8040 - acc: 0.6200 - val_loss: 0.9036 - val_acc: 0.5556\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7974 - acc: 0.6265 - val_loss: 0.9045 - val_acc: 0.5556\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8061 - acc: 0.6249 - val_loss: 0.9055 - val_acc: 0.5481\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7996 - acc: 0.6150 - val_loss: 0.9050 - val_acc: 0.5630\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7933 - acc: 0.6307 - val_loss: 0.9064 - val_acc: 0.5481\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8066 - acc: 0.6191 - val_loss: 0.9078 - val_acc: 0.5407\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7997 - acc: 0.6265 - val_loss: 0.9090 - val_acc: 0.5481\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7984 - acc: 0.6109 - val_loss: 0.9096 - val_acc: 0.5481\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8007 - acc: 0.6167 - val_loss: 0.9098 - val_acc: 0.5481\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8131 - acc: 0.6142 - val_loss: 0.9107 - val_acc: 0.5630\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8007 - acc: 0.6257 - val_loss: 0.9118 - val_acc: 0.5481\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8054 - acc: 0.6282 - val_loss: 0.9124 - val_acc: 0.5630\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8084 - acc: 0.6191 - val_loss: 0.9128 - val_acc: 0.5481\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7861 - acc: 0.6290 - val_loss: 0.9133 - val_acc: 0.5556\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7888 - acc: 0.6191 - val_loss: 0.9121 - val_acc: 0.5704\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7830 - acc: 0.6257 - val_loss: 0.9129 - val_acc: 0.5481\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8025 - acc: 0.6249 - val_loss: 0.9130 - val_acc: 0.5630\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7814 - acc: 0.6241 - val_loss: 0.9142 - val_acc: 0.5630\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8006 - acc: 0.6265 - val_loss: 0.9149 - val_acc: 0.5481\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8088 - acc: 0.6224 - val_loss: 0.9147 - val_acc: 0.5481\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7897 - acc: 0.6290 - val_loss: 0.9168 - val_acc: 0.5481\n",
      "Epoch 00021: early stopping\n",
      "269/269 [==============================] - 0s     \n",
      "1079/1079 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [477 264 338], val_loss: 0.723\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7906 - acc: 0.6216 - val_loss: 0.9189 - val_acc: 0.5481\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7892 - acc: 0.6397 - val_loss: 0.9185 - val_acc: 0.5481\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7746 - acc: 0.6348 - val_loss: 0.9199 - val_acc: 0.5556\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7963 - acc: 0.6200 - val_loss: 0.9219 - val_acc: 0.5481\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7960 - acc: 0.6208 - val_loss: 0.9222 - val_acc: 0.5481\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7908 - acc: 0.6348 - val_loss: 0.9227 - val_acc: 0.5630\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7907 - acc: 0.6200 - val_loss: 0.9218 - val_acc: 0.5481\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7740 - acc: 0.6216 - val_loss: 0.9232 - val_acc: 0.5481\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7831 - acc: 0.6265 - val_loss: 0.9236 - val_acc: 0.5407\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7926 - acc: 0.6224 - val_loss: 0.9251 - val_acc: 0.5407\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7888 - acc: 0.6282 - val_loss: 0.9244 - val_acc: 0.5407\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7845 - acc: 0.6175 - val_loss: 0.9251 - val_acc: 0.5407\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7867 - acc: 0.6373 - val_loss: 0.9258 - val_acc: 0.5407\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7687 - acc: 0.6307 - val_loss: 0.9266 - val_acc: 0.5407\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7769 - acc: 0.6397 - val_loss: 0.9260 - val_acc: 0.5481\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7851 - acc: 0.6364 - val_loss: 0.9272 - val_acc: 0.5407\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7956 - acc: 0.6216 - val_loss: 0.9275 - val_acc: 0.5481\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7840 - acc: 0.6232 - val_loss: 0.9272 - val_acc: 0.5481\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8026 - acc: 0.6298 - val_loss: 0.9275 - val_acc: 0.5556\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7947 - acc: 0.6249 - val_loss: 0.9278 - val_acc: 0.5407\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7802 - acc: 0.6232 - val_loss: 0.9291 - val_acc: 0.5407\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7792 - acc: 0.6208 - val_loss: 0.9304 - val_acc: 0.5556\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7833 - acc: 0.6373 - val_loss: 0.9303 - val_acc: 0.5407\n",
      "Epoch 00022: early stopping\n",
      "269/269 [==============================] - 0s     \n",
      "1079/1079 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [477 264 338], val_loss: 0.731\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7871 - acc: 0.6249 - val_loss: 0.9313 - val_acc: 0.5407\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7872 - acc: 0.6290 - val_loss: 0.9298 - val_acc: 0.5556\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7692 - acc: 0.6249 - val_loss: 0.9316 - val_acc: 0.5481\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7736 - acc: 0.6381 - val_loss: 0.9323 - val_acc: 0.5481\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7879 - acc: 0.6282 - val_loss: 0.9316 - val_acc: 0.5407\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7789 - acc: 0.6274 - val_loss: 0.9344 - val_acc: 0.5481\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7603 - acc: 0.6323 - val_loss: 0.9350 - val_acc: 0.5407\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7776 - acc: 0.6331 - val_loss: 0.9335 - val_acc: 0.5407\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7629 - acc: 0.6348 - val_loss: 0.9344 - val_acc: 0.5407\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7767 - acc: 0.6290 - val_loss: 0.9346 - val_acc: 0.5407\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7473 - acc: 0.6562 - val_loss: 0.9344 - val_acc: 0.5407\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7768 - acc: 0.6331 - val_loss: 0.9350 - val_acc: 0.5481\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7465 - acc: 0.6414 - val_loss: 0.9372 - val_acc: 0.5481\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7715 - acc: 0.6249 - val_loss: 0.9373 - val_acc: 0.5481\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7624 - acc: 0.6397 - val_loss: 0.9373 - val_acc: 0.5481\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7771 - acc: 0.6290 - val_loss: 0.9383 - val_acc: 0.5481\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7750 - acc: 0.6183 - val_loss: 0.9384 - val_acc: 0.5481\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7638 - acc: 0.6439 - val_loss: 0.9401 - val_acc: 0.5481\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7666 - acc: 0.6348 - val_loss: 0.9401 - val_acc: 0.5481\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7630 - acc: 0.6356 - val_loss: 0.9397 - val_acc: 0.5481\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7879 - acc: 0.6274 - val_loss: 0.9392 - val_acc: 0.5481\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7839 - acc: 0.6249 - val_loss: 0.9391 - val_acc: 0.5259\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.7591 - acc: 0.6257 - val_loss: 0.9404 - val_acc: 0.5556\n",
      "Epoch 00022: early stopping\n",
      "269/269 [==============================] - 0s     \n",
      "1079/1079 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [477 264 338], val_loss: 0.810\n"
     ]
    }
   ],
   "source": [
    "train_scores,scores, proba_test,proba_y = crossValidate2(model,X_scaled,y,fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNodeNum(X,y, sizes):\n",
    "    train_loss=[] \n",
    "    score_loss=[]\n",
    "    for s in sizes:\n",
    "        model=createModel([s,s])\n",
    "        train_scores,scores, firstNScores = crossValidate2(model,X,y,fold=5)\n",
    "        print(\"size:{} , val_loss_mean:{}\".format(s,np.mean(scores)))\n",
    "        train_loss.append(train_scores)\n",
    "        score_loss.append(scores)\n",
    "    return train_loss,score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.5632 - acc: 0.2679 - val_loss: 1.3519 - val_acc: 0.2519\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.4029 - acc: 0.2819 - val_loss: 1.2532 - val_acc: 0.2519\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.3606 - acc: 0.2844 - val_loss: 1.1842 - val_acc: 0.2519\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.2405 - acc: 0.3314 - val_loss: 1.1401 - val_acc: 0.2444\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.2314 - acc: 0.3388 - val_loss: 1.1116 - val_acc: 0.2667\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1942 - acc: 0.3430 - val_loss: 1.0962 - val_acc: 0.4222\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1634 - acc: 0.3693 - val_loss: 1.0847 - val_acc: 0.4370\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1571 - acc: 0.3603 - val_loss: 1.0777 - val_acc: 0.4296\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1386 - acc: 0.3759 - val_loss: 1.0722 - val_acc: 0.4667\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1433 - acc: 0.3677 - val_loss: 1.0688 - val_acc: 0.4444\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1200 - acc: 0.3932 - val_loss: 1.0667 - val_acc: 0.4148\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1112 - acc: 0.3916 - val_loss: 1.0654 - val_acc: 0.4296\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1030 - acc: 0.4097 - val_loss: 1.0649 - val_acc: 0.4296\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1109 - acc: 0.4031 - val_loss: 1.0633 - val_acc: 0.4444\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0987 - acc: 0.4089 - val_loss: 1.0629 - val_acc: 0.4444\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1033 - acc: 0.4073 - val_loss: 1.0625 - val_acc: 0.4370\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0904 - acc: 0.4147 - val_loss: 1.0622 - val_acc: 0.4370\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0868 - acc: 0.4196 - val_loss: 1.0619 - val_acc: 0.4296\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0891 - acc: 0.4089 - val_loss: 1.0618 - val_acc: 0.4296\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0877 - acc: 0.4089 - val_loss: 1.0617 - val_acc: 0.4296\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0656 - acc: 0.4361 - val_loss: 1.0615 - val_acc: 0.4296\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0766 - acc: 0.4246 - val_loss: 1.0615 - val_acc: 0.4296\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0748 - acc: 0.4419 - val_loss: 1.0608 - val_acc: 0.4296\n",
      "Epoch 24/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0663 - acc: 0.4345 - val_loss: 1.0606 - val_acc: 0.4296\n",
      "Epoch 25/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0587 - acc: 0.4650 - val_loss: 1.0602 - val_acc: 0.4296\n",
      "Epoch 26/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0711 - acc: 0.4402 - val_loss: 1.0603 - val_acc: 0.4296\n",
      "Epoch 27/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0763 - acc: 0.4336 - val_loss: 1.0605 - val_acc: 0.4296\n",
      "Epoch 28/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0538 - acc: 0.4551 - val_loss: 1.0603 - val_acc: 0.4296\n",
      "Epoch 29/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0704 - acc: 0.4460 - val_loss: 1.0601 - val_acc: 0.4296\n",
      "Epoch 30/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0604 - acc: 0.4378 - val_loss: 1.0605 - val_acc: 0.4296\n",
      "Epoch 31/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0600 - acc: 0.4575 - val_loss: 1.0601 - val_acc: 0.4296\n",
      "Epoch 32/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0477 - acc: 0.4592 - val_loss: 1.0596 - val_acc: 0.4296\n",
      "Epoch 33/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0591 - acc: 0.4559 - val_loss: 1.0589 - val_acc: 0.4296\n",
      "Epoch 34/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0676 - acc: 0.4518 - val_loss: 1.0585 - val_acc: 0.4296\n",
      "Epoch 35/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0537 - acc: 0.4559 - val_loss: 1.0579 - val_acc: 0.4296\n",
      "Epoch 36/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0515 - acc: 0.4567 - val_loss: 1.0578 - val_acc: 0.4296\n",
      "Epoch 37/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0563 - acc: 0.4707 - val_loss: 1.0574 - val_acc: 0.4296\n",
      "Epoch 38/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0631 - acc: 0.4526 - val_loss: 1.0573 - val_acc: 0.4296\n",
      "Epoch 39/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0541 - acc: 0.4542 - val_loss: 1.0573 - val_acc: 0.4296\n",
      "Epoch 40/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0539 - acc: 0.4468 - val_loss: 1.0568 - val_acc: 0.4296\n",
      "Epoch 41/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0552 - acc: 0.4691 - val_loss: 1.0564 - val_acc: 0.4296\n",
      "Epoch 42/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0494 - acc: 0.4584 - val_loss: 1.0557 - val_acc: 0.4370\n",
      "Epoch 43/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0493 - acc: 0.4674 - val_loss: 1.0548 - val_acc: 0.4519\n",
      "Epoch 44/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0547 - acc: 0.4608 - val_loss: 1.0542 - val_acc: 0.4519\n",
      "Epoch 45/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0478 - acc: 0.4674 - val_loss: 1.0540 - val_acc: 0.4519\n",
      "Epoch 46/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0447 - acc: 0.4757 - val_loss: 1.0533 - val_acc: 0.4444\n",
      "Epoch 47/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0507 - acc: 0.4740 - val_loss: 1.0522 - val_acc: 0.4593\n",
      "Epoch 48/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0454 - acc: 0.4716 - val_loss: 1.0515 - val_acc: 0.4667\n",
      "Epoch 49/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0491 - acc: 0.4666 - val_loss: 1.0514 - val_acc: 0.4667\n",
      "Epoch 50/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0459 - acc: 0.4806 - val_loss: 1.0506 - val_acc: 0.4667\n",
      "Epoch 51/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0344 - acc: 0.4872 - val_loss: 1.0494 - val_acc: 0.4667\n",
      "Epoch 52/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0386 - acc: 0.4798 - val_loss: 1.0494 - val_acc: 0.4667\n",
      "Epoch 53/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0391 - acc: 0.4839 - val_loss: 1.0486 - val_acc: 0.4667\n",
      "Epoch 54/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0430 - acc: 0.4749 - val_loss: 1.0473 - val_acc: 0.4667\n",
      "Epoch 55/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0370 - acc: 0.4856 - val_loss: 1.0467 - val_acc: 0.4889\n",
      "Epoch 56/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0364 - acc: 0.4823 - val_loss: 1.0463 - val_acc: 0.4741\n",
      "Epoch 57/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0430 - acc: 0.4707 - val_loss: 1.0462 - val_acc: 0.4815\n",
      "Epoch 58/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0397 - acc: 0.4806 - val_loss: 1.0455 - val_acc: 0.4667\n",
      "Epoch 59/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0279 - acc: 0.4815 - val_loss: 1.0446 - val_acc: 0.4815\n",
      "Epoch 60/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0282 - acc: 0.4955 - val_loss: 1.0438 - val_acc: 0.4815\n",
      "Epoch 61/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0476 - acc: 0.4608 - val_loss: 1.0432 - val_acc: 0.4889\n",
      "Epoch 62/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0407 - acc: 0.4839 - val_loss: 1.0423 - val_acc: 0.4815\n",
      "Epoch 63/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0364 - acc: 0.4716 - val_loss: 1.0414 - val_acc: 0.4815\n",
      "Epoch 64/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0363 - acc: 0.4765 - val_loss: 1.0412 - val_acc: 0.4815\n",
      "Epoch 65/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0436 - acc: 0.4897 - val_loss: 1.0414 - val_acc: 0.4889\n",
      "Epoch 66/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0362 - acc: 0.4889 - val_loss: 1.0405 - val_acc: 0.4963\n",
      "Epoch 67/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0312 - acc: 0.4880 - val_loss: 1.0401 - val_acc: 0.5037\n",
      "Epoch 68/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0240 - acc: 0.4831 - val_loss: 1.0396 - val_acc: 0.4889\n",
      "Epoch 69/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0380 - acc: 0.4790 - val_loss: 1.0392 - val_acc: 0.4889\n",
      "Epoch 70/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0310 - acc: 0.4880 - val_loss: 1.0387 - val_acc: 0.4889\n",
      "Epoch 71/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0522 - acc: 0.4765 - val_loss: 1.0394 - val_acc: 0.4815\n",
      "Epoch 72/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0371 - acc: 0.4749 - val_loss: 1.0391 - val_acc: 0.4889\n",
      "Epoch 73/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0291 - acc: 0.4872 - val_loss: 1.0384 - val_acc: 0.5037\n",
      "Epoch 74/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0364 - acc: 0.4757 - val_loss: 1.0370 - val_acc: 0.5259\n",
      "Epoch 75/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0286 - acc: 0.4839 - val_loss: 1.0365 - val_acc: 0.5111\n",
      "Epoch 76/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0247 - acc: 0.5004 - val_loss: 1.0345 - val_acc: 0.5259\n",
      "Epoch 77/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0296 - acc: 0.4889 - val_loss: 1.0346 - val_acc: 0.5259\n",
      "Epoch 78/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0331 - acc: 0.4856 - val_loss: 1.0345 - val_acc: 0.5259\n",
      "Epoch 79/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0231 - acc: 0.5136 - val_loss: 1.0338 - val_acc: 0.5259\n",
      "Epoch 80/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0286 - acc: 0.4963 - val_loss: 1.0341 - val_acc: 0.5333\n",
      "Epoch 81/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0323 - acc: 0.4839 - val_loss: 1.0343 - val_acc: 0.5333\n",
      "Epoch 82/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0254 - acc: 0.4930 - val_loss: 1.0338 - val_acc: 0.5333\n",
      "Epoch 83/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0322 - acc: 0.4880 - val_loss: 1.0346 - val_acc: 0.5407\n",
      "Epoch 84/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0152 - acc: 0.5169 - val_loss: 1.0326 - val_acc: 0.5333\n",
      "Epoch 85/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0171 - acc: 0.4922 - val_loss: 1.0324 - val_acc: 0.5333\n",
      "Epoch 86/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0203 - acc: 0.5045 - val_loss: 1.0313 - val_acc: 0.5259\n",
      "Epoch 87/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0178 - acc: 0.4971 - val_loss: 1.0312 - val_acc: 0.5259\n",
      "Epoch 88/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0320 - acc: 0.5062 - val_loss: 1.0315 - val_acc: 0.5333\n",
      "Epoch 89/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0246 - acc: 0.4922 - val_loss: 1.0323 - val_acc: 0.5407\n",
      "Epoch 90/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0355 - acc: 0.4847 - val_loss: 1.0330 - val_acc: 0.5407\n",
      "Epoch 91/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0128 - acc: 0.5037 - val_loss: 1.0318 - val_acc: 0.5333\n",
      "Epoch 92/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0165 - acc: 0.5169 - val_loss: 1.0316 - val_acc: 0.5259\n",
      "Epoch 93/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0270 - acc: 0.4979 - val_loss: 1.0309 - val_acc: 0.5259\n",
      "Epoch 94/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0121 - acc: 0.5062 - val_loss: 1.0313 - val_acc: 0.5185\n",
      "Epoch 95/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0105 - acc: 0.5120 - val_loss: 1.0304 - val_acc: 0.5185\n",
      "Epoch 96/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0268 - acc: 0.4988 - val_loss: 1.0309 - val_acc: 0.5185\n",
      "Epoch 97/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0058 - acc: 0.5177 - val_loss: 1.0301 - val_acc: 0.5037\n",
      "Epoch 98/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0238 - acc: 0.5087 - val_loss: 1.0297 - val_acc: 0.5037\n",
      "Epoch 99/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0193 - acc: 0.5144 - val_loss: 1.0297 - val_acc: 0.5037\n",
      "Epoch 100/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0169 - acc: 0.5128 - val_loss: 1.0297 - val_acc: 0.5037\n",
      "Epoch 101/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0195 - acc: 0.5087 - val_loss: 1.0305 - val_acc: 0.5185\n",
      "Epoch 102/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0180 - acc: 0.4955 - val_loss: 1.0302 - val_acc: 0.5037\n",
      "Epoch 103/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0226 - acc: 0.4988 - val_loss: 1.0299 - val_acc: 0.5037\n",
      "Epoch 104/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0153 - acc: 0.5161 - val_loss: 1.0296 - val_acc: 0.5037\n",
      "Epoch 105/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0204 - acc: 0.5087 - val_loss: 1.0302 - val_acc: 0.5037\n",
      "Epoch 106/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0082 - acc: 0.5326 - val_loss: 1.0292 - val_acc: 0.5037\n",
      "Epoch 107/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0133 - acc: 0.5103 - val_loss: 1.0293 - val_acc: 0.5037\n",
      "Epoch 108/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0302 - acc: 0.4979 - val_loss: 1.0293 - val_acc: 0.5111\n",
      "Epoch 109/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0157 - acc: 0.5136 - val_loss: 1.0288 - val_acc: 0.5037\n",
      "Epoch 110/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0153 - acc: 0.5087 - val_loss: 1.0280 - val_acc: 0.5037\n",
      "Epoch 111/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0063 - acc: 0.5235 - val_loss: 1.0277 - val_acc: 0.5037\n",
      "Epoch 112/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0250 - acc: 0.5021 - val_loss: 1.0277 - val_acc: 0.5111\n",
      "Epoch 113/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0110 - acc: 0.5227 - val_loss: 1.0283 - val_acc: 0.5111\n",
      "Epoch 114/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0153 - acc: 0.5210 - val_loss: 1.0285 - val_acc: 0.5111\n",
      "Epoch 115/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0192 - acc: 0.5120 - val_loss: 1.0286 - val_acc: 0.5111\n",
      "Epoch 116/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0128 - acc: 0.5185 - val_loss: 1.0282 - val_acc: 0.5111\n",
      "Epoch 117/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0123 - acc: 0.5120 - val_loss: 1.0281 - val_acc: 0.5111\n",
      "Epoch 118/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0100 - acc: 0.5185 - val_loss: 1.0286 - val_acc: 0.5185\n",
      "Epoch 119/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0302 - acc: 0.5103 - val_loss: 1.0283 - val_acc: 0.5185\n",
      "Epoch 120/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0112 - acc: 0.5169 - val_loss: 1.0280 - val_acc: 0.5185\n",
      "Epoch 121/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0019 - acc: 0.5218 - val_loss: 1.0269 - val_acc: 0.5037\n",
      "Epoch 122/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0117 - acc: 0.5111 - val_loss: 1.0266 - val_acc: 0.5037\n",
      "Epoch 123/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0125 - acc: 0.5276 - val_loss: 1.0267 - val_acc: 0.5037\n",
      "Epoch 124/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0122 - acc: 0.5177 - val_loss: 1.0262 - val_acc: 0.5037\n",
      "Epoch 125/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0153 - acc: 0.5161 - val_loss: 1.0263 - val_acc: 0.5037\n",
      "Epoch 126/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0155 - acc: 0.5070 - val_loss: 1.0262 - val_acc: 0.5111\n",
      "Epoch 127/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0077 - acc: 0.5235 - val_loss: 1.0259 - val_acc: 0.5185\n",
      "Epoch 128/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0068 - acc: 0.5227 - val_loss: 1.0259 - val_acc: 0.5259\n",
      "Epoch 129/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0119 - acc: 0.5169 - val_loss: 1.0249 - val_acc: 0.5185\n",
      "Epoch 130/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0162 - acc: 0.5260 - val_loss: 1.0248 - val_acc: 0.5259\n",
      "Epoch 131/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0161 - acc: 0.5111 - val_loss: 1.0255 - val_acc: 0.5111\n",
      "Epoch 132/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9971 - acc: 0.5317 - val_loss: 1.0243 - val_acc: 0.5111\n",
      "Epoch 133/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0123 - acc: 0.5218 - val_loss: 1.0243 - val_acc: 0.5111\n",
      "Epoch 134/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9921 - acc: 0.5392 - val_loss: 1.0243 - val_acc: 0.5259\n",
      "Epoch 135/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0108 - acc: 0.5251 - val_loss: 1.0246 - val_acc: 0.5185\n",
      "Epoch 136/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0154 - acc: 0.5120 - val_loss: 1.0246 - val_acc: 0.5185\n",
      "Epoch 137/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0214 - acc: 0.5169 - val_loss: 1.0240 - val_acc: 0.5185\n",
      "Epoch 138/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0094 - acc: 0.5111 - val_loss: 1.0248 - val_acc: 0.5185\n",
      "Epoch 139/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0063 - acc: 0.5350 - val_loss: 1.0246 - val_acc: 0.5259\n",
      "Epoch 140/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0140 - acc: 0.5260 - val_loss: 1.0244 - val_acc: 0.5185\n",
      "Epoch 141/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0060 - acc: 0.5177 - val_loss: 1.0242 - val_acc: 0.5185\n",
      "Epoch 142/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0138 - acc: 0.5070 - val_loss: 1.0244 - val_acc: 0.5185\n",
      "Epoch 143/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0120 - acc: 0.5218 - val_loss: 1.0241 - val_acc: 0.5185\n",
      "Epoch 144/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0040 - acc: 0.5260 - val_loss: 1.0237 - val_acc: 0.5185\n",
      "Epoch 145/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0048 - acc: 0.5326 - val_loss: 1.0239 - val_acc: 0.5185\n",
      "Epoch 146/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9981 - acc: 0.5334 - val_loss: 1.0241 - val_acc: 0.5185\n",
      "Epoch 147/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0045 - acc: 0.5235 - val_loss: 1.0239 - val_acc: 0.5185\n",
      "Epoch 148/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0036 - acc: 0.5218 - val_loss: 1.0240 - val_acc: 0.5185\n",
      "Epoch 149/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9990 - acc: 0.5153 - val_loss: 1.0235 - val_acc: 0.5185\n",
      "Epoch 150/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0108 - acc: 0.5111 - val_loss: 1.0236 - val_acc: 0.5259\n",
      "Epoch 151/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0123 - acc: 0.5177 - val_loss: 1.0235 - val_acc: 0.5185\n",
      "Epoch 152/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9974 - acc: 0.5293 - val_loss: 1.0230 - val_acc: 0.5259\n",
      "Epoch 153/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9896 - acc: 0.5350 - val_loss: 1.0222 - val_acc: 0.5037\n",
      "Epoch 154/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0062 - acc: 0.5243 - val_loss: 1.0223 - val_acc: 0.5111\n",
      "Epoch 155/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0304 - acc: 0.5078 - val_loss: 1.0230 - val_acc: 0.5259\n",
      "Epoch 156/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0109 - acc: 0.5284 - val_loss: 1.0231 - val_acc: 0.5259\n",
      "Epoch 157/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9975 - acc: 0.5268 - val_loss: 1.0234 - val_acc: 0.5259\n",
      "Epoch 158/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9978 - acc: 0.5400 - val_loss: 1.0225 - val_acc: 0.5111\n",
      "Epoch 159/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9942 - acc: 0.5243 - val_loss: 1.0218 - val_acc: 0.5185\n",
      "Epoch 160/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0022 - acc: 0.5284 - val_loss: 1.0221 - val_acc: 0.5111\n",
      "Epoch 161/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0107 - acc: 0.5260 - val_loss: 1.0219 - val_acc: 0.5111\n",
      "Epoch 162/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0072 - acc: 0.5185 - val_loss: 1.0220 - val_acc: 0.5185\n",
      "Epoch 163/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0085 - acc: 0.5309 - val_loss: 1.0218 - val_acc: 0.5111\n",
      "Epoch 164/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0033 - acc: 0.5202 - val_loss: 1.0217 - val_acc: 0.5185\n",
      "Epoch 165/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9950 - acc: 0.5383 - val_loss: 1.0217 - val_acc: 0.5037\n",
      "Epoch 166/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9972 - acc: 0.5334 - val_loss: 1.0216 - val_acc: 0.5111\n",
      "Epoch 167/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0068 - acc: 0.5251 - val_loss: 1.0216 - val_acc: 0.5037\n",
      "Epoch 168/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0015 - acc: 0.5268 - val_loss: 1.0221 - val_acc: 0.5037\n",
      "Epoch 169/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0022 - acc: 0.5251 - val_loss: 1.0218 - val_acc: 0.5037\n",
      "Epoch 170/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9999 - acc: 0.5260 - val_loss: 1.0215 - val_acc: 0.5037\n",
      "Epoch 171/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0020 - acc: 0.5293 - val_loss: 1.0216 - val_acc: 0.5037\n",
      "Epoch 172/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9970 - acc: 0.5400 - val_loss: 1.0210 - val_acc: 0.5037\n",
      "Epoch 173/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9979 - acc: 0.5350 - val_loss: 1.0206 - val_acc: 0.5037\n",
      "Epoch 174/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0042 - acc: 0.5293 - val_loss: 1.0207 - val_acc: 0.5037\n",
      "Epoch 175/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0059 - acc: 0.5260 - val_loss: 1.0203 - val_acc: 0.5111\n",
      "Epoch 176/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0068 - acc: 0.5218 - val_loss: 1.0200 - val_acc: 0.5111\n",
      "Epoch 177/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0067 - acc: 0.5218 - val_loss: 1.0202 - val_acc: 0.5111\n",
      "Epoch 178/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9977 - acc: 0.5276 - val_loss: 1.0196 - val_acc: 0.5111\n",
      "Epoch 179/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0107 - acc: 0.5161 - val_loss: 1.0191 - val_acc: 0.5111\n",
      "Epoch 180/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0128 - acc: 0.5153 - val_loss: 1.0196 - val_acc: 0.5185\n",
      "Epoch 181/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9968 - acc: 0.5350 - val_loss: 1.0200 - val_acc: 0.5185\n",
      "Epoch 182/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0058 - acc: 0.5169 - val_loss: 1.0202 - val_acc: 0.5111\n",
      "Epoch 183/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0179 - acc: 0.5218 - val_loss: 1.0206 - val_acc: 0.5111\n",
      "Epoch 184/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9989 - acc: 0.5383 - val_loss: 1.0197 - val_acc: 0.5185\n",
      "Epoch 185/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9933 - acc: 0.5334 - val_loss: 1.0197 - val_acc: 0.5111\n",
      "Epoch 186/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0113 - acc: 0.5202 - val_loss: 1.0196 - val_acc: 0.5185\n",
      "Epoch 187/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9991 - acc: 0.5268 - val_loss: 1.0200 - val_acc: 0.5185\n",
      "Epoch 188/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0007 - acc: 0.5235 - val_loss: 1.0201 - val_acc: 0.5111\n",
      "Epoch 189/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0102 - acc: 0.5251 - val_loss: 1.0201 - val_acc: 0.5111\n",
      "Epoch 190/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0126 - acc: 0.5235 - val_loss: 1.0205 - val_acc: 0.5111\n",
      "Epoch 191/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0114 - acc: 0.5235 - val_loss: 1.0201 - val_acc: 0.5185\n",
      "Epoch 192/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0053 - acc: 0.5276 - val_loss: 1.0192 - val_acc: 0.5185\n",
      "Epoch 193/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0020 - acc: 0.5276 - val_loss: 1.0195 - val_acc: 0.5185\n",
      "Epoch 194/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0041 - acc: 0.5260 - val_loss: 1.0194 - val_acc: 0.5185\n",
      "Epoch 195/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9979 - acc: 0.5350 - val_loss: 1.0192 - val_acc: 0.5111\n",
      "Epoch 196/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9961 - acc: 0.5284 - val_loss: 1.0187 - val_acc: 0.5185\n",
      "Epoch 197/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9955 - acc: 0.5243 - val_loss: 1.0187 - val_acc: 0.5111\n",
      "Epoch 198/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0012 - acc: 0.5301 - val_loss: 1.0188 - val_acc: 0.5111\n",
      "Epoch 199/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9989 - acc: 0.5293 - val_loss: 1.0185 - val_acc: 0.5185\n",
      "Epoch 200/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0078 - acc: 0.5218 - val_loss: 1.0186 - val_acc: 0.5111\n",
      "Epoch 201/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9995 - acc: 0.5416 - val_loss: 1.0178 - val_acc: 0.5111\n",
      "Epoch 202/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9974 - acc: 0.5317 - val_loss: 1.0174 - val_acc: 0.5111\n",
      "Epoch 203/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0024 - acc: 0.5284 - val_loss: 1.0178 - val_acc: 0.5111\n",
      "Epoch 204/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0081 - acc: 0.5260 - val_loss: 1.0175 - val_acc: 0.5111\n",
      "Epoch 205/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0053 - acc: 0.5317 - val_loss: 1.0176 - val_acc: 0.5185\n",
      "Epoch 206/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9991 - acc: 0.5235 - val_loss: 1.0174 - val_acc: 0.5185\n",
      "Epoch 207/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9977 - acc: 0.5359 - val_loss: 1.0175 - val_acc: 0.5259\n",
      "Epoch 208/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9978 - acc: 0.5276 - val_loss: 1.0174 - val_acc: 0.5185\n",
      "Epoch 209/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0080 - acc: 0.5161 - val_loss: 1.0178 - val_acc: 0.5185\n",
      "Epoch 210/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0010 - acc: 0.5334 - val_loss: 1.0181 - val_acc: 0.5185\n",
      "Epoch 211/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9870 - acc: 0.5433 - val_loss: 1.0182 - val_acc: 0.5111\n",
      "Epoch 212/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0009 - acc: 0.5235 - val_loss: 1.0185 - val_acc: 0.5185\n",
      "Epoch 213/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0004 - acc: 0.5342 - val_loss: 1.0179 - val_acc: 0.5185\n",
      "Epoch 214/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0052 - acc: 0.5218 - val_loss: 1.0176 - val_acc: 0.5333\n",
      "Epoch 215/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9970 - acc: 0.5326 - val_loss: 1.0170 - val_acc: 0.5185\n",
      "Epoch 216/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0026 - acc: 0.5276 - val_loss: 1.0169 - val_acc: 0.5259\n",
      "Epoch 217/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0213 - acc: 0.5095 - val_loss: 1.0175 - val_acc: 0.5259\n",
      "Epoch 218/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9869 - acc: 0.5400 - val_loss: 1.0173 - val_acc: 0.5259\n",
      "Epoch 219/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9950 - acc: 0.5392 - val_loss: 1.0174 - val_acc: 0.5111\n",
      "Epoch 220/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9983 - acc: 0.5251 - val_loss: 1.0179 - val_acc: 0.5259\n",
      "Epoch 221/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0001 - acc: 0.5276 - val_loss: 1.0177 - val_acc: 0.5259\n",
      "Epoch 222/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0096 - acc: 0.5177 - val_loss: 1.0185 - val_acc: 0.5185\n",
      "Epoch 223/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0075 - acc: 0.5210 - val_loss: 1.0186 - val_acc: 0.5259\n",
      "Epoch 224/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9965 - acc: 0.5425 - val_loss: 1.0180 - val_acc: 0.5259\n",
      "Epoch 225/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9923 - acc: 0.5210 - val_loss: 1.0183 - val_acc: 0.5185\n",
      "Epoch 226/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9964 - acc: 0.5317 - val_loss: 1.0181 - val_acc: 0.5185\n",
      "Epoch 227/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9896 - acc: 0.5425 - val_loss: 1.0173 - val_acc: 0.5185\n",
      "Epoch 228/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9948 - acc: 0.5449 - val_loss: 1.0167 - val_acc: 0.5185\n",
      "Epoch 229/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9892 - acc: 0.5309 - val_loss: 1.0163 - val_acc: 0.5185\n",
      "Epoch 230/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9940 - acc: 0.5433 - val_loss: 1.0160 - val_acc: 0.5185\n",
      "Epoch 231/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0025 - acc: 0.5350 - val_loss: 1.0160 - val_acc: 0.5185\n",
      "Epoch 232/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9870 - acc: 0.5408 - val_loss: 1.0162 - val_acc: 0.5185\n",
      "Epoch 233/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0010 - acc: 0.5284 - val_loss: 1.0164 - val_acc: 0.5185\n",
      "Epoch 234/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9810 - acc: 0.5449 - val_loss: 1.0156 - val_acc: 0.5185\n",
      "Epoch 235/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9917 - acc: 0.5334 - val_loss: 1.0154 - val_acc: 0.5185\n",
      "Epoch 236/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9873 - acc: 0.5317 - val_loss: 1.0147 - val_acc: 0.5185\n",
      "Epoch 237/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9921 - acc: 0.5441 - val_loss: 1.0147 - val_acc: 0.5185\n",
      "Epoch 238/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9957 - acc: 0.5400 - val_loss: 1.0150 - val_acc: 0.5185\n",
      "Epoch 239/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9819 - acc: 0.5433 - val_loss: 1.0151 - val_acc: 0.5185\n",
      "Epoch 240/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9921 - acc: 0.5334 - val_loss: 1.0149 - val_acc: 0.5185\n",
      "Epoch 241/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9885 - acc: 0.5375 - val_loss: 1.0149 - val_acc: 0.5185\n",
      "Epoch 242/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9896 - acc: 0.5400 - val_loss: 1.0146 - val_acc: 0.5259\n",
      "Epoch 243/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9965 - acc: 0.5243 - val_loss: 1.0149 - val_acc: 0.5185\n",
      "Epoch 244/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9992 - acc: 0.5309 - val_loss: 1.0155 - val_acc: 0.5185\n",
      "Epoch 245/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9993 - acc: 0.5392 - val_loss: 1.0147 - val_acc: 0.5185\n",
      "Epoch 246/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9912 - acc: 0.5359 - val_loss: 1.0141 - val_acc: 0.5185\n",
      "Epoch 247/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0000 - acc: 0.5342 - val_loss: 1.0142 - val_acc: 0.5185\n",
      "Epoch 248/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9868 - acc: 0.5416 - val_loss: 1.0141 - val_acc: 0.5185\n",
      "Epoch 249/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9928 - acc: 0.5392 - val_loss: 1.0143 - val_acc: 0.5185\n",
      "Epoch 250/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0210 - acc: 0.5120 - val_loss: 1.0151 - val_acc: 0.5259\n",
      "Epoch 251/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9890 - acc: 0.5474 - val_loss: 1.0151 - val_acc: 0.5259\n",
      "Epoch 252/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9992 - acc: 0.5260 - val_loss: 1.0153 - val_acc: 0.5185\n",
      "Epoch 253/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9816 - acc: 0.5482 - val_loss: 1.0146 - val_acc: 0.5111\n",
      "Epoch 254/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9874 - acc: 0.5507 - val_loss: 1.0147 - val_acc: 0.5185\n",
      "Epoch 255/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9971 - acc: 0.5284 - val_loss: 1.0143 - val_acc: 0.5037\n",
      "Epoch 256/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0004 - acc: 0.5284 - val_loss: 1.0147 - val_acc: 0.5185\n",
      "Epoch 257/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9972 - acc: 0.5383 - val_loss: 1.0146 - val_acc: 0.5185\n",
      "Epoch 258/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9934 - acc: 0.5293 - val_loss: 1.0141 - val_acc: 0.5185\n",
      "Epoch 259/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9864 - acc: 0.5416 - val_loss: 1.0145 - val_acc: 0.5259\n",
      "Epoch 260/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9773 - acc: 0.5507 - val_loss: 1.0137 - val_acc: 0.5185\n",
      "Epoch 261/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9943 - acc: 0.5408 - val_loss: 1.0138 - val_acc: 0.5259\n",
      "Epoch 262/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9952 - acc: 0.5342 - val_loss: 1.0134 - val_acc: 0.5259\n",
      "Epoch 263/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9966 - acc: 0.5334 - val_loss: 1.0128 - val_acc: 0.5259\n",
      "Epoch 264/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0004 - acc: 0.5326 - val_loss: 1.0126 - val_acc: 0.5259\n",
      "Epoch 265/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9919 - acc: 0.5359 - val_loss: 1.0124 - val_acc: 0.5259\n",
      "Epoch 266/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9861 - acc: 0.5540 - val_loss: 1.0130 - val_acc: 0.5259\n",
      "Epoch 267/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9893 - acc: 0.5491 - val_loss: 1.0130 - val_acc: 0.5259\n",
      "Epoch 268/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9907 - acc: 0.5301 - val_loss: 1.0128 - val_acc: 0.5259\n",
      "Epoch 269/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9941 - acc: 0.5433 - val_loss: 1.0126 - val_acc: 0.5259\n",
      "Epoch 270/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9813 - acc: 0.5556 - val_loss: 1.0120 - val_acc: 0.5259\n",
      "Epoch 271/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0016 - acc: 0.5260 - val_loss: 1.0122 - val_acc: 0.5259\n",
      "Epoch 272/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9915 - acc: 0.5392 - val_loss: 1.0116 - val_acc: 0.5333\n",
      "Epoch 273/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9893 - acc: 0.5466 - val_loss: 1.0118 - val_acc: 0.5333\n",
      "Epoch 274/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9967 - acc: 0.5309 - val_loss: 1.0122 - val_acc: 0.5333\n",
      "Epoch 275/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0047 - acc: 0.5334 - val_loss: 1.0124 - val_acc: 0.5259\n",
      "Epoch 276/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9973 - acc: 0.5227 - val_loss: 1.0128 - val_acc: 0.5259\n",
      "Epoch 277/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9977 - acc: 0.5243 - val_loss: 1.0121 - val_acc: 0.5333\n",
      "Epoch 278/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0037 - acc: 0.5251 - val_loss: 1.0120 - val_acc: 0.5259\n",
      "Epoch 279/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9817 - acc: 0.5408 - val_loss: 1.0123 - val_acc: 0.5259\n",
      "Epoch 280/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9791 - acc: 0.5523 - val_loss: 1.0122 - val_acc: 0.5259\n",
      "Epoch 281/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0011 - acc: 0.5400 - val_loss: 1.0123 - val_acc: 0.5259\n",
      "Epoch 282/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9843 - acc: 0.5441 - val_loss: 1.0126 - val_acc: 0.5259\n",
      "Epoch 283/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9953 - acc: 0.5268 - val_loss: 1.0129 - val_acc: 0.5259\n",
      "Epoch 284/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9901 - acc: 0.5408 - val_loss: 1.0130 - val_acc: 0.5259\n",
      "Epoch 285/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9776 - acc: 0.5466 - val_loss: 1.0119 - val_acc: 0.5259\n",
      "Epoch 286/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9953 - acc: 0.5466 - val_loss: 1.0116 - val_acc: 0.5259\n",
      "Epoch 287/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9951 - acc: 0.5367 - val_loss: 1.0113 - val_acc: 0.5259\n",
      "Epoch 288/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9844 - acc: 0.5556 - val_loss: 1.0112 - val_acc: 0.5259\n",
      "Epoch 289/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9886 - acc: 0.5491 - val_loss: 1.0108 - val_acc: 0.5259\n",
      "Epoch 290/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0007 - acc: 0.5359 - val_loss: 1.0114 - val_acc: 0.5259\n",
      "Epoch 291/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9824 - acc: 0.5491 - val_loss: 1.0111 - val_acc: 0.5259\n",
      "Epoch 292/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9891 - acc: 0.5548 - val_loss: 1.0114 - val_acc: 0.5259\n",
      "Epoch 293/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0005 - acc: 0.5359 - val_loss: 1.0104 - val_acc: 0.5333\n",
      "Epoch 294/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9951 - acc: 0.5466 - val_loss: 1.0104 - val_acc: 0.5333\n",
      "Epoch 295/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9930 - acc: 0.5334 - val_loss: 1.0099 - val_acc: 0.5333\n",
      "Epoch 296/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0002 - acc: 0.5202 - val_loss: 1.0103 - val_acc: 0.5333\n",
      "Epoch 297/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9817 - acc: 0.5556 - val_loss: 1.0106 - val_acc: 0.5333\n",
      "Epoch 298/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9982 - acc: 0.5227 - val_loss: 1.0107 - val_acc: 0.5333\n",
      "Epoch 299/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9973 - acc: 0.5317 - val_loss: 1.0102 - val_acc: 0.5333\n",
      "Epoch 300/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9888 - acc: 0.5449 - val_loss: 1.0102 - val_acc: 0.5333\n",
      "Epoch 301/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9794 - acc: 0.5474 - val_loss: 1.0104 - val_acc: 0.5333\n",
      "Epoch 302/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0009 - acc: 0.5251 - val_loss: 1.0109 - val_acc: 0.5333\n",
      "Epoch 303/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9806 - acc: 0.5392 - val_loss: 1.0108 - val_acc: 0.5333\n",
      "Epoch 304/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9956 - acc: 0.5293 - val_loss: 1.0105 - val_acc: 0.5333\n",
      "Epoch 305/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9872 - acc: 0.5441 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 306/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9971 - acc: 0.5342 - val_loss: 1.0099 - val_acc: 0.5333\n",
      "Epoch 307/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9955 - acc: 0.5260 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 308/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9939 - acc: 0.5326 - val_loss: 1.0103 - val_acc: 0.5333\n",
      "Epoch 309/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9832 - acc: 0.5507 - val_loss: 1.0101 - val_acc: 0.5333\n",
      "Epoch 310/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9975 - acc: 0.5458 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 311/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0013 - acc: 0.5342 - val_loss: 1.0104 - val_acc: 0.5333\n",
      "Epoch 312/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9824 - acc: 0.5326 - val_loss: 1.0101 - val_acc: 0.5333\n",
      "Epoch 313/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9805 - acc: 0.5507 - val_loss: 1.0105 - val_acc: 0.5333\n",
      "Epoch 314/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9871 - acc: 0.5334 - val_loss: 1.0102 - val_acc: 0.5333\n",
      "Epoch 315/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9997 - acc: 0.5317 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 316/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9853 - acc: 0.5375 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 00315: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "1077/1077 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [476 264 337], val_loss: 0.934\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9928 - acc: 0.5260 - val_loss: 1.0097 - val_acc: 0.5333\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9951 - acc: 0.5317 - val_loss: 1.0098 - val_acc: 0.5407\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9773 - acc: 0.5523 - val_loss: 1.0091 - val_acc: 0.5333\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9815 - acc: 0.5556 - val_loss: 1.0096 - val_acc: 0.5333\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9820 - acc: 0.5392 - val_loss: 1.0097 - val_acc: 0.5333\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9946 - acc: 0.5499 - val_loss: 1.0094 - val_acc: 0.5333\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9932 - acc: 0.5260 - val_loss: 1.0096 - val_acc: 0.5333\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9918 - acc: 0.5317 - val_loss: 1.0099 - val_acc: 0.5333\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9983 - acc: 0.5284 - val_loss: 1.0097 - val_acc: 0.5333\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9946 - acc: 0.5293 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9909 - acc: 0.5317 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9901 - acc: 0.5367 - val_loss: 1.0095 - val_acc: 0.5333\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9924 - acc: 0.5293 - val_loss: 1.0094 - val_acc: 0.5333\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9918 - acc: 0.5350 - val_loss: 1.0098 - val_acc: 0.5333\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9883 - acc: 0.5482 - val_loss: 1.0099 - val_acc: 0.5333\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9814 - acc: 0.5400 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0053 - acc: 0.5235 - val_loss: 1.0103 - val_acc: 0.5333\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9949 - acc: 0.5301 - val_loss: 1.0097 - val_acc: 0.5333\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0079 - acc: 0.5317 - val_loss: 1.0098 - val_acc: 0.5333\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0022 - acc: 0.5194 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9668 - acc: 0.5589 - val_loss: 1.0102 - val_acc: 0.5333\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9924 - acc: 0.5342 - val_loss: 1.0097 - val_acc: 0.5333\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9982 - acc: 0.5334 - val_loss: 1.0096 - val_acc: 0.5259\n",
      "Epoch 24/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9856 - acc: 0.5317 - val_loss: 1.0101 - val_acc: 0.5259\n",
      "Epoch 00023: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "1078/1078 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [477 264 337], val_loss: 0.916\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9878 - acc: 0.5251 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9795 - acc: 0.5499 - val_loss: 1.0097 - val_acc: 0.5259\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0036 - acc: 0.5153 - val_loss: 1.0099 - val_acc: 0.5333\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9924 - acc: 0.5317 - val_loss: 1.0096 - val_acc: 0.5333\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9838 - acc: 0.5433 - val_loss: 1.0086 - val_acc: 0.5333\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9982 - acc: 0.5317 - val_loss: 1.0094 - val_acc: 0.5333\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0033 - acc: 0.5235 - val_loss: 1.0098 - val_acc: 0.5333\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9909 - acc: 0.5416 - val_loss: 1.0097 - val_acc: 0.5333\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9872 - acc: 0.5482 - val_loss: 1.0102 - val_acc: 0.5333\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9856 - acc: 0.5441 - val_loss: 1.0104 - val_acc: 0.5333\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9758 - acc: 0.5416 - val_loss: 1.0102 - val_acc: 0.5259\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9914 - acc: 0.5400 - val_loss: 1.0100 - val_acc: 0.5259\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9865 - acc: 0.5532 - val_loss: 1.0100 - val_acc: 0.5333\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9681 - acc: 0.5367 - val_loss: 1.0104 - val_acc: 0.5333\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9898 - acc: 0.5383 - val_loss: 1.0099 - val_acc: 0.5333\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9956 - acc: 0.5458 - val_loss: 1.0099 - val_acc: 0.5333\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9839 - acc: 0.5375 - val_loss: 1.0095 - val_acc: 0.5333\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9858 - acc: 0.5466 - val_loss: 1.0093 - val_acc: 0.5333\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9821 - acc: 0.5367 - val_loss: 1.0092 - val_acc: 0.5259\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9839 - acc: 0.5449 - val_loss: 1.0091 - val_acc: 0.5259\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9819 - acc: 0.5375 - val_loss: 1.0093 - val_acc: 0.5333\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9872 - acc: 0.5515 - val_loss: 1.0087 - val_acc: 0.5333\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9983 - acc: 0.5301 - val_loss: 1.0088 - val_acc: 0.5333\n",
      "Epoch 24/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9811 - acc: 0.5416 - val_loss: 1.0090 - val_acc: 0.5333\n",
      "Epoch 25/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9745 - acc: 0.5466 - val_loss: 1.0090 - val_acc: 0.5333\n",
      "Epoch 26/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9697 - acc: 0.5342 - val_loss: 1.0090 - val_acc: 0.5333\n",
      "Epoch 00025: early stopping\n",
      "269/269 [==============================] - 0s     \n",
      "1079/1079 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [477 264 338], val_loss: 0.928\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9852 - acc: 0.5392 - val_loss: 1.0088 - val_acc: 0.5333\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9876 - acc: 0.5408 - val_loss: 1.0093 - val_acc: 0.5333\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9889 - acc: 0.5367 - val_loss: 1.0093 - val_acc: 0.5333\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9853 - acc: 0.5276 - val_loss: 1.0095 - val_acc: 0.5333\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9966 - acc: 0.5408 - val_loss: 1.0093 - val_acc: 0.5259\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9856 - acc: 0.5474 - val_loss: 1.0092 - val_acc: 0.5259\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9885 - acc: 0.5392 - val_loss: 1.0090 - val_acc: 0.5259\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9895 - acc: 0.5400 - val_loss: 1.0093 - val_acc: 0.5333\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9992 - acc: 0.5425 - val_loss: 1.0091 - val_acc: 0.5259\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9792 - acc: 0.5433 - val_loss: 1.0088 - val_acc: 0.5259\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9792 - acc: 0.5400 - val_loss: 1.0088 - val_acc: 0.5259\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9839 - acc: 0.5466 - val_loss: 1.0088 - val_acc: 0.5333\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9927 - acc: 0.5260 - val_loss: 1.0090 - val_acc: 0.5333\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9994 - acc: 0.5309 - val_loss: 1.0083 - val_acc: 0.5259\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9851 - acc: 0.5383 - val_loss: 1.0081 - val_acc: 0.5259\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9918 - acc: 0.5284 - val_loss: 1.0084 - val_acc: 0.5259\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9928 - acc: 0.5425 - val_loss: 1.0085 - val_acc: 0.5333\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9920 - acc: 0.5367 - val_loss: 1.0084 - val_acc: 0.5259\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9796 - acc: 0.5449 - val_loss: 1.0086 - val_acc: 0.5333\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9756 - acc: 0.5482 - val_loss: 1.0089 - val_acc: 0.5185\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0001 - acc: 0.5392 - val_loss: 1.0083 - val_acc: 0.5185\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9845 - acc: 0.5449 - val_loss: 1.0086 - val_acc: 0.5185\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9843 - acc: 0.5441 - val_loss: 1.0086 - val_acc: 0.5259\n",
      "Epoch 24/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0030 - acc: 0.5309 - val_loss: 1.0082 - val_acc: 0.5259\n",
      "Epoch 25/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9945 - acc: 0.5309 - val_loss: 1.0082 - val_acc: 0.5259\n",
      "Epoch 26/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9850 - acc: 0.5416 - val_loss: 1.0081 - val_acc: 0.5259\n",
      "Epoch 27/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9723 - acc: 0.5540 - val_loss: 1.0083 - val_acc: 0.5185\n",
      "Epoch 28/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9842 - acc: 0.5499 - val_loss: 1.0083 - val_acc: 0.5185\n",
      "Epoch 29/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9860 - acc: 0.5499 - val_loss: 1.0085 - val_acc: 0.5259\n",
      "Epoch 30/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9812 - acc: 0.5425 - val_loss: 1.0086 - val_acc: 0.5259\n",
      "Epoch 31/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9815 - acc: 0.5309 - val_loss: 1.0089 - val_acc: 0.5259\n",
      "Epoch 32/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9790 - acc: 0.5449 - val_loss: 1.0090 - val_acc: 0.5333\n",
      "Epoch 33/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9831 - acc: 0.5433 - val_loss: 1.0094 - val_acc: 0.5333\n",
      "Epoch 34/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9944 - acc: 0.5301 - val_loss: 1.0091 - val_acc: 0.5333\n",
      "Epoch 35/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9770 - acc: 0.5425 - val_loss: 1.0103 - val_acc: 0.5333\n",
      "Epoch 36/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9958 - acc: 0.5449 - val_loss: 1.0093 - val_acc: 0.5333\n",
      "Epoch 37/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9772 - acc: 0.5458 - val_loss: 1.0095 - val_acc: 0.5333\n",
      "Epoch 38/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9828 - acc: 0.5359 - val_loss: 1.0095 - val_acc: 0.5333\n",
      "Epoch 39/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9882 - acc: 0.5433 - val_loss: 1.0084 - val_acc: 0.5333\n",
      "Epoch 40/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9804 - acc: 0.5441 - val_loss: 1.0084 - val_acc: 0.5333\n",
      "Epoch 41/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9855 - acc: 0.5342 - val_loss: 1.0084 - val_acc: 0.5185\n",
      "Epoch 42/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9710 - acc: 0.5458 - val_loss: 1.0082 - val_acc: 0.5185\n",
      "Epoch 43/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9807 - acc: 0.5425 - val_loss: 1.0091 - val_acc: 0.5185\n",
      "Epoch 44/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9839 - acc: 0.5482 - val_loss: 1.0088 - val_acc: 0.5185\n",
      "Epoch 45/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9739 - acc: 0.5491 - val_loss: 1.0090 - val_acc: 0.5259\n",
      "Epoch 46/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9909 - acc: 0.5392 - val_loss: 1.0086 - val_acc: 0.5185\n",
      "Epoch 47/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9833 - acc: 0.5367 - val_loss: 1.0085 - val_acc: 0.5259\n",
      "Epoch 00046: early stopping\n",
      "269/269 [==============================] - 0s     \n",
      "1079/1079 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [477 264 338], val_loss: 0.937\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9779 - acc: 0.5425 - val_loss: 1.0087 - val_acc: 0.5185\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9871 - acc: 0.5367 - val_loss: 1.0087 - val_acc: 0.5111\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9651 - acc: 0.5532 - val_loss: 1.0092 - val_acc: 0.5111\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9853 - acc: 0.5359 - val_loss: 1.0096 - val_acc: 0.5111\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9773 - acc: 0.5482 - val_loss: 1.0100 - val_acc: 0.5111\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9824 - acc: 0.5449 - val_loss: 1.0104 - val_acc: 0.5111\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9672 - acc: 0.5532 - val_loss: 1.0103 - val_acc: 0.5111\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9817 - acc: 0.5433 - val_loss: 1.0101 - val_acc: 0.5111\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9775 - acc: 0.5491 - val_loss: 1.0103 - val_acc: 0.5111\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9739 - acc: 0.5392 - val_loss: 1.0096 - val_acc: 0.5111\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9836 - acc: 0.5375 - val_loss: 1.0101 - val_acc: 0.5111\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9980 - acc: 0.5326 - val_loss: 1.0092 - val_acc: 0.5111\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9894 - acc: 0.5441 - val_loss: 1.0091 - val_acc: 0.5111\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9795 - acc: 0.5359 - val_loss: 1.0090 - val_acc: 0.5185\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9806 - acc: 0.5342 - val_loss: 1.0092 - val_acc: 0.5185\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9840 - acc: 0.5466 - val_loss: 1.0097 - val_acc: 0.5333\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9872 - acc: 0.5383 - val_loss: 1.0089 - val_acc: 0.5333\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9786 - acc: 0.5466 - val_loss: 1.0084 - val_acc: 0.5333\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9712 - acc: 0.5441 - val_loss: 1.0086 - val_acc: 0.5259\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9924 - acc: 0.5334 - val_loss: 1.0086 - val_acc: 0.5185\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9748 - acc: 0.5449 - val_loss: 1.0086 - val_acc: 0.5185\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9792 - acc: 0.5491 - val_loss: 1.0083 - val_acc: 0.5185\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9851 - acc: 0.5342 - val_loss: 1.0081 - val_acc: 0.5111\n",
      "Epoch 24/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9743 - acc: 0.5491 - val_loss: 1.0082 - val_acc: 0.5111\n",
      "Epoch 25/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9894 - acc: 0.5416 - val_loss: 1.0084 - val_acc: 0.5111\n",
      "Epoch 26/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9804 - acc: 0.5441 - val_loss: 1.0084 - val_acc: 0.5185\n",
      "Epoch 27/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9862 - acc: 0.5392 - val_loss: 1.0086 - val_acc: 0.5111\n",
      "Epoch 28/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9783 - acc: 0.5392 - val_loss: 1.0087 - val_acc: 0.5111\n",
      "Epoch 29/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9926 - acc: 0.5449 - val_loss: 1.0086 - val_acc: 0.5111\n",
      "Epoch 30/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9857 - acc: 0.5326 - val_loss: 1.0087 - val_acc: 0.5111\n",
      "Epoch 31/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9754 - acc: 0.5491 - val_loss: 1.0089 - val_acc: 0.5111\n",
      "Epoch 32/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9859 - acc: 0.5383 - val_loss: 1.0086 - val_acc: 0.5111\n",
      "Epoch 33/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9983 - acc: 0.5284 - val_loss: 1.0083 - val_acc: 0.5111\n",
      "Epoch 34/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9945 - acc: 0.5334 - val_loss: 1.0087 - val_acc: 0.5185\n",
      "Epoch 35/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9939 - acc: 0.5342 - val_loss: 1.0089 - val_acc: 0.5333\n",
      "Epoch 36/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9772 - acc: 0.5507 - val_loss: 1.0086 - val_acc: 0.5259\n",
      "Epoch 37/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9734 - acc: 0.5540 - val_loss: 1.0090 - val_acc: 0.5259\n",
      "Epoch 38/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0038 - acc: 0.5309 - val_loss: 1.0090 - val_acc: 0.5333\n",
      "Epoch 39/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9785 - acc: 0.5375 - val_loss: 1.0087 - val_acc: 0.5333\n",
      "Epoch 40/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9917 - acc: 0.5301 - val_loss: 1.0087 - val_acc: 0.5333\n",
      "Epoch 41/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9909 - acc: 0.5408 - val_loss: 1.0087 - val_acc: 0.5333\n",
      "Epoch 42/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9879 - acc: 0.5466 - val_loss: 1.0082 - val_acc: 0.5333\n",
      "Epoch 43/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9825 - acc: 0.5392 - val_loss: 1.0081 - val_acc: 0.5185\n",
      "Epoch 44/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9663 - acc: 0.5639 - val_loss: 1.0084 - val_acc: 0.5259\n",
      "Epoch 45/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9885 - acc: 0.5383 - val_loss: 1.0081 - val_acc: 0.5259\n",
      "Epoch 46/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9754 - acc: 0.5433 - val_loss: 1.0078 - val_acc: 0.5259\n",
      "Epoch 47/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9857 - acc: 0.5367 - val_loss: 1.0084 - val_acc: 0.5259\n",
      "Epoch 48/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9906 - acc: 0.5383 - val_loss: 1.0079 - val_acc: 0.5259\n",
      "Epoch 49/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0036 - acc: 0.5243 - val_loss: 1.0079 - val_acc: 0.5259\n",
      "Epoch 50/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9850 - acc: 0.5408 - val_loss: 1.0076 - val_acc: 0.5185\n",
      "Epoch 51/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9830 - acc: 0.5515 - val_loss: 1.0069 - val_acc: 0.5185\n",
      "Epoch 52/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9810 - acc: 0.5367 - val_loss: 1.0077 - val_acc: 0.5259\n",
      "Epoch 53/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9832 - acc: 0.5458 - val_loss: 1.0073 - val_acc: 0.5259\n",
      "Epoch 54/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9907 - acc: 0.5392 - val_loss: 1.0073 - val_acc: 0.5259\n",
      "Epoch 55/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9800 - acc: 0.5474 - val_loss: 1.0077 - val_acc: 0.5333\n",
      "Epoch 56/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9736 - acc: 0.5491 - val_loss: 1.0079 - val_acc: 0.5259\n",
      "Epoch 57/500\n",
      " 256/1213 [=====>........................] - ETA: 0s - loss: 1.0101 - acc: 0.5156"
     ]
    }
   ],
   "source": [
    "sizes= range(5,X.shape[1],5)\n",
    "train_loss,score_loss= testNodeNum(X_scaled,y,sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(5, 96, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG6eXweRzLCpeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbad11e9be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 85\n"
     ]
    }
   ],
   "source": [
    "print(sizes)\n",
    "loss = (np.mean(score_loss,axis=1))\n",
    "loss_std = (np.std(score_loss,axis=1))\n",
    "plt.plot(sizes,loss)\n",
    "plt.fill_between(sizes,loss-loss_std,loss+loss_std,alpha=0.3)\n",
    "plt.show()\n",
    "print(\"max: {}\".format(sizes[np.argmin(loss,axis=0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.2856 - acc: 0.3504 - val_loss: 1.0621 - val_acc: 0.4296\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1599 - acc: 0.3800 - val_loss: 1.0528 - val_acc: 0.4519\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1504 - acc: 0.3850 - val_loss: 1.0486 - val_acc: 0.4963\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.1287 - acc: 0.4114 - val_loss: 1.0430 - val_acc: 0.4593\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0971 - acc: 0.4303 - val_loss: 1.0358 - val_acc: 0.4889\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0686 - acc: 0.4427 - val_loss: 1.0410 - val_acc: 0.5185\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0767 - acc: 0.4633 - val_loss: 1.0328 - val_acc: 0.5185\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0639 - acc: 0.4782 - val_loss: 1.0327 - val_acc: 0.5037\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0359 - acc: 0.5012 - val_loss: 1.0252 - val_acc: 0.4815\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0246 - acc: 0.4905 - val_loss: 1.0239 - val_acc: 0.4815\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0316 - acc: 0.4979 - val_loss: 1.0281 - val_acc: 0.4741\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0256 - acc: 0.4831 - val_loss: 1.0277 - val_acc: 0.4741\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0110 - acc: 0.5136 - val_loss: 1.0216 - val_acc: 0.5037\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0020 - acc: 0.5120 - val_loss: 1.0185 - val_acc: 0.5185\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0083 - acc: 0.5177 - val_loss: 1.0189 - val_acc: 0.5185\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0141 - acc: 0.5078 - val_loss: 1.0192 - val_acc: 0.5259\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0007 - acc: 0.5202 - val_loss: 1.0176 - val_acc: 0.5185\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0107 - acc: 0.5177 - val_loss: 1.0221 - val_acc: 0.4963\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0010 - acc: 0.5268 - val_loss: 1.0194 - val_acc: 0.4889\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9971 - acc: 0.5161 - val_loss: 1.0295 - val_acc: 0.4815\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9870 - acc: 0.5070 - val_loss: 1.0142 - val_acc: 0.5185\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 1.0009 - acc: 0.5095 - val_loss: 1.0165 - val_acc: 0.5333\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9984 - acc: 0.5383 - val_loss: 1.0135 - val_acc: 0.5333\n",
      "Epoch 24/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9828 - acc: 0.5416 - val_loss: 1.0142 - val_acc: 0.5333\n",
      "Epoch 25/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9809 - acc: 0.5317 - val_loss: 1.0146 - val_acc: 0.4963\n",
      "Epoch 26/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9798 - acc: 0.5425 - val_loss: 1.0144 - val_acc: 0.4889\n",
      "Epoch 27/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9857 - acc: 0.5359 - val_loss: 1.0132 - val_acc: 0.5333\n",
      "Epoch 28/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9790 - acc: 0.5416 - val_loss: 1.0115 - val_acc: 0.5333\n",
      "Epoch 29/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9699 - acc: 0.5515 - val_loss: 1.0109 - val_acc: 0.5111\n",
      "Epoch 30/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9862 - acc: 0.5458 - val_loss: 1.0118 - val_acc: 0.5333\n",
      "Epoch 31/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9851 - acc: 0.5317 - val_loss: 1.0153 - val_acc: 0.4889\n",
      "Epoch 32/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9861 - acc: 0.5392 - val_loss: 1.0177 - val_acc: 0.4963\n",
      "Epoch 33/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9723 - acc: 0.5441 - val_loss: 1.0142 - val_acc: 0.4741\n",
      "Epoch 34/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9776 - acc: 0.5367 - val_loss: 1.0112 - val_acc: 0.4963\n",
      "Epoch 35/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9752 - acc: 0.5458 - val_loss: 1.0115 - val_acc: 0.5037\n",
      "Epoch 36/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9635 - acc: 0.5416 - val_loss: 1.0113 - val_acc: 0.4963\n",
      "Epoch 37/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9712 - acc: 0.5441 - val_loss: 1.0115 - val_acc: 0.5037\n",
      "Epoch 38/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9692 - acc: 0.5433 - val_loss: 1.0110 - val_acc: 0.5333\n",
      "Epoch 39/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9672 - acc: 0.5515 - val_loss: 1.0118 - val_acc: 0.4963\n",
      "Epoch 40/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9626 - acc: 0.5458 - val_loss: 1.0088 - val_acc: 0.5037\n",
      "Epoch 41/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9602 - acc: 0.5622 - val_loss: 1.0095 - val_acc: 0.5037\n",
      "Epoch 42/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9707 - acc: 0.5458 - val_loss: 1.0100 - val_acc: 0.5037\n",
      "Epoch 43/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9675 - acc: 0.5523 - val_loss: 1.0082 - val_acc: 0.4963\n",
      "Epoch 44/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9668 - acc: 0.5425 - val_loss: 1.0086 - val_acc: 0.5037\n",
      "Epoch 45/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9527 - acc: 0.5482 - val_loss: 1.0086 - val_acc: 0.5037\n",
      "Epoch 46/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9532 - acc: 0.5491 - val_loss: 1.0077 - val_acc: 0.5037\n",
      "Epoch 47/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9648 - acc: 0.5425 - val_loss: 1.0079 - val_acc: 0.5037\n",
      "Epoch 48/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9507 - acc: 0.5664 - val_loss: 1.0068 - val_acc: 0.5037\n",
      "Epoch 49/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9607 - acc: 0.5499 - val_loss: 1.0048 - val_acc: 0.5111\n",
      "Epoch 50/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9520 - acc: 0.5507 - val_loss: 1.0079 - val_acc: 0.4815\n",
      "Epoch 51/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9597 - acc: 0.5449 - val_loss: 1.0052 - val_acc: 0.4963\n",
      "Epoch 52/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9575 - acc: 0.5655 - val_loss: 1.0089 - val_acc: 0.4815\n",
      "Epoch 53/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9607 - acc: 0.5392 - val_loss: 1.0046 - val_acc: 0.4963\n",
      "Epoch 54/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9491 - acc: 0.5655 - val_loss: 1.0067 - val_acc: 0.4889\n",
      "Epoch 55/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9581 - acc: 0.5474 - val_loss: 1.0053 - val_acc: 0.4963\n",
      "Epoch 56/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9582 - acc: 0.5523 - val_loss: 1.0115 - val_acc: 0.4741\n",
      "Epoch 57/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9551 - acc: 0.5499 - val_loss: 1.0058 - val_acc: 0.5037\n",
      "Epoch 58/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9588 - acc: 0.5482 - val_loss: 1.0048 - val_acc: 0.5111\n",
      "Epoch 59/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9499 - acc: 0.5565 - val_loss: 1.0059 - val_acc: 0.4963\n",
      "Epoch 60/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9404 - acc: 0.5573 - val_loss: 1.0059 - val_acc: 0.4889\n",
      "Epoch 61/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9512 - acc: 0.5614 - val_loss: 1.0053 - val_acc: 0.4963\n",
      "Epoch 62/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9420 - acc: 0.5713 - val_loss: 1.0071 - val_acc: 0.4889\n",
      "Epoch 63/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9503 - acc: 0.5515 - val_loss: 1.0040 - val_acc: 0.5037\n",
      "Epoch 64/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9454 - acc: 0.5581 - val_loss: 1.0049 - val_acc: 0.5111\n",
      "Epoch 65/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9519 - acc: 0.5664 - val_loss: 1.0056 - val_acc: 0.5037\n",
      "Epoch 66/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9536 - acc: 0.5507 - val_loss: 1.0046 - val_acc: 0.5037\n",
      "Epoch 67/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9383 - acc: 0.5622 - val_loss: 1.0071 - val_acc: 0.4963\n",
      "Epoch 68/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9377 - acc: 0.5730 - val_loss: 1.0073 - val_acc: 0.4889\n",
      "Epoch 69/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9331 - acc: 0.5581 - val_loss: 1.0051 - val_acc: 0.4889\n",
      "Epoch 70/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9395 - acc: 0.5697 - val_loss: 1.0059 - val_acc: 0.5037\n",
      "Epoch 71/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9429 - acc: 0.5672 - val_loss: 1.0052 - val_acc: 0.4963\n",
      "Epoch 72/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9312 - acc: 0.5639 - val_loss: 1.0059 - val_acc: 0.4889\n",
      "Epoch 73/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9434 - acc: 0.5523 - val_loss: 1.0049 - val_acc: 0.5185\n",
      "Epoch 74/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9367 - acc: 0.5606 - val_loss: 1.0049 - val_acc: 0.4963\n",
      "Epoch 75/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9320 - acc: 0.5697 - val_loss: 1.0059 - val_acc: 0.4889\n",
      "Epoch 76/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9353 - acc: 0.5606 - val_loss: 1.0050 - val_acc: 0.5037\n",
      "Epoch 77/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9344 - acc: 0.5672 - val_loss: 1.0039 - val_acc: 0.5037\n",
      "Epoch 78/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9317 - acc: 0.5672 - val_loss: 1.0065 - val_acc: 0.4963\n",
      "Epoch 79/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9339 - acc: 0.5647 - val_loss: 1.0052 - val_acc: 0.4963\n",
      "Epoch 80/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9258 - acc: 0.5862 - val_loss: 1.0089 - val_acc: 0.4889\n",
      "Epoch 81/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9297 - acc: 0.5655 - val_loss: 1.0089 - val_acc: 0.4889\n",
      "Epoch 82/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9239 - acc: 0.5672 - val_loss: 1.0074 - val_acc: 0.4963\n",
      "Epoch 83/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9266 - acc: 0.5713 - val_loss: 1.0070 - val_acc: 0.5037\n",
      "Epoch 84/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9218 - acc: 0.5697 - val_loss: 1.0090 - val_acc: 0.4889\n",
      "Epoch 85/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9248 - acc: 0.5664 - val_loss: 1.0113 - val_acc: 0.4963\n",
      "Epoch 86/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9260 - acc: 0.5796 - val_loss: 1.0083 - val_acc: 0.4963\n",
      "Epoch 87/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9287 - acc: 0.5655 - val_loss: 1.0076 - val_acc: 0.5037\n",
      "Epoch 88/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9244 - acc: 0.5738 - val_loss: 1.0069 - val_acc: 0.4963\n",
      "Epoch 89/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9006 - acc: 0.5845 - val_loss: 1.0083 - val_acc: 0.4889\n",
      "Epoch 90/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9284 - acc: 0.5779 - val_loss: 1.0077 - val_acc: 0.4963\n",
      "Epoch 91/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9261 - acc: 0.5672 - val_loss: 1.0074 - val_acc: 0.4963\n",
      "Epoch 92/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9149 - acc: 0.5746 - val_loss: 1.0083 - val_acc: 0.4963\n",
      "Epoch 93/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9060 - acc: 0.5845 - val_loss: 1.0064 - val_acc: 0.5037\n",
      "Epoch 94/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9150 - acc: 0.5763 - val_loss: 1.0077 - val_acc: 0.5037\n",
      "Epoch 95/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9194 - acc: 0.5804 - val_loss: 1.0071 - val_acc: 0.4963\n",
      "Epoch 96/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9189 - acc: 0.5738 - val_loss: 1.0074 - val_acc: 0.5037\n",
      "Epoch 97/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9171 - acc: 0.5713 - val_loss: 1.0104 - val_acc: 0.4963\n",
      "Epoch 98/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9079 - acc: 0.5697 - val_loss: 1.0073 - val_acc: 0.5037\n",
      "Epoch 00097: early stopping\n",
      "271/271 [==============================] - 0s     \n",
      "271/271 [==============================] - 0s     \n",
      "1077/1077 [==============================] - 0s     \n",
      "Fold: 1, Class dist.: [476 264 337], val_loss: 0.879\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9187 - acc: 0.5829 - val_loss: 1.0087 - val_acc: 0.5037\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9050 - acc: 0.5878 - val_loss: 1.0074 - val_acc: 0.5037\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9064 - acc: 0.5845 - val_loss: 1.0070 - val_acc: 0.5037\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9131 - acc: 0.5738 - val_loss: 1.0080 - val_acc: 0.4963\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9140 - acc: 0.5787 - val_loss: 1.0053 - val_acc: 0.5037\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9002 - acc: 0.5796 - val_loss: 1.0058 - val_acc: 0.5037\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9105 - acc: 0.5614 - val_loss: 1.0061 - val_acc: 0.5111\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8985 - acc: 0.5721 - val_loss: 1.0077 - val_acc: 0.5111\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9077 - acc: 0.5837 - val_loss: 1.0076 - val_acc: 0.5111\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9040 - acc: 0.5730 - val_loss: 1.0080 - val_acc: 0.5111\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9048 - acc: 0.5837 - val_loss: 1.0078 - val_acc: 0.5037\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9024 - acc: 0.5672 - val_loss: 1.0095 - val_acc: 0.5259\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9003 - acc: 0.5812 - val_loss: 1.0118 - val_acc: 0.4815\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9001 - acc: 0.5837 - val_loss: 1.0118 - val_acc: 0.4815\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9032 - acc: 0.5894 - val_loss: 1.0140 - val_acc: 0.4815\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8876 - acc: 0.5919 - val_loss: 1.0115 - val_acc: 0.4815\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8984 - acc: 0.5845 - val_loss: 1.0088 - val_acc: 0.5185\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8922 - acc: 0.5730 - val_loss: 1.0105 - val_acc: 0.5111\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8995 - acc: 0.5853 - val_loss: 1.0077 - val_acc: 0.5037\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9082 - acc: 0.5787 - val_loss: 1.0076 - val_acc: 0.5037\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8953 - acc: 0.5927 - val_loss: 1.0075 - val_acc: 0.5185\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8894 - acc: 0.5952 - val_loss: 1.0101 - val_acc: 0.5037\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8930 - acc: 0.5903 - val_loss: 1.0086 - val_acc: 0.4963\n",
      "Epoch 24/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8908 - acc: 0.5862 - val_loss: 1.0089 - val_acc: 0.5037\n",
      "Epoch 25/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8937 - acc: 0.5903 - val_loss: 1.0100 - val_acc: 0.5185\n",
      "Epoch 26/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8874 - acc: 0.5985 - val_loss: 1.0120 - val_acc: 0.5037\n",
      "Epoch 00025: early stopping\n",
      "270/270 [==============================] - 0s     \n",
      "270/270 [==============================] - 0s     \n",
      "1078/1078 [==============================] - 0s     \n",
      "Fold: 2, Class dist.: [477 264 337], val_loss: 0.810\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8907 - acc: 0.5870 - val_loss: 1.0121 - val_acc: 0.4963\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9012 - acc: 0.5796 - val_loss: 1.0117 - val_acc: 0.5037\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8779 - acc: 0.5862 - val_loss: 1.0127 - val_acc: 0.4963\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8749 - acc: 0.5886 - val_loss: 1.0113 - val_acc: 0.4963\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8864 - acc: 0.5903 - val_loss: 1.0110 - val_acc: 0.4889\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8964 - acc: 0.5894 - val_loss: 1.0140 - val_acc: 0.5037\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8809 - acc: 0.5886 - val_loss: 1.0140 - val_acc: 0.5037\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.9006 - acc: 0.5960 - val_loss: 1.0102 - val_acc: 0.5185\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8823 - acc: 0.6035 - val_loss: 1.0104 - val_acc: 0.4963\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8946 - acc: 0.5894 - val_loss: 1.0104 - val_acc: 0.4889\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8835 - acc: 0.5927 - val_loss: 1.0167 - val_acc: 0.5037\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8890 - acc: 0.5927 - val_loss: 1.0100 - val_acc: 0.5111\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8801 - acc: 0.5969 - val_loss: 1.0124 - val_acc: 0.5111\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8838 - acc: 0.5812 - val_loss: 1.0123 - val_acc: 0.5111\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8890 - acc: 0.5919 - val_loss: 1.0132 - val_acc: 0.5037\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8897 - acc: 0.5829 - val_loss: 1.0131 - val_acc: 0.5111\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8688 - acc: 0.6076 - val_loss: 1.0168 - val_acc: 0.4963\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8730 - acc: 0.6092 - val_loss: 1.0152 - val_acc: 0.5037\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8832 - acc: 0.5829 - val_loss: 1.0150 - val_acc: 0.4963\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8809 - acc: 0.5919 - val_loss: 1.0143 - val_acc: 0.5037\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8833 - acc: 0.5862 - val_loss: 1.0143 - val_acc: 0.5185\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8615 - acc: 0.5977 - val_loss: 1.0186 - val_acc: 0.5037\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8767 - acc: 0.5820 - val_loss: 1.0186 - val_acc: 0.5037\n",
      "Epoch 24/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8713 - acc: 0.5944 - val_loss: 1.0165 - val_acc: 0.5259\n",
      "Epoch 25/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8703 - acc: 0.6092 - val_loss: 1.0204 - val_acc: 0.4889\n",
      "Epoch 26/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8708 - acc: 0.5903 - val_loss: 1.0237 - val_acc: 0.5111\n",
      "Epoch 27/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8699 - acc: 0.5969 - val_loss: 1.0211 - val_acc: 0.4963\n",
      "Epoch 28/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8756 - acc: 0.5845 - val_loss: 1.0208 - val_acc: 0.5037\n",
      "Epoch 29/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8828 - acc: 0.5894 - val_loss: 1.0203 - val_acc: 0.4963\n",
      "Epoch 30/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8755 - acc: 0.5845 - val_loss: 1.0198 - val_acc: 0.4889\n",
      "Epoch 31/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8660 - acc: 0.5894 - val_loss: 1.0239 - val_acc: 0.4963\n",
      "Epoch 32/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8684 - acc: 0.5936 - val_loss: 1.0258 - val_acc: 0.5037\n",
      "Epoch 33/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8609 - acc: 0.6059 - val_loss: 1.0239 - val_acc: 0.4963\n",
      "Epoch 00032: early stopping\n",
      "269/269 [==============================] - 0s     \n",
      "269/269 [==============================] - 0s     \n",
      "1079/1079 [==============================] - 0s     \n",
      "Fold: 3, Class dist.: [477 264 338], val_loss: 0.849\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8688 - acc: 0.5886 - val_loss: 1.0226 - val_acc: 0.5111\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8602 - acc: 0.6018 - val_loss: 1.0255 - val_acc: 0.4889\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8738 - acc: 0.5870 - val_loss: 1.0256 - val_acc: 0.5037\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8604 - acc: 0.5960 - val_loss: 1.0237 - val_acc: 0.5037\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8613 - acc: 0.6043 - val_loss: 1.0254 - val_acc: 0.5111\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8563 - acc: 0.6051 - val_loss: 1.0268 - val_acc: 0.5037\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8695 - acc: 0.6101 - val_loss: 1.0244 - val_acc: 0.5037\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8608 - acc: 0.6018 - val_loss: 1.0232 - val_acc: 0.5111\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8695 - acc: 0.6018 - val_loss: 1.0223 - val_acc: 0.5111\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8532 - acc: 0.6026 - val_loss: 1.0213 - val_acc: 0.5185\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8561 - acc: 0.6059 - val_loss: 1.0239 - val_acc: 0.5111\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8665 - acc: 0.5985 - val_loss: 1.0250 - val_acc: 0.5037\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8643 - acc: 0.6142 - val_loss: 1.0214 - val_acc: 0.5111\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8527 - acc: 0.5969 - val_loss: 1.0233 - val_acc: 0.5185\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8592 - acc: 0.6076 - val_loss: 1.0246 - val_acc: 0.5111\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8406 - acc: 0.6051 - val_loss: 1.0286 - val_acc: 0.4889\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8494 - acc: 0.5960 - val_loss: 1.0255 - val_acc: 0.4889\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8692 - acc: 0.5952 - val_loss: 1.0197 - val_acc: 0.5185\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8528 - acc: 0.6076 - val_loss: 1.0209 - val_acc: 0.5185\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8585 - acc: 0.6092 - val_loss: 1.0248 - val_acc: 0.5111\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8534 - acc: 0.6125 - val_loss: 1.0238 - val_acc: 0.5185\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8479 - acc: 0.6059 - val_loss: 1.0227 - val_acc: 0.5185\n",
      "Epoch 23/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8482 - acc: 0.5977 - val_loss: 1.0275 - val_acc: 0.5037\n",
      "Epoch 24/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8484 - acc: 0.6101 - val_loss: 1.0247 - val_acc: 0.5185\n",
      "Epoch 25/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8385 - acc: 0.6249 - val_loss: 1.0282 - val_acc: 0.4963\n",
      "Epoch 26/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8446 - acc: 0.5927 - val_loss: 1.0275 - val_acc: 0.5111\n",
      "Epoch 27/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8540 - acc: 0.6191 - val_loss: 1.0291 - val_acc: 0.4963\n",
      "Epoch 28/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8543 - acc: 0.6026 - val_loss: 1.0326 - val_acc: 0.4889\n",
      "Epoch 29/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8587 - acc: 0.5969 - val_loss: 1.0329 - val_acc: 0.4889\n",
      "Epoch 30/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8438 - acc: 0.6068 - val_loss: 1.0317 - val_acc: 0.5111\n",
      "Epoch 31/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8492 - acc: 0.6026 - val_loss: 1.0298 - val_acc: 0.5111\n",
      "Epoch 32/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8303 - acc: 0.6084 - val_loss: 1.0330 - val_acc: 0.5111\n",
      "Epoch 33/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8319 - acc: 0.5993 - val_loss: 1.0354 - val_acc: 0.4815\n",
      "Epoch 34/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8341 - acc: 0.6208 - val_loss: 1.0349 - val_acc: 0.5111\n",
      "Epoch 35/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8337 - acc: 0.6018 - val_loss: 1.0348 - val_acc: 0.5111\n",
      "Epoch 36/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8360 - acc: 0.6084 - val_loss: 1.0339 - val_acc: 0.4963\n",
      "Epoch 37/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8387 - acc: 0.5977 - val_loss: 1.0357 - val_acc: 0.5037\n",
      "Epoch 38/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8268 - acc: 0.6076 - val_loss: 1.0370 - val_acc: 0.4889\n",
      "Epoch 39/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8400 - acc: 0.6076 - val_loss: 1.0330 - val_acc: 0.5111\n",
      "Epoch 00038: early stopping\n",
      "269/269 [==============================] - 0s     \n",
      "269/269 [==============================] - 0s     \n",
      "1079/1079 [==============================] - 0s     \n",
      "Fold: 4, Class dist.: [477 264 338], val_loss: 0.829\n",
      "Train on 1213 samples, validate on 135 samples\n",
      "Epoch 1/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8533 - acc: 0.5894 - val_loss: 1.0300 - val_acc: 0.5259\n",
      "Epoch 2/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8266 - acc: 0.6158 - val_loss: 1.0338 - val_acc: 0.5111\n",
      "Epoch 3/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8338 - acc: 0.6191 - val_loss: 1.0385 - val_acc: 0.5111\n",
      "Epoch 4/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8266 - acc: 0.6109 - val_loss: 1.0376 - val_acc: 0.5259\n",
      "Epoch 5/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8266 - acc: 0.6249 - val_loss: 1.0412 - val_acc: 0.4963\n",
      "Epoch 6/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8223 - acc: 0.6018 - val_loss: 1.0394 - val_acc: 0.5185\n",
      "Epoch 7/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8272 - acc: 0.6200 - val_loss: 1.0388 - val_acc: 0.5037\n",
      "Epoch 8/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8343 - acc: 0.5993 - val_loss: 1.0379 - val_acc: 0.5185\n",
      "Epoch 9/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8240 - acc: 0.6142 - val_loss: 1.0354 - val_acc: 0.5259\n",
      "Epoch 10/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8306 - acc: 0.6158 - val_loss: 1.0334 - val_acc: 0.5259\n",
      "Epoch 11/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8180 - acc: 0.6059 - val_loss: 1.0327 - val_acc: 0.5259\n",
      "Epoch 12/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8371 - acc: 0.6084 - val_loss: 1.0328 - val_acc: 0.5259\n",
      "Epoch 13/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8346 - acc: 0.6150 - val_loss: 1.0312 - val_acc: 0.5259\n",
      "Epoch 14/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8320 - acc: 0.5977 - val_loss: 1.0355 - val_acc: 0.5333\n",
      "Epoch 15/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8361 - acc: 0.6142 - val_loss: 1.0339 - val_acc: 0.5259\n",
      "Epoch 16/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8024 - acc: 0.6257 - val_loss: 1.0367 - val_acc: 0.5333\n",
      "Epoch 17/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8425 - acc: 0.6084 - val_loss: 1.0316 - val_acc: 0.5333\n",
      "Epoch 18/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8339 - acc: 0.6117 - val_loss: 1.0365 - val_acc: 0.5185\n",
      "Epoch 19/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8218 - acc: 0.6101 - val_loss: 1.0334 - val_acc: 0.5333\n",
      "Epoch 20/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8124 - acc: 0.6265 - val_loss: 1.0378 - val_acc: 0.5333\n",
      "Epoch 21/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8073 - acc: 0.6150 - val_loss: 1.0401 - val_acc: 0.5259\n",
      "Epoch 22/500\n",
      "1213/1213 [==============================] - 0s - loss: 0.8181 - acc: 0.6158 - val_loss: 1.0367 - val_acc: 0.5333\n",
      "Epoch 00021: early stopping\n",
      "269/269 [==============================] - 0s     \n",
      "269/269 [==============================] - 0s     \n",
      "1079/1079 [==============================] - 0s     \n",
      "Fold: 5, Class dist.: [477 264 338], val_loss: 0.905\n"
     ]
    }
   ],
   "source": [
    "model = createModel([85,85])\n",
    "train_scores,scores, proba_test,proba_y = crossValidate2(model,X_scaled,y,fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_Correct</th>\n",
       "      <th>h_Wrong</th>\n",
       "      <th>h_Precent</th>\n",
       "      <th>d_Correct</th>\n",
       "      <th>d_Wrong</th>\n",
       "      <th>d_Precent</th>\n",
       "      <th>a_Correct</th>\n",
       "      <th>a_Wrong</th>\n",
       "      <th>a_Precent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&gt;80</th>\n",
       "      <td>109</td>\n",
       "      <td>8</td>\n",
       "      <td>0.931624</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80</th>\n",
       "      <td>161</td>\n",
       "      <td>57</td>\n",
       "      <td>0.738532</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>82</td>\n",
       "      <td>26</td>\n",
       "      <td>0.759259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-60</th>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>49</td>\n",
       "      <td>36</td>\n",
       "      <td>0.576471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-50</th>\n",
       "      <td>113</td>\n",
       "      <td>141</td>\n",
       "      <td>0.444882</td>\n",
       "      <td>41</td>\n",
       "      <td>27</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>44</td>\n",
       "      <td>46</td>\n",
       "      <td>0.488889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30-40</th>\n",
       "      <td>87</td>\n",
       "      <td>131</td>\n",
       "      <td>0.399083</td>\n",
       "      <td>95</td>\n",
       "      <td>170</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>70</td>\n",
       "      <td>159</td>\n",
       "      <td>0.305677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-30</th>\n",
       "      <td>26</td>\n",
       "      <td>116</td>\n",
       "      <td>0.183099</td>\n",
       "      <td>135</td>\n",
       "      <td>396</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>91</td>\n",
       "      <td>253</td>\n",
       "      <td>0.264535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;20</th>\n",
       "      <td>16</td>\n",
       "      <td>215</td>\n",
       "      <td>0.069264</td>\n",
       "      <td>36</td>\n",
       "      <td>416</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>38</td>\n",
       "      <td>402</td>\n",
       "      <td>0.086364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       h_Correct  h_Wrong  h_Precent  d_Correct  d_Wrong  d_Precent  \\\n",
       ">80          109        8   0.931624          2        0   1.000000   \n",
       "60-80        161       57   0.738532          7        2   0.777778   \n",
       "50-60         84       84   0.500000         14        7   0.666667   \n",
       "40-50        113      141   0.444882         41       27   0.602941   \n",
       "30-40         87      131   0.399083         95      170   0.358491   \n",
       "20-30         26      116   0.183099        135      396   0.254237   \n",
       "<20           16      215   0.069264         36      416   0.079646   \n",
       "\n",
       "       a_Correct  a_Wrong  a_Precent  \n",
       ">80           48        4   0.923077  \n",
       "60-80         82       26   0.759259  \n",
       "50-60         49       36   0.576471  \n",
       "40-50         44       46   0.488889  \n",
       "30-40         70      159   0.305677  \n",
       "20-30         91      253   0.264535  \n",
       "<20           38      402   0.086364  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisionMatrix(np.vstack(proba_test),np.vstack(proba_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
